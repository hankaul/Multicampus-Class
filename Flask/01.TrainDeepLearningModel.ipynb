{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1f38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flask import Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d671a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0cd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Import 데이터 불러오기\n",
    "DataFile = pd.read_csv(\"MY2022 Fuel Consumption Ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75231593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 946 entries, 0 to 945\n",
      "Data columns (total 15 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Model Year                         946 non-null    int64  \n",
      " 1   Make                               946 non-null    object \n",
      " 2   Model                              946 non-null    object \n",
      " 3   Vehicle Class                      946 non-null    object \n",
      " 4   Engine Size(L)                     946 non-null    float64\n",
      " 5   Cylinders                          946 non-null    int64  \n",
      " 6   Transmission                       946 non-null    object \n",
      " 7   Fuel Type                          946 non-null    object \n",
      " 8   Fuel Consumption (City (L/100 km)  946 non-null    float64\n",
      " 9   Fuel Consumption(Hwy (L/100 km))   946 non-null    float64\n",
      " 10  Fuel Consumption(Comb (L/100 km))  946 non-null    float64\n",
      " 11  Fuel Consumption(Comb (mpg))       946 non-null    int64  \n",
      " 12  CO2 Emissions(g/km)                946 non-null    int64  \n",
      " 13  CO2 Rating                         946 non-null    int64  \n",
      " 14  Smog Rating                        946 non-null    int64  \n",
      "dtypes: float64(4), int64(6), object(5)\n",
      "memory usage: 111.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분석했었던 것 읽어오기\n",
    "DataFile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf42f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model Year', 'Make', 'Model', 'Vehicle Class', 'Engine Size(L)',\n",
       "       'Cylinders', 'Transmission', 'Fuel Type',\n",
       "       'Fuel Consumption (City (L/100 km)', 'Fuel Consumption(Hwy (L/100 km))',\n",
       "       'Fuel Consumption(Comb (L/100 km))', 'Fuel Consumption(Comb (mpg))',\n",
       "       'CO2 Emissions(g/km)', 'CO2 Rating', 'Smog Rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column들 보기\n",
    "DataFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b46b43ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cylinders', 'Fuel Consumption(Comb (L/100 km))', 'CO2 Rating'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중요한 변수 3가지만 일단 선택 (API 서비스를 위한)\n",
    "DataFile.columns[[5, 10, 13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbff197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 선택 및 X, Y 데이터 선정\n",
    "Data = DataFile[DataFile.columns[[5, 10, 13]]]\n",
    "Label = DataFile['CO2 Emissions(g/km)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972538a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Valid 나누기\n",
    "TrainX = Data.iloc[:-200,:]\n",
    "TrainY = Label[:-200]\n",
    "\n",
    "ValidX = Data.iloc[-200:,:]\n",
    "ValidY = Label[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe0bb109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Fuel Consumption(Comb (L/100 km))</th>\n",
       "      <th>CO2 Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>8.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>11.2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>4</td>\n",
       "      <td>7.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>4</td>\n",
       "      <td>8.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>4</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>746 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cylinders  Fuel Consumption(Comb (L/100 km))  CO2 Rating\n",
       "0            4                                8.6           6\n",
       "1            6                               11.2           4\n",
       "2            4                                9.9           5\n",
       "3            4                               10.3           5\n",
       "4            4                                9.8           5\n",
       "..         ...                                ...         ...\n",
       "741          4                                7.3           7\n",
       "742          4                                7.9           6\n",
       "743          4                                8.1           6\n",
       "744          4                                6.7           7\n",
       "745          4                                7.7           7\n",
       "\n",
       "[746 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f885eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조\n",
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu', input_shape=(3,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# CallBack 함수를 통해 LR을 낮출 것이므로, 초기 LR을 높게 잡기\n",
    "model.compile(loss='mse',\n",
    "              optimizer='Adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "#0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609eab90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 32        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                270       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333\n",
      "Trainable params: 333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377b72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 저장할 폴더 위치 선정\n",
    "os.makedirs('Model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3d0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-back 함수\n",
    "# CheckPoint: Epoch 마다 validation 성능을 검증하여, best performance 일 경우 저장\n",
    "CP = ModelCheckpoint(filepath='Model/{epoch:03d}-{loss:.4f}-{val_loss:.4f}.hdf5',\n",
    "            monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#minitor -> loss -> val_loss\n",
    "\n",
    "# Learning Rate 줄여나가기\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5,\n",
    "                       verbose=1, min_lr=1e-8)\n",
    "# factor: Learning rate에 곱할 것.\n",
    "#0.1 -> 0.08 -> 0.064 ....\n",
    "#monitor='loss' -> monitor='val_loss'\n",
    "\n",
    "CALLBACK = [CP, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882e5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      " 1/16 [>.............................] - ETA: 4s - loss: 74077.8359 - root_mean_squared_error: 272.1724\n",
      "Epoch 1: val_loss improved from inf to 64709.50391, saving model to Model\\001-71016.4141-64709.5039.hdf5\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 71016.4141 - root_mean_squared_error: 266.4891 - val_loss: 64709.5039 - val_root_mean_squared_error: 254.3806 - lr: 0.0010\n",
      "Epoch 2/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 65081.9062 - root_mean_squared_error: 255.1116\n",
      "Epoch 2: val_loss improved from 64709.50391 to 63228.64844, saving model to Model\\002-69510.8906-63228.6484.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 69510.8906 - root_mean_squared_error: 263.6492 - val_loss: 63228.6484 - val_root_mean_squared_error: 251.4531 - lr: 0.0010\n",
      "Epoch 3/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 72461.8516 - root_mean_squared_error: 269.1874\n",
      "Epoch 3: val_loss improved from 63228.64844 to 61372.71875, saving model to Model\\003-67756.3984-61372.7188.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 67756.3984 - root_mean_squared_error: 260.3006 - val_loss: 61372.7188 - val_root_mean_squared_error: 247.7352 - lr: 0.0010\n",
      "Epoch 4/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 62771.3867 - root_mean_squared_error: 250.5422\n",
      "Epoch 4: val_loss improved from 61372.71875 to 58938.73047, saving model to Model\\004-65445.1836-58938.7305.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 65445.1836 - root_mean_squared_error: 255.8226 - val_loss: 58938.7305 - val_root_mean_squared_error: 242.7730 - lr: 0.0010\n",
      "Epoch 5/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 63585.8008 - root_mean_squared_error: 252.1622\n",
      "Epoch 5: val_loss improved from 58938.73047 to 55875.21875, saving model to Model\\005-62540.3594-55875.2188.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 62540.3594 - root_mean_squared_error: 250.0807 - val_loss: 55875.2188 - val_root_mean_squared_error: 236.3794 - lr: 0.0010\n",
      "Epoch 6/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 63397.4883 - root_mean_squared_error: 251.7886\n",
      "Epoch 6: val_loss improved from 55875.21875 to 52096.68359, saving model to Model\\006-58871.5859-52096.6836.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 58871.5859 - root_mean_squared_error: 242.6347 - val_loss: 52096.6836 - val_root_mean_squared_error: 228.2470 - lr: 0.0010\n",
      "Epoch 7/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 57330.0625 - root_mean_squared_error: 239.4370\n",
      "Epoch 7: val_loss improved from 52096.68359 to 47449.66016, saving model to Model\\007-54393.7148-47449.6602.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 54393.7148 - root_mean_squared_error: 233.2246 - val_loss: 47449.6602 - val_root_mean_squared_error: 217.8294 - lr: 0.0010\n",
      "Epoch 8/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 56323.0117 - root_mean_squared_error: 237.3247\n",
      "Epoch 8: val_loss improved from 47449.66016 to 41971.64062, saving model to Model\\008-48968.5508-41971.6406.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 48968.5508 - root_mean_squared_error: 221.2884 - val_loss: 41971.6406 - val_root_mean_squared_error: 204.8698 - lr: 0.0010\n",
      "Epoch 9/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46855.7305 - root_mean_squared_error: 216.4618\n",
      "Epoch 9: val_loss improved from 41971.64062 to 35838.67188, saving model to Model\\009-42679.6641-35838.6719.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 42679.6641 - root_mean_squared_error: 206.5906 - val_loss: 35838.6719 - val_root_mean_squared_error: 189.3111 - lr: 0.0010\n",
      "Epoch 10/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 37330.6445 - root_mean_squared_error: 193.2114\n",
      "Epoch 10: val_loss improved from 35838.67188 to 29277.43164, saving model to Model\\010-35837.7422-29277.4316.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 35837.7422 - root_mean_squared_error: 189.3086 - val_loss: 29277.4316 - val_root_mean_squared_error: 171.1065 - lr: 0.0010\n",
      "Epoch 11/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 31446.7363 - root_mean_squared_error: 177.3323\n",
      "Epoch 11: val_loss improved from 29277.43164 to 22779.46680, saving model to Model\\011-28705.8496-22779.4668.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 28705.8496 - root_mean_squared_error: 169.4280 - val_loss: 22779.4668 - val_root_mean_squared_error: 150.9287 - lr: 0.0010\n",
      "Epoch 12/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 24498.0605 - root_mean_squared_error: 156.5186\n",
      "Epoch 12: val_loss improved from 22779.46680 to 16586.82031, saving model to Model\\012-21794.4590-16586.8203.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 21794.4590 - root_mean_squared_error: 147.6295 - val_loss: 16586.8203 - val_root_mean_squared_error: 128.7898 - lr: 0.0010\n",
      "Epoch 13/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 23338.7090 - root_mean_squared_error: 152.7701\n",
      "Epoch 13: val_loss improved from 16586.82031 to 11153.54492, saving model to Model\\013-15402.7559-11153.5449.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 15402.7559 - root_mean_squared_error: 124.1078 - val_loss: 11153.5449 - val_root_mean_squared_error: 105.6103 - lr: 0.0010\n",
      "Epoch 14/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 13096.7549 - root_mean_squared_error: 114.4411\n",
      "Epoch 14: val_loss improved from 11153.54492 to 6828.25586, saving model to Model\\014-10048.2920-6828.2559.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10048.2920 - root_mean_squared_error: 100.2412 - val_loss: 6828.2559 - val_root_mean_squared_error: 82.6333 - lr: 0.0010\n",
      "Epoch 15/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 8129.7676 - root_mean_squared_error: 90.1652\n",
      "Epoch 15: val_loss improved from 6828.25586 to 3892.49194, saving model to Model\\015-5942.5015-3892.4919.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5942.5015 - root_mean_squared_error: 77.0876 - val_loss: 3892.4919 - val_root_mean_squared_error: 62.3898 - lr: 0.0010\n",
      "Epoch 16/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 4716.1606 - root_mean_squared_error: 68.6743\n",
      "Epoch 16: val_loss improved from 3892.49194 to 2138.05762, saving model to Model\\016-3317.8264-2138.0576.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3317.8264 - root_mean_squared_error: 57.6006 - val_loss: 2138.0576 - val_root_mean_squared_error: 46.2391 - lr: 0.0010\n",
      "Epoch 17/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 2152.0632 - root_mean_squared_error: 46.3903\n",
      "Epoch 17: val_loss improved from 2138.05762 to 1299.50378, saving model to Model\\017-1827.3722-1299.5038.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1827.3722 - root_mean_squared_error: 42.7478 - val_loss: 1299.5038 - val_root_mean_squared_error: 36.0486 - lr: 0.0010\n",
      "Epoch 18/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 1511.1152 - root_mean_squared_error: 38.8731\n",
      "Epoch 18: val_loss improved from 1299.50378 to 980.70544, saving model to Model\\018-1165.6864-980.7054.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1165.6864 - root_mean_squared_error: 34.1422 - val_loss: 980.7054 - val_root_mean_squared_error: 31.3162 - lr: 0.0010\n",
      "Epoch 19/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 1026.9027 - root_mean_squared_error: 32.0453\n",
      "Epoch 19: val_loss improved from 980.70544 to 891.67072, saving model to Model\\019-914.7594-891.6707.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 914.7594 - root_mean_squared_error: 30.2450 - val_loss: 891.6707 - val_root_mean_squared_error: 29.8609 - lr: 0.0010\n",
      "Epoch 20/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 757.3411 - root_mean_squared_error: 27.5198\n",
      "Epoch 20: val_loss improved from 891.67072 to 870.76056, saving model to Model\\020-839.7842-870.7606.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 839.7842 - root_mean_squared_error: 28.9790 - val_loss: 870.7606 - val_root_mean_squared_error: 29.5087 - lr: 0.0010\n",
      "Epoch 21/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 745.5372 - root_mean_squared_error: 27.3045\n",
      "Epoch 21: val_loss improved from 870.76056 to 861.70294, saving model to Model\\021-819.0128-861.7029.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 819.0128 - root_mean_squared_error: 28.6184 - val_loss: 861.7029 - val_root_mean_squared_error: 29.3548 - lr: 0.0010\n",
      "Epoch 22/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 632.1540 - root_mean_squared_error: 25.1427\n",
      "Epoch 22: val_loss improved from 861.70294 to 853.15631, saving model to Model\\022-809.6612-853.1563.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 809.6612 - root_mean_squared_error: 28.4545 - val_loss: 853.1563 - val_root_mean_squared_error: 29.2088 - lr: 0.0010\n",
      "Epoch 23/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 626.1164 - root_mean_squared_error: 25.0223\n",
      "Epoch 23: val_loss improved from 853.15631 to 841.24683, saving model to Model\\023-799.9535-841.2468.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 799.9535 - root_mean_squared_error: 28.2834 - val_loss: 841.2468 - val_root_mean_squared_error: 29.0043 - lr: 0.0010\n",
      "Epoch 24/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 484.0887 - root_mean_squared_error: 22.0020\n",
      "Epoch 24: val_loss improved from 841.24683 to 829.75946, saving model to Model\\024-790.6129-829.7595.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 790.6129 - root_mean_squared_error: 28.1178 - val_loss: 829.7595 - val_root_mean_squared_error: 28.8055 - lr: 0.0010\n",
      "Epoch 25/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 779.5755 - root_mean_squared_error: 27.9209\n",
      "Epoch 25: val_loss improved from 829.75946 to 817.18469, saving model to Model\\025-781.5910-817.1847.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 781.5910 - root_mean_squared_error: 27.9569 - val_loss: 817.1847 - val_root_mean_squared_error: 28.5864 - lr: 0.0010\n",
      "Epoch 26/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 645.4038 - root_mean_squared_error: 25.4048\n",
      "Epoch 26: val_loss improved from 817.18469 to 804.81165, saving model to Model\\026-772.3777-804.8116.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 772.3777 - root_mean_squared_error: 27.7917 - val_loss: 804.8116 - val_root_mean_squared_error: 28.3692 - lr: 0.0010\n",
      "Epoch 27/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 827.2143 - root_mean_squared_error: 28.7613\n",
      "Epoch 27: val_loss improved from 804.81165 to 793.98615, saving model to Model\\027-763.3694-793.9861.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 763.3694 - root_mean_squared_error: 27.6291 - val_loss: 793.9861 - val_root_mean_squared_error: 28.1778 - lr: 0.0010\n",
      "Epoch 28/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 951.4725 - root_mean_squared_error: 30.8459\n",
      "Epoch 28: val_loss improved from 793.98615 to 781.09943, saving model to Model\\028-754.3080-781.0994.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 754.3080 - root_mean_squared_error: 27.4647 - val_loss: 781.0994 - val_root_mean_squared_error: 27.9482 - lr: 0.0010\n",
      "Epoch 29/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 894.1075 - root_mean_squared_error: 29.9016\n",
      "Epoch 29: val_loss improved from 781.09943 to 768.85522, saving model to Model\\029-745.1602-768.8552.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 745.1602 - root_mean_squared_error: 27.2976 - val_loss: 768.8552 - val_root_mean_squared_error: 27.7282 - lr: 0.0010\n",
      "Epoch 30/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 645.3477 - root_mean_squared_error: 25.4037\n",
      "Epoch 30: val_loss improved from 768.85522 to 758.04810, saving model to Model\\030-735.5425-758.0481.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 735.5425 - root_mean_squared_error: 27.1209 - val_loss: 758.0481 - val_root_mean_squared_error: 27.5327 - lr: 0.0010\n",
      "Epoch 31/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 528.2014 - root_mean_squared_error: 22.9826\n",
      "Epoch 31: val_loss improved from 758.04810 to 747.06134, saving model to Model\\031-726.7924-747.0613.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 726.7924 - root_mean_squared_error: 26.9591 - val_loss: 747.0613 - val_root_mean_squared_error: 27.3324 - lr: 0.0010\n",
      "Epoch 32/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 836.1052 - root_mean_squared_error: 28.9155\n",
      "Epoch 32: val_loss improved from 747.06134 to 734.89453, saving model to Model\\032-717.7808-734.8945.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 717.7808 - root_mean_squared_error: 26.7914 - val_loss: 734.8945 - val_root_mean_squared_error: 27.1089 - lr: 0.0010\n",
      "Epoch 33/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 1143.5946 - root_mean_squared_error: 33.8171\n",
      "Epoch 33: val_loss improved from 734.89453 to 723.12616, saving model to Model\\033-708.8079-723.1262.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 708.8079 - root_mean_squared_error: 26.6234 - val_loss: 723.1262 - val_root_mean_squared_error: 26.8910 - lr: 0.0010\n",
      "Epoch 34/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 844.4631 - root_mean_squared_error: 29.0596\n",
      "Epoch 34: val_loss improved from 723.12616 to 711.84747, saving model to Model\\034-699.7018-711.8475.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 699.7018 - root_mean_squared_error: 26.4519 - val_loss: 711.8475 - val_root_mean_squared_error: 26.6805 - lr: 0.0010\n",
      "Epoch 35/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 639.7318 - root_mean_squared_error: 25.2929\n",
      "Epoch 35: val_loss improved from 711.84747 to 699.96844, saving model to Model\\035-690.9948-699.9684.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 690.9948 - root_mean_squared_error: 26.2868 - val_loss: 699.9684 - val_root_mean_squared_error: 26.4569 - lr: 0.0010\n",
      "Epoch 36/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 681.5223 - root_mean_squared_error: 26.1060\n",
      "Epoch 36: val_loss improved from 699.96844 to 689.39740, saving model to Model\\036-682.0364-689.3974.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 682.0364 - root_mean_squared_error: 26.1158 - val_loss: 689.3974 - val_root_mean_squared_error: 26.2564 - lr: 0.0010\n",
      "Epoch 37/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 873.8265 - root_mean_squared_error: 29.5606\n",
      "Epoch 37: val_loss improved from 689.39740 to 677.06091, saving model to Model\\037-673.7072-677.0609.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 673.7072 - root_mean_squared_error: 25.9559 - val_loss: 677.0609 - val_root_mean_squared_error: 26.0204 - lr: 0.0010\n",
      "Epoch 38/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 710.3705 - root_mean_squared_error: 26.6528\n",
      "Epoch 38: val_loss improved from 677.06091 to 666.32794, saving model to Model\\038-664.6225-666.3279.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 664.6225 - root_mean_squared_error: 25.7803 - val_loss: 666.3279 - val_root_mean_squared_error: 25.8133 - lr: 0.0010\n",
      "Epoch 39/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 467.0421 - root_mean_squared_error: 21.6112\n",
      "Epoch 39: val_loss improved from 666.32794 to 655.22369, saving model to Model\\039-656.3673-655.2237.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 656.3673 - root_mean_squared_error: 25.6197 - val_loss: 655.2237 - val_root_mean_squared_error: 25.5973 - lr: 0.0010\n",
      "Epoch 40/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 636.0867 - root_mean_squared_error: 25.2208\n",
      "Epoch 40: val_loss improved from 655.22369 to 643.98096, saving model to Model\\040-648.2904-643.9810.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 648.2904 - root_mean_squared_error: 25.4615 - val_loss: 643.9810 - val_root_mean_squared_error: 25.3768 - lr: 0.0010\n",
      "Epoch 41/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 858.6356 - root_mean_squared_error: 29.3025\n",
      "Epoch 41: val_loss improved from 643.98096 to 633.14166, saving model to Model\\041-640.0086-633.1417.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 640.0086 - root_mean_squared_error: 25.2984 - val_loss: 633.1417 - val_root_mean_squared_error: 25.1623 - lr: 0.0010\n",
      "Epoch 42/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 574.1385 - root_mean_squared_error: 23.9612\n",
      "Epoch 42: val_loss improved from 633.14166 to 623.08240, saving model to Model\\042-631.9144-623.0824.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 631.9144 - root_mean_squared_error: 25.1379 - val_loss: 623.0824 - val_root_mean_squared_error: 24.9616 - lr: 0.0010\n",
      "Epoch 43/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 588.2338 - root_mean_squared_error: 24.2535\n",
      "Epoch 43: val_loss improved from 623.08240 to 612.42932, saving model to Model\\043-624.1584-612.4293.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 624.1584 - root_mean_squared_error: 24.9832 - val_loss: 612.4293 - val_root_mean_squared_error: 24.7473 - lr: 0.0010\n",
      "Epoch 44/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 783.9372 - root_mean_squared_error: 27.9989\n",
      "Epoch 44: val_loss improved from 612.42932 to 602.35468, saving model to Model\\044-616.5541-602.3547.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 616.5541 - root_mean_squared_error: 24.8305 - val_loss: 602.3547 - val_root_mean_squared_error: 24.5429 - lr: 0.0010\n",
      "Epoch 45/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 534.2531 - root_mean_squared_error: 23.1139\n",
      "Epoch 45: val_loss improved from 602.35468 to 591.70337, saving model to Model\\045-609.0095-591.7034.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 609.0095 - root_mean_squared_error: 24.6781 - val_loss: 591.7034 - val_root_mean_squared_error: 24.3250 - lr: 0.0010\n",
      "Epoch 46/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 494.9210 - root_mean_squared_error: 22.2468\n",
      "Epoch 46: val_loss improved from 591.70337 to 581.87384, saving model to Model\\046-601.8881-581.8738.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 601.8881 - root_mean_squared_error: 24.5334 - val_loss: 581.8738 - val_root_mean_squared_error: 24.1221 - lr: 0.0010\n",
      "Epoch 47/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 486.3461 - root_mean_squared_error: 22.0533\n",
      "Epoch 47: val_loss improved from 581.87384 to 571.21478, saving model to Model\\047-594.1494-571.2148.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 594.1494 - root_mean_squared_error: 24.3752 - val_loss: 571.2148 - val_root_mean_squared_error: 23.9001 - lr: 0.0010\n",
      "Epoch 48/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 526.8309 - root_mean_squared_error: 22.9528\n",
      "Epoch 48: val_loss improved from 571.21478 to 561.87811, saving model to Model\\048-586.3689-561.8781.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 586.3689 - root_mean_squared_error: 24.2151 - val_loss: 561.8781 - val_root_mean_squared_error: 23.7040 - lr: 0.0010\n",
      "Epoch 49/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 593.6359 - root_mean_squared_error: 24.3646\n",
      "Epoch 49: val_loss improved from 561.87811 to 552.59143, saving model to Model\\049-579.4186-552.5914.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 579.4186 - root_mean_squared_error: 24.0711 - val_loss: 552.5914 - val_root_mean_squared_error: 23.5073 - lr: 0.0010\n",
      "Epoch 50/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 654.5993 - root_mean_squared_error: 25.5851\n",
      "Epoch 50: val_loss improved from 552.59143 to 542.92987, saving model to Model\\050-572.4338-542.9299.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 572.4338 - root_mean_squared_error: 23.9256 - val_loss: 542.9299 - val_root_mean_squared_error: 23.3009 - lr: 0.0010\n",
      "Epoch 51/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 485.0225 - root_mean_squared_error: 22.0232\n",
      "Epoch 51: val_loss improved from 542.92987 to 533.54926, saving model to Model\\051-565.7347-533.5493.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 565.7347 - root_mean_squared_error: 23.7852 - val_loss: 533.5493 - val_root_mean_squared_error: 23.0987 - lr: 0.0010\n",
      "Epoch 52/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 569.5923 - root_mean_squared_error: 23.8661\n",
      "Epoch 52: val_loss improved from 533.54926 to 524.34058, saving model to Model\\052-559.3886-524.3406.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 559.3886 - root_mean_squared_error: 23.6514 - val_loss: 524.3406 - val_root_mean_squared_error: 22.8985 - lr: 0.0010\n",
      "Epoch 53/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 399.6763 - root_mean_squared_error: 19.9919\n",
      "Epoch 53: val_loss improved from 524.34058 to 515.47620, saving model to Model\\053-552.8297-515.4762.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 552.8297 - root_mean_squared_error: 23.5123 - val_loss: 515.4762 - val_root_mean_squared_error: 22.7041 - lr: 0.0010\n",
      "Epoch 54/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 673.1131 - root_mean_squared_error: 25.9444\n",
      "Epoch 54: val_loss improved from 515.47620 to 506.83051, saving model to Model\\054-546.3722-506.8305.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 546.3722 - root_mean_squared_error: 23.3746 - val_loss: 506.8305 - val_root_mean_squared_error: 22.5129 - lr: 0.0010\n",
      "Epoch 55/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 638.9805 - root_mean_squared_error: 25.2781\n",
      "Epoch 55: val_loss improved from 506.83051 to 498.57642, saving model to Model\\055-540.1450-498.5764.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 540.1450 - root_mean_squared_error: 23.2410 - val_loss: 498.5764 - val_root_mean_squared_error: 22.3288 - lr: 0.0010\n",
      "Epoch 56/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 585.3106 - root_mean_squared_error: 24.1932\n",
      "Epoch 56: val_loss improved from 498.57642 to 490.46292, saving model to Model\\056-534.7312-490.4629.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 534.7312 - root_mean_squared_error: 23.1243 - val_loss: 490.4629 - val_root_mean_squared_error: 22.1464 - lr: 0.0010\n",
      "Epoch 57/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 596.2502 - root_mean_squared_error: 24.4182\n",
      "Epoch 57: val_loss improved from 490.46292 to 481.64117, saving model to Model\\057-528.7433-481.6412.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 528.7433 - root_mean_squared_error: 22.9944 - val_loss: 481.6412 - val_root_mean_squared_error: 21.9463 - lr: 0.0010\n",
      "Epoch 58/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 566.8672 - root_mean_squared_error: 23.8090\n",
      "Epoch 58: val_loss improved from 481.64117 to 473.73578, saving model to Model\\058-523.2103-473.7358.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 523.2103 - root_mean_squared_error: 22.8738 - val_loss: 473.7358 - val_root_mean_squared_error: 21.7655 - lr: 0.0010\n",
      "Epoch 59/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 402.8441 - root_mean_squared_error: 20.0710\n",
      "Epoch 59: val_loss improved from 473.73578 to 465.69797, saving model to Model\\059-517.3281-465.6980.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 517.3281 - root_mean_squared_error: 22.7448 - val_loss: 465.6980 - val_root_mean_squared_error: 21.5800 - lr: 0.0010\n",
      "Epoch 60/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 753.6470 - root_mean_squared_error: 27.4526\n",
      "Epoch 60: val_loss improved from 465.69797 to 458.15247, saving model to Model\\060-511.8593-458.1525.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 511.8593 - root_mean_squared_error: 22.6243 - val_loss: 458.1525 - val_root_mean_squared_error: 21.4045 - lr: 0.0010\n",
      "Epoch 61/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 606.5872 - root_mean_squared_error: 24.6290\n",
      "Epoch 61: val_loss improved from 458.15247 to 450.79279, saving model to Model\\061-506.5882-450.7928.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 506.5882 - root_mean_squared_error: 22.5075 - val_loss: 450.7928 - val_root_mean_squared_error: 21.2319 - lr: 0.0010\n",
      "Epoch 62/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 461.2913 - root_mean_squared_error: 21.4777\n",
      "Epoch 62: val_loss improved from 450.79279 to 443.40488, saving model to Model\\062-501.0608-443.4049.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 501.0608 - root_mean_squared_error: 22.3844 - val_loss: 443.4049 - val_root_mean_squared_error: 21.0572 - lr: 0.0010\n",
      "Epoch 63/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 613.1325 - root_mean_squared_error: 24.7615\n",
      "Epoch 63: val_loss improved from 443.40488 to 435.84891, saving model to Model\\063-497.3755-435.8489.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 497.3755 - root_mean_squared_error: 22.3019 - val_loss: 435.8489 - val_root_mean_squared_error: 20.8770 - lr: 0.0010\n",
      "Epoch 64/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 451.0391 - root_mean_squared_error: 21.2377\n",
      "Epoch 64: val_loss improved from 435.84891 to 429.25836, saving model to Model\\064-492.5018-429.2584.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 492.5018 - root_mean_squared_error: 22.1924 - val_loss: 429.2584 - val_root_mean_squared_error: 20.7185 - lr: 0.0010\n",
      "Epoch 65/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 478.8109 - root_mean_squared_error: 21.8817\n",
      "Epoch 65: val_loss improved from 429.25836 to 421.92313, saving model to Model\\065-486.8188-421.9231.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 486.8188 - root_mean_squared_error: 22.0640 - val_loss: 421.9231 - val_root_mean_squared_error: 20.5408 - lr: 0.0010\n",
      "Epoch 66/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 369.0212 - root_mean_squared_error: 19.2099\n",
      "Epoch 66: val_loss improved from 421.92313 to 415.49280, saving model to Model\\066-482.1520-415.4928.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 482.1520 - root_mean_squared_error: 21.9580 - val_loss: 415.4928 - val_root_mean_squared_error: 20.3836 - lr: 0.0010\n",
      "Epoch 67/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 488.0027 - root_mean_squared_error: 22.0908\n",
      "Epoch 67: val_loss improved from 415.49280 to 409.42380, saving model to Model\\067-478.0509-409.4238.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 478.0509 - root_mean_squared_error: 21.8644 - val_loss: 409.4238 - val_root_mean_squared_error: 20.2342 - lr: 0.0010\n",
      "Epoch 68/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 542.2639 - root_mean_squared_error: 23.2866\n",
      "Epoch 68: val_loss improved from 409.42380 to 402.60522, saving model to Model\\068-473.7898-402.6052.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 473.7898 - root_mean_squared_error: 21.7667 - val_loss: 402.6052 - val_root_mean_squared_error: 20.0650 - lr: 0.0010\n",
      "Epoch 69/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 352.2360 - root_mean_squared_error: 18.7680\n",
      "Epoch 69: val_loss improved from 402.60522 to 396.74866, saving model to Model\\069-469.1408-396.7487.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 469.1408 - root_mean_squared_error: 21.6597 - val_loss: 396.7487 - val_root_mean_squared_error: 19.9186 - lr: 0.0010\n",
      "Epoch 70/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 219.4672 - root_mean_squared_error: 14.8144\n",
      "Epoch 70: val_loss improved from 396.74866 to 390.68350, saving model to Model\\070-465.2464-390.6835.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 465.2464 - root_mean_squared_error: 21.5696 - val_loss: 390.6835 - val_root_mean_squared_error: 19.7657 - lr: 0.0010\n",
      "Epoch 71/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 274.5476 - root_mean_squared_error: 16.5695\n",
      "Epoch 71: val_loss improved from 390.68350 to 385.04022, saving model to Model\\071-461.3773-385.0402.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 461.3773 - root_mean_squared_error: 21.4797 - val_loss: 385.0402 - val_root_mean_squared_error: 19.6224 - lr: 0.0010\n",
      "Epoch 72/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 735.2202 - root_mean_squared_error: 27.1149\n",
      "Epoch 72: val_loss improved from 385.04022 to 379.15317, saving model to Model\\072-458.0493-379.1532.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 458.0493 - root_mean_squared_error: 21.4021 - val_loss: 379.1532 - val_root_mean_squared_error: 19.4719 - lr: 0.0010\n",
      "Epoch 73/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 281.2584 - root_mean_squared_error: 16.7708\n",
      "Epoch 73: val_loss improved from 379.15317 to 373.77304, saving model to Model\\073-454.0372-373.7730.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 454.0372 - root_mean_squared_error: 21.3081 - val_loss: 373.7730 - val_root_mean_squared_error: 19.3332 - lr: 0.0010\n",
      "Epoch 74/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 480.3415 - root_mean_squared_error: 21.9167\n",
      "Epoch 74: val_loss improved from 373.77304 to 368.24875, saving model to Model\\074-450.5471-368.2487.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 450.5471 - root_mean_squared_error: 21.2261 - val_loss: 368.2487 - val_root_mean_squared_error: 19.1898 - lr: 0.0010\n",
      "Epoch 75/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 459.2276 - root_mean_squared_error: 21.4296\n",
      "Epoch 75: val_loss improved from 368.24875 to 363.29962, saving model to Model\\075-447.0148-363.2996.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 447.0148 - root_mean_squared_error: 21.1427 - val_loss: 363.2996 - val_root_mean_squared_error: 19.0604 - lr: 0.0010\n",
      "Epoch 76/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 369.8163 - root_mean_squared_error: 19.2306\n",
      "Epoch 76: val_loss improved from 363.29962 to 357.89117, saving model to Model\\076-443.8344-357.8912.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 443.8344 - root_mean_squared_error: 21.0674 - val_loss: 357.8912 - val_root_mean_squared_error: 18.9180 - lr: 0.0010\n",
      "Epoch 77/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 395.9328 - root_mean_squared_error: 19.8981\n",
      "Epoch 77: val_loss improved from 357.89117 to 352.60028, saving model to Model\\077-441.0072-352.6003.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 441.0072 - root_mean_squared_error: 21.0002 - val_loss: 352.6003 - val_root_mean_squared_error: 18.7777 - lr: 0.0010\n",
      "Epoch 78/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 598.9610 - root_mean_squared_error: 24.4737\n",
      "Epoch 78: val_loss improved from 352.60028 to 348.32245, saving model to Model\\078-437.7264-348.3224.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 437.7264 - root_mean_squared_error: 20.9219 - val_loss: 348.3224 - val_root_mean_squared_error: 18.6634 - lr: 0.0010\n",
      "Epoch 79/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 235.4213 - root_mean_squared_error: 15.3434\n",
      "Epoch 79: val_loss improved from 348.32245 to 343.60953, saving model to Model\\079-434.5193-343.6095.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 434.5193 - root_mean_squared_error: 20.8451 - val_loss: 343.6095 - val_root_mean_squared_error: 18.5367 - lr: 0.0010\n",
      "Epoch 80/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 257.7870 - root_mean_squared_error: 16.0557\n",
      "Epoch 80: val_loss improved from 343.60953 to 339.14218, saving model to Model\\080-432.0668-339.1422.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 432.0668 - root_mean_squared_error: 20.7862 - val_loss: 339.1422 - val_root_mean_squared_error: 18.4158 - lr: 0.0010\n",
      "Epoch 81/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 523.2335 - root_mean_squared_error: 22.8743\n",
      "Epoch 81: val_loss improved from 339.14218 to 333.90390, saving model to Model\\081-428.8570-333.9039.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 428.8570 - root_mean_squared_error: 20.7089 - val_loss: 333.9039 - val_root_mean_squared_error: 18.2730 - lr: 0.0010\n",
      "Epoch 82/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 315.1178 - root_mean_squared_error: 17.7516\n",
      "Epoch 82: val_loss improved from 333.90390 to 329.69156, saving model to Model\\082-426.2916-329.6916.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 426.2916 - root_mean_squared_error: 20.6468 - val_loss: 329.6916 - val_root_mean_squared_error: 18.1574 - lr: 0.0010\n",
      "Epoch 83/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 355.0278 - root_mean_squared_error: 18.8422\n",
      "Epoch 83: val_loss improved from 329.69156 to 326.11365, saving model to Model\\083-423.7294-326.1136.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 423.7294 - root_mean_squared_error: 20.5847 - val_loss: 326.1136 - val_root_mean_squared_error: 18.0586 - lr: 0.0010\n",
      "Epoch 84/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 516.4056 - root_mean_squared_error: 22.7246\n",
      "Epoch 84: val_loss improved from 326.11365 to 321.35858, saving model to Model\\084-421.3253-321.3586.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 421.3253 - root_mean_squared_error: 20.5262 - val_loss: 321.3586 - val_root_mean_squared_error: 17.9265 - lr: 0.0010\n",
      "Epoch 85/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 642.2571 - root_mean_squared_error: 25.3428\n",
      "Epoch 85: val_loss improved from 321.35858 to 318.20969, saving model to Model\\085-418.9280-318.2097.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 418.9280 - root_mean_squared_error: 20.4677 - val_loss: 318.2097 - val_root_mean_squared_error: 17.8384 - lr: 0.0010\n",
      "Epoch 86/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 344.9213 - root_mean_squared_error: 18.5721\n",
      "Epoch 86: val_loss improved from 318.20969 to 313.71655, saving model to Model\\086-416.2955-313.7166.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 416.2955 - root_mean_squared_error: 20.4033 - val_loss: 313.7166 - val_root_mean_squared_error: 17.7120 - lr: 0.0010\n",
      "Epoch 87/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 398.8348 - root_mean_squared_error: 19.9708\n",
      "Epoch 87: val_loss improved from 313.71655 to 309.83337, saving model to Model\\087-414.1411-309.8334.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 414.1411 - root_mean_squared_error: 20.3505 - val_loss: 309.8334 - val_root_mean_squared_error: 17.6021 - lr: 0.0010\n",
      "Epoch 88/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 359.6000 - root_mean_squared_error: 18.9631\n",
      "Epoch 88: val_loss improved from 309.83337 to 306.31339, saving model to Model\\088-412.0718-306.3134.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 412.0718 - root_mean_squared_error: 20.2996 - val_loss: 306.3134 - val_root_mean_squared_error: 17.5018 - lr: 0.0010\n",
      "Epoch 89/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 556.5077 - root_mean_squared_error: 23.5904\n",
      "Epoch 89: val_loss improved from 306.31339 to 303.66425, saving model to Model\\089-409.8604-303.6642.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 409.8604 - root_mean_squared_error: 20.2450 - val_loss: 303.6642 - val_root_mean_squared_error: 17.4260 - lr: 0.0010\n",
      "Epoch 90/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 605.9089 - root_mean_squared_error: 24.6152\n",
      "Epoch 90: val_loss improved from 303.66425 to 299.98907, saving model to Model\\090-407.8445-299.9891.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 407.8445 - root_mean_squared_error: 20.1952 - val_loss: 299.9891 - val_root_mean_squared_error: 17.3202 - lr: 0.0010\n",
      "Epoch 91/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 237.9676 - root_mean_squared_error: 15.4262\n",
      "Epoch 91: val_loss improved from 299.98907 to 296.17960, saving model to Model\\091-405.7268-296.1796.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 405.7268 - root_mean_squared_error: 20.1427 - val_loss: 296.1796 - val_root_mean_squared_error: 17.2099 - lr: 0.0010\n",
      "Epoch 92/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 529.4928 - root_mean_squared_error: 23.0107\n",
      "Epoch 92: val_loss improved from 296.17960 to 292.84839, saving model to Model\\092-403.9409-292.8484.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 403.9409 - root_mean_squared_error: 20.0983 - val_loss: 292.8484 - val_root_mean_squared_error: 17.1128 - lr: 0.0010\n",
      "Epoch 93/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 405.0763 - root_mean_squared_error: 20.1265\n",
      "Epoch 93: val_loss improved from 292.84839 to 289.22577, saving model to Model\\093-402.1198-289.2258.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 402.1198 - root_mean_squared_error: 20.0529 - val_loss: 289.2258 - val_root_mean_squared_error: 17.0066 - lr: 0.0010\n",
      "Epoch 94/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 724.6646 - root_mean_squared_error: 26.9196\n",
      "Epoch 94: val_loss improved from 289.22577 to 286.73322, saving model to Model\\094-400.7625-286.7332.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 400.7625 - root_mean_squared_error: 20.0191 - val_loss: 286.7332 - val_root_mean_squared_error: 16.9332 - lr: 0.0010\n",
      "Epoch 95/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 414.7306 - root_mean_squared_error: 20.3649\n",
      "Epoch 95: val_loss improved from 286.73322 to 283.58728, saving model to Model\\095-398.7084-283.5873.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 398.7084 - root_mean_squared_error: 19.9677 - val_loss: 283.5873 - val_root_mean_squared_error: 16.8400 - lr: 0.0010\n",
      "Epoch 96/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 241.9980 - root_mean_squared_error: 15.5563\n",
      "Epoch 96: val_loss improved from 283.58728 to 281.75812, saving model to Model\\096-397.7159-281.7581.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 397.7159 - root_mean_squared_error: 19.9428 - val_loss: 281.7581 - val_root_mean_squared_error: 16.7857 - lr: 0.0010\n",
      "Epoch 97/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 401.7917 - root_mean_squared_error: 20.0447\n",
      "Epoch 97: val_loss improved from 281.75812 to 277.66840, saving model to Model\\097-394.8883-277.6684.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 394.8883 - root_mean_squared_error: 19.8718 - val_loss: 277.6684 - val_root_mean_squared_error: 16.6634 - lr: 0.0010\n",
      "Epoch 98/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 494.0917 - root_mean_squared_error: 22.2282\n",
      "Epoch 98: val_loss improved from 277.66840 to 274.63910, saving model to Model\\098-393.7917-274.6391.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 393.7917 - root_mean_squared_error: 19.8442 - val_loss: 274.6391 - val_root_mean_squared_error: 16.5722 - lr: 0.0010\n",
      "Epoch 99/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 434.0584 - root_mean_squared_error: 20.8341\n",
      "Epoch 99: val_loss improved from 274.63910 to 272.42883, saving model to Model\\099-392.1235-272.4288.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 392.1235 - root_mean_squared_error: 19.8021 - val_loss: 272.4288 - val_root_mean_squared_error: 16.5054 - lr: 0.0010\n",
      "Epoch 100/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 332.8717 - root_mean_squared_error: 18.2448\n",
      "Epoch 100: val_loss improved from 272.42883 to 270.19180, saving model to Model\\100-391.1524-270.1918.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 391.1524 - root_mean_squared_error: 19.7776 - val_loss: 270.1918 - val_root_mean_squared_error: 16.4375 - lr: 0.0010\n",
      "Epoch 101/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 465.7494 - root_mean_squared_error: 21.5812\n",
      "Epoch 101: val_loss improved from 270.19180 to 266.37247, saving model to Model\\101-388.9369-266.3725.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.9369 - root_mean_squared_error: 19.7215 - val_loss: 266.3725 - val_root_mean_squared_error: 16.3209 - lr: 0.0010\n",
      "Epoch 102/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 397.0385 - root_mean_squared_error: 19.9258\n",
      "Epoch 102: val_loss improved from 266.37247 to 263.49451, saving model to Model\\102-388.0664-263.4945.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 388.0664 - root_mean_squared_error: 19.6994 - val_loss: 263.4945 - val_root_mean_squared_error: 16.2325 - lr: 0.0010\n",
      "Epoch 103/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 507.8382 - root_mean_squared_error: 22.5353\n",
      "Epoch 103: val_loss improved from 263.49451 to 262.33221, saving model to Model\\103-386.6809-262.3322.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 386.6809 - root_mean_squared_error: 19.6642 - val_loss: 262.3322 - val_root_mean_squared_error: 16.1967 - lr: 0.0010\n",
      "Epoch 104/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 403.6676 - root_mean_squared_error: 20.0915\n",
      "Epoch 104: val_loss improved from 262.33221 to 259.54364, saving model to Model\\104-385.9204-259.5436.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.9204 - root_mean_squared_error: 19.6449 - val_loss: 259.5436 - val_root_mean_squared_error: 16.1104 - lr: 0.0010\n",
      "Epoch 105/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 674.5079 - root_mean_squared_error: 25.9713\n",
      "Epoch 105: val_loss did not improve from 259.54364\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 385.8604 - root_mean_squared_error: 19.6433 - val_loss: 261.6040 - val_root_mean_squared_error: 16.1742 - lr: 0.0010\n",
      "Epoch 106/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 225.0910 - root_mean_squared_error: 15.0030\n",
      "Epoch 106: val_loss improved from 259.54364 to 255.59821, saving model to Model\\106-382.5052-255.5982.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 382.5052 - root_mean_squared_error: 19.5577 - val_loss: 255.5982 - val_root_mean_squared_error: 15.9874 - lr: 0.0010\n",
      "Epoch 107/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 289.1716 - root_mean_squared_error: 17.0050\n",
      "Epoch 107: val_loss improved from 255.59821 to 252.93085, saving model to Model\\107-381.3896-252.9308.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 381.3896 - root_mean_squared_error: 19.5292 - val_loss: 252.9308 - val_root_mean_squared_error: 15.9038 - lr: 0.0010\n",
      "Epoch 108/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 511.6361 - root_mean_squared_error: 22.6194\n",
      "Epoch 108: val_loss improved from 252.93085 to 250.75385, saving model to Model\\108-380.1076-250.7538.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 380.1076 - root_mean_squared_error: 19.4963 - val_loss: 250.7538 - val_root_mean_squared_error: 15.8352 - lr: 0.0010\n",
      "Epoch 109/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 482.2362 - root_mean_squared_error: 21.9599\n",
      "Epoch 109: val_loss improved from 250.75385 to 248.99602, saving model to Model\\109-379.2178-248.9960.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 379.2178 - root_mean_squared_error: 19.4735 - val_loss: 248.9960 - val_root_mean_squared_error: 15.7796 - lr: 0.0010\n",
      "Epoch 110/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 327.1196 - root_mean_squared_error: 18.0864\n",
      "Epoch 110: val_loss improved from 248.99602 to 247.96434, saving model to Model\\110-378.3656-247.9643.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.3656 - root_mean_squared_error: 19.4516 - val_loss: 247.9643 - val_root_mean_squared_error: 15.7469 - lr: 0.0010\n",
      "Epoch 111/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 216.9094 - root_mean_squared_error: 14.7278\n",
      "Epoch 111: val_loss improved from 247.96434 to 246.62158, saving model to Model\\111-378.1614-246.6216.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 378.1614 - root_mean_squared_error: 19.4464 - val_loss: 246.6216 - val_root_mean_squared_error: 15.7042 - lr: 0.0010\n",
      "Epoch 112/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 397.0846 - root_mean_squared_error: 19.9270\n",
      "Epoch 112: val_loss improved from 246.62158 to 242.30316, saving model to Model\\112-375.5166-242.3032.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 375.5166 - root_mean_squared_error: 19.3783 - val_loss: 242.3032 - val_root_mean_squared_error: 15.5661 - lr: 0.0010\n",
      "Epoch 113/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 235.1167 - root_mean_squared_error: 15.3335\n",
      "Epoch 113: val_loss improved from 242.30316 to 240.54814, saving model to Model\\113-374.9185-240.5481.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 374.9185 - root_mean_squared_error: 19.3628 - val_loss: 240.5481 - val_root_mean_squared_error: 15.5096 - lr: 0.0010\n",
      "Epoch 114/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 181.9250 - root_mean_squared_error: 13.4880\n",
      "Epoch 114: val_loss did not improve from 240.54814\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 373.8638 - root_mean_squared_error: 19.3356 - val_loss: 240.6279 - val_root_mean_squared_error: 15.5122 - lr: 0.0010\n",
      "Epoch 115/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 243.8884 - root_mean_squared_error: 15.6169\n",
      "Epoch 115: val_loss improved from 240.54814 to 238.55028, saving model to Model\\115-372.7219-238.5503.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.7219 - root_mean_squared_error: 19.3060 - val_loss: 238.5503 - val_root_mean_squared_error: 15.4451 - lr: 0.0010\n",
      "Epoch 116/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 299.1543 - root_mean_squared_error: 17.2961\n",
      "Epoch 116: val_loss improved from 238.55028 to 234.59721, saving model to Model\\116-372.0509-234.5972.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 372.0509 - root_mean_squared_error: 19.2886 - val_loss: 234.5972 - val_root_mean_squared_error: 15.3166 - lr: 0.0010\n",
      "Epoch 117/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 423.6837 - root_mean_squared_error: 20.5836\n",
      "Epoch 117: val_loss did not improve from 234.59721\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 370.5468 - root_mean_squared_error: 19.2496 - val_loss: 235.8521 - val_root_mean_squared_error: 15.3575 - lr: 0.0010\n",
      "Epoch 118/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 214.6362 - root_mean_squared_error: 14.6505\n",
      "Epoch 118: val_loss improved from 234.59721 to 233.07414, saving model to Model\\118-369.4359-233.0741.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 369.4359 - root_mean_squared_error: 19.2207 - val_loss: 233.0741 - val_root_mean_squared_error: 15.2668 - lr: 0.0010\n",
      "Epoch 119/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 262.8205 - root_mean_squared_error: 16.2117\n",
      "Epoch 119: val_loss improved from 233.07414 to 230.68629, saving model to Model\\119-368.6180-230.6863.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 368.6180 - root_mean_squared_error: 19.1994 - val_loss: 230.6863 - val_root_mean_squared_error: 15.1884 - lr: 0.0010\n",
      "Epoch 120/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 146.8745 - root_mean_squared_error: 12.1192\n",
      "Epoch 120: val_loss improved from 230.68629 to 229.53493, saving model to Model\\120-367.4325-229.5349.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.4325 - root_mean_squared_error: 19.1685 - val_loss: 229.5349 - val_root_mean_squared_error: 15.1504 - lr: 0.0010\n",
      "Epoch 121/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 372.7152 - root_mean_squared_error: 19.3058\n",
      "Epoch 121: val_loss improved from 229.53493 to 225.60654, saving model to Model\\121-367.8174-225.6065.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 367.8174 - root_mean_squared_error: 19.1786 - val_loss: 225.6065 - val_root_mean_squared_error: 15.0202 - lr: 0.0010\n",
      "Epoch 122/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 276.9322 - root_mean_squared_error: 16.6413\n",
      "Epoch 122: val_loss did not improve from 225.60654\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 366.4801 - root_mean_squared_error: 19.1437 - val_loss: 228.2208 - val_root_mean_squared_error: 15.1070 - lr: 0.0010\n",
      "Epoch 123/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 292.5853 - root_mean_squared_error: 17.1051\n",
      "Epoch 123: val_loss improved from 225.60654 to 222.65388, saving model to Model\\123-365.4419-222.6539.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 365.4419 - root_mean_squared_error: 19.1165 - val_loss: 222.6539 - val_root_mean_squared_error: 14.9216 - lr: 0.0010\n",
      "Epoch 124/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 709.6836 - root_mean_squared_error: 26.6399\n",
      "Epoch 124: val_loss did not improve from 222.65388\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 363.6168 - root_mean_squared_error: 19.0687 - val_loss: 223.2794 - val_root_mean_squared_error: 14.9425 - lr: 0.0010\n",
      "Epoch 125/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 319.6292 - root_mean_squared_error: 17.8782\n",
      "Epoch 125: val_loss improved from 222.65388 to 222.04068, saving model to Model\\125-362.7252-222.0407.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 362.7252 - root_mean_squared_error: 19.0453 - val_loss: 222.0407 - val_root_mean_squared_error: 14.9010 - lr: 0.0010\n",
      "Epoch 126/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 570.3943 - root_mean_squared_error: 23.8829\n",
      "Epoch 126: val_loss improved from 222.04068 to 218.57484, saving model to Model\\126-361.9855-218.5748.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 361.9855 - root_mean_squared_error: 19.0259 - val_loss: 218.5748 - val_root_mean_squared_error: 14.7843 - lr: 0.0010\n",
      "Epoch 127/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 606.2452 - root_mean_squared_error: 24.6220\n",
      "Epoch 127: val_loss improved from 218.57484 to 218.07106, saving model to Model\\127-360.9168-218.0711.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 360.9168 - root_mean_squared_error: 18.9978 - val_loss: 218.0711 - val_root_mean_squared_error: 14.7672 - lr: 0.0010\n",
      "Epoch 128/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 650.4701 - root_mean_squared_error: 25.5043\n",
      "Epoch 128: val_loss improved from 218.07106 to 217.19794, saving model to Model\\128-360.1033-217.1979.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.1033 - root_mean_squared_error: 18.9764 - val_loss: 217.1979 - val_root_mean_squared_error: 14.7376 - lr: 0.0010\n",
      "Epoch 129/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 294.1074 - root_mean_squared_error: 17.1496\n",
      "Epoch 129: val_loss improved from 217.19794 to 213.22746, saving model to Model\\129-360.3190-213.2275.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 360.3190 - root_mean_squared_error: 18.9821 - val_loss: 213.2275 - val_root_mean_squared_error: 14.6023 - lr: 0.0010\n",
      "Epoch 130/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 299.9266 - root_mean_squared_error: 17.3184\n",
      "Epoch 130: val_loss did not improve from 213.22746\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 358.9757 - root_mean_squared_error: 18.9467 - val_loss: 216.7308 - val_root_mean_squared_error: 14.7218 - lr: 0.0010\n",
      "Epoch 131/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 279.4256 - root_mean_squared_error: 16.7160\n",
      "Epoch 131: val_loss improved from 213.22746 to 211.54420, saving model to Model\\131-357.1059-211.5442.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 357.1059 - root_mean_squared_error: 18.8972 - val_loss: 211.5442 - val_root_mean_squared_error: 14.5446 - lr: 0.0010\n",
      "Epoch 132/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 242.3251 - root_mean_squared_error: 15.5668\n",
      "Epoch 132: val_loss improved from 211.54420 to 210.41896, saving model to Model\\132-356.3478-210.4190.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 356.3478 - root_mean_squared_error: 18.8772 - val_loss: 210.4190 - val_root_mean_squared_error: 14.5058 - lr: 0.0010\n",
      "Epoch 133/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 370.1642 - root_mean_squared_error: 19.2397\n",
      "Epoch 133: val_loss improved from 210.41896 to 206.83812, saving model to Model\\133-355.5045-206.8381.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 355.5045 - root_mean_squared_error: 18.8548 - val_loss: 206.8381 - val_root_mean_squared_error: 14.3819 - lr: 0.0010\n",
      "Epoch 134/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 238.8677 - root_mean_squared_error: 15.4553\n",
      "Epoch 134: val_loss did not improve from 206.83812\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 354.5944 - root_mean_squared_error: 18.8307 - val_loss: 209.1061 - val_root_mean_squared_error: 14.4605 - lr: 0.0010\n",
      "Epoch 135/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 353.5726 - root_mean_squared_error: 18.8035\n",
      "Epoch 135: val_loss improved from 206.83812 to 205.93413, saving model to Model\\135-353.4936-205.9341.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 353.4936 - root_mean_squared_error: 18.8014 - val_loss: 205.9341 - val_root_mean_squared_error: 14.3504 - lr: 0.0010\n",
      "Epoch 136/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 296.7034 - root_mean_squared_error: 17.2251\n",
      "Epoch 136: val_loss improved from 205.93413 to 204.49779, saving model to Model\\136-352.6990-204.4978.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 352.6990 - root_mean_squared_error: 18.7803 - val_loss: 204.4978 - val_root_mean_squared_error: 14.3003 - lr: 0.0010\n",
      "Epoch 137/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 486.2804 - root_mean_squared_error: 22.0518\n",
      "Epoch 137: val_loss did not improve from 204.49779\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 351.7883 - root_mean_squared_error: 18.7560 - val_loss: 204.7838 - val_root_mean_squared_error: 14.3103 - lr: 0.0010\n",
      "Epoch 138/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 240.7669 - root_mean_squared_error: 15.5167\n",
      "Epoch 138: val_loss improved from 204.49779 to 201.89308, saving model to Model\\138-351.1166-201.8931.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 351.1166 - root_mean_squared_error: 18.7381 - val_loss: 201.8931 - val_root_mean_squared_error: 14.2089 - lr: 0.0010\n",
      "Epoch 139/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 89.7035 - root_mean_squared_error: 9.4712\n",
      "Epoch 139: val_loss improved from 201.89308 to 201.39661, saving model to Model\\139-351.1147-201.3966.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 351.1147 - root_mean_squared_error: 18.7381 - val_loss: 201.3966 - val_root_mean_squared_error: 14.1914 - lr: 0.0010\n",
      "Epoch 140/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 317.6559 - root_mean_squared_error: 17.8229\n",
      "Epoch 140: val_loss improved from 201.39661 to 198.91777, saving model to Model\\140-349.2126-198.9178.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 349.2126 - root_mean_squared_error: 18.6872 - val_loss: 198.9178 - val_root_mean_squared_error: 14.1038 - lr: 0.0010\n",
      "Epoch 141/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 381.0055 - root_mean_squared_error: 19.5194\n",
      "Epoch 141: val_loss improved from 198.91777 to 197.58957, saving model to Model\\141-348.4120-197.5896.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 348.4120 - root_mean_squared_error: 18.6658 - val_loss: 197.5896 - val_root_mean_squared_error: 14.0567 - lr: 0.0010\n",
      "Epoch 142/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 184.4106 - root_mean_squared_error: 13.5798\n",
      "Epoch 142: val_loss improved from 197.58957 to 195.99286, saving model to Model\\142-347.9237-195.9929.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.9237 - root_mean_squared_error: 18.6527 - val_loss: 195.9929 - val_root_mean_squared_error: 13.9997 - lr: 0.0010\n",
      "Epoch 143/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 388.6292 - root_mean_squared_error: 19.7137\n",
      "Epoch 143: val_loss improved from 195.99286 to 194.81561, saving model to Model\\143-347.7780-194.8156.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 347.7780 - root_mean_squared_error: 18.6488 - val_loss: 194.8156 - val_root_mean_squared_error: 13.9576 - lr: 0.0010\n",
      "Epoch 144/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 612.6172 - root_mean_squared_error: 24.7511\n",
      "Epoch 144: val_loss did not improve from 194.81561\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 346.9289 - root_mean_squared_error: 18.6260 - val_loss: 197.9181 - val_root_mean_squared_error: 14.0683 - lr: 0.0010\n",
      "Epoch 145/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 265.2730 - root_mean_squared_error: 16.2872\n",
      "Epoch 145: val_loss improved from 194.81561 to 191.73103, saving model to Model\\145-344.4018-191.7310.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.4018 - root_mean_squared_error: 18.5581 - val_loss: 191.7310 - val_root_mean_squared_error: 13.8467 - lr: 0.0010\n",
      "Epoch 146/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 607.6962 - root_mean_squared_error: 24.6515\n",
      "Epoch 146: val_loss improved from 191.73103 to 190.19093, saving model to Model\\146-344.1983-190.1909.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.1983 - root_mean_squared_error: 18.5526 - val_loss: 190.1909 - val_root_mean_squared_error: 13.7910 - lr: 0.0010\n",
      "Epoch 147/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 287.2515 - root_mean_squared_error: 16.9485\n",
      "Epoch 147: val_loss improved from 190.19093 to 188.99321, saving model to Model\\147-344.3546-188.9932.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 344.3546 - root_mean_squared_error: 18.5568 - val_loss: 188.9932 - val_root_mean_squared_error: 13.7475 - lr: 0.0010\n",
      "Epoch 148/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 522.6818 - root_mean_squared_error: 22.8622\n",
      "Epoch 148: val_loss did not improve from 188.99321\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 342.1915 - root_mean_squared_error: 18.4984 - val_loss: 190.0166 - val_root_mean_squared_error: 13.7847 - lr: 0.0010\n",
      "Epoch 149/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 439.7025 - root_mean_squared_error: 20.9691\n",
      "Epoch 149: val_loss improved from 188.99321 to 188.62125, saving model to Model\\149-341.4083-188.6212.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 341.4083 - root_mean_squared_error: 18.4772 - val_loss: 188.6212 - val_root_mean_squared_error: 13.7339 - lr: 0.0010\n",
      "Epoch 150/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 426.8925 - root_mean_squared_error: 20.6614\n",
      "Epoch 150: val_loss improved from 188.62125 to 185.46135, saving model to Model\\150-340.6337-185.4613.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 340.6337 - root_mean_squared_error: 18.4563 - val_loss: 185.4613 - val_root_mean_squared_error: 13.6184 - lr: 0.0010\n",
      "Epoch 151/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 165.6384 - root_mean_squared_error: 12.8701\n",
      "Epoch 151: val_loss improved from 185.46135 to 183.50159, saving model to Model\\151-340.2437-183.5016.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 340.2437 - root_mean_squared_error: 18.4457 - val_loss: 183.5016 - val_root_mean_squared_error: 13.5463 - lr: 0.0010\n",
      "Epoch 152/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 416.7494 - root_mean_squared_error: 20.4144\n",
      "Epoch 152: val_loss improved from 183.50159 to 183.06357, saving model to Model\\152-338.9724-183.0636.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.9724 - root_mean_squared_error: 18.4112 - val_loss: 183.0636 - val_root_mean_squared_error: 13.5301 - lr: 0.0010\n",
      "Epoch 153/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 206.9368 - root_mean_squared_error: 14.3853\n",
      "Epoch 153: val_loss did not improve from 183.06357\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 338.7072 - root_mean_squared_error: 18.4040 - val_loss: 185.4190 - val_root_mean_squared_error: 13.6169 - lr: 0.0010\n",
      "Epoch 154/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 221.8522 - root_mean_squared_error: 14.8947\n",
      "Epoch 154: val_loss improved from 183.06357 to 179.21043, saving model to Model\\154-336.8998-179.2104.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 336.8998 - root_mean_squared_error: 18.3548 - val_loss: 179.2104 - val_root_mean_squared_error: 13.3870 - lr: 0.0010\n",
      "Epoch 155/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 393.1157 - root_mean_squared_error: 19.8271\n",
      "Epoch 155: val_loss did not improve from 179.21043\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 337.0128 - root_mean_squared_error: 18.3579 - val_loss: 180.0772 - val_root_mean_squared_error: 13.4193 - lr: 0.0010\n",
      "Epoch 156/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 201.5254 - root_mean_squared_error: 14.1960\n",
      "Epoch 156: val_loss improved from 179.21043 to 177.66579, saving model to Model\\156-335.8696-177.6658.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 335.8696 - root_mean_squared_error: 18.3267 - val_loss: 177.6658 - val_root_mean_squared_error: 13.3291 - lr: 0.0010\n",
      "Epoch 157/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 229.9249 - root_mean_squared_error: 15.1633\n",
      "Epoch 157: val_loss did not improve from 177.66579\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 334.7808 - root_mean_squared_error: 18.2970 - val_loss: 177.8882 - val_root_mean_squared_error: 13.3375 - lr: 0.0010\n",
      "Epoch 158/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 178.6467 - root_mean_squared_error: 13.3659\n",
      "Epoch 158: val_loss improved from 177.66579 to 174.82355, saving model to Model\\158-333.9691-174.8235.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.9691 - root_mean_squared_error: 18.2748 - val_loss: 174.8235 - val_root_mean_squared_error: 13.2221 - lr: 0.0010\n",
      "Epoch 159/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 443.4941 - root_mean_squared_error: 21.0593\n",
      "Epoch 159: val_loss did not improve from 174.82355\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 333.0702 - root_mean_squared_error: 18.2502 - val_loss: 175.4624 - val_root_mean_squared_error: 13.2462 - lr: 0.0010\n",
      "Epoch 160/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 404.1375 - root_mean_squared_error: 20.1032\n",
      "Epoch 160: val_loss improved from 174.82355 to 174.17737, saving model to Model\\160-332.1058-174.1774.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 332.1058 - root_mean_squared_error: 18.2238 - val_loss: 174.1774 - val_root_mean_squared_error: 13.1976 - lr: 0.0010\n",
      "Epoch 161/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 146.5032 - root_mean_squared_error: 12.1038\n",
      "Epoch 161: val_loss improved from 174.17737 to 173.20012, saving model to Model\\161-331.7937-173.2001.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 331.7937 - root_mean_squared_error: 18.2152 - val_loss: 173.2001 - val_root_mean_squared_error: 13.1606 - lr: 0.0010\n",
      "Epoch 162/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 227.2091 - root_mean_squared_error: 15.0735\n",
      "Epoch 162: val_loss improved from 173.20012 to 170.00800, saving model to Model\\162-330.6318-170.0080.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.6318 - root_mean_squared_error: 18.1833 - val_loss: 170.0080 - val_root_mean_squared_error: 13.0387 - lr: 0.0010\n",
      "Epoch 163/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 334.2965 - root_mean_squared_error: 18.2838\n",
      "Epoch 163: val_loss did not improve from 170.00800\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 330.6748 - root_mean_squared_error: 18.1845 - val_loss: 170.6326 - val_root_mean_squared_error: 13.0626 - lr: 0.0010\n",
      "Epoch 164/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 150.7329 - root_mean_squared_error: 12.2773\n",
      "Epoch 164: val_loss improved from 170.00800 to 169.58731, saving model to Model\\164-329.3831-169.5873.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 329.3831 - root_mean_squared_error: 18.1489 - val_loss: 169.5873 - val_root_mean_squared_error: 13.0226 - lr: 0.0010\n",
      "Epoch 165/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 399.9560 - root_mean_squared_error: 19.9989\n",
      "Epoch 165: val_loss improved from 169.58731 to 165.45555, saving model to Model\\165-327.9339-165.4556.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 327.9339 - root_mean_squared_error: 18.1089 - val_loss: 165.4556 - val_root_mean_squared_error: 12.8630 - lr: 0.0010\n",
      "Epoch 166/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 172.4216 - root_mean_squared_error: 13.1309\n",
      "Epoch 166: val_loss did not improve from 165.45555\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 327.9836 - root_mean_squared_error: 18.1103 - val_loss: 167.0928 - val_root_mean_squared_error: 12.9264 - lr: 0.0010\n",
      "Epoch 167/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 411.2254 - root_mean_squared_error: 20.2787\n",
      "Epoch 167: val_loss improved from 165.45555 to 164.45412, saving model to Model\\167-326.4303-164.4541.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 326.4303 - root_mean_squared_error: 18.0674 - val_loss: 164.4541 - val_root_mean_squared_error: 12.8240 - lr: 0.0010\n",
      "Epoch 168/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 272.9970 - root_mean_squared_error: 16.5226\n",
      "Epoch 168: val_loss improved from 164.45412 to 163.57977, saving model to Model\\168-325.9589-163.5798.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 325.9589 - root_mean_squared_error: 18.0543 - val_loss: 163.5798 - val_root_mean_squared_error: 12.7898 - lr: 0.0010\n",
      "Epoch 169/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 486.6318 - root_mean_squared_error: 22.0597\n",
      "Epoch 169: val_loss improved from 163.57977 to 163.10994, saving model to Model\\169-324.9106-163.1099.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.9106 - root_mean_squared_error: 18.0253 - val_loss: 163.1099 - val_root_mean_squared_error: 12.7715 - lr: 0.0010\n",
      "Epoch 170/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 331.7710 - root_mean_squared_error: 18.2146\n",
      "Epoch 170: val_loss improved from 163.10994 to 162.17679, saving model to Model\\170-324.1039-162.1768.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 324.1039 - root_mean_squared_error: 18.0029 - val_loss: 162.1768 - val_root_mean_squared_error: 12.7349 - lr: 0.0010\n",
      "Epoch 171/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 415.2052 - root_mean_squared_error: 20.3766\n",
      "Epoch 171: val_loss improved from 162.17679 to 161.04607, saving model to Model\\171-324.7090-161.0461.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 324.7090 - root_mean_squared_error: 18.0197 - val_loss: 161.0461 - val_root_mean_squared_error: 12.6904 - lr: 0.0010\n",
      "Epoch 172/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 405.3523 - root_mean_squared_error: 20.1334\n",
      "Epoch 172: val_loss improved from 161.04607 to 160.07072, saving model to Model\\172-322.5299-160.0707.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 322.5299 - root_mean_squared_error: 17.9591 - val_loss: 160.0707 - val_root_mean_squared_error: 12.6519 - lr: 0.0010\n",
      "Epoch 173/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 388.3706 - root_mean_squared_error: 19.7071\n",
      "Epoch 173: val_loss improved from 160.07072 to 157.56917, saving model to Model\\173-323.0384-157.5692.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 323.0384 - root_mean_squared_error: 17.9733 - val_loss: 157.5692 - val_root_mean_squared_error: 12.5527 - lr: 0.0010\n",
      "Epoch 174/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 308.2843 - root_mean_squared_error: 17.5580\n",
      "Epoch 174: val_loss did not improve from 157.56917\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 320.6019 - root_mean_squared_error: 17.9054 - val_loss: 160.0437 - val_root_mean_squared_error: 12.6508 - lr: 0.0010\n",
      "Epoch 175/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 108.6336 - root_mean_squared_error: 10.4227\n",
      "Epoch 175: val_loss improved from 157.56917 to 156.17435, saving model to Model\\175-321.1600-156.1743.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.1600 - root_mean_squared_error: 17.9209 - val_loss: 156.1743 - val_root_mean_squared_error: 12.4970 - lr: 0.0010\n",
      "Epoch 176/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 356.2738 - root_mean_squared_error: 18.8752\n",
      "Epoch 176: val_loss did not improve from 156.17435\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 321.3387 - root_mean_squared_error: 17.9259 - val_loss: 158.5338 - val_root_mean_squared_error: 12.5910 - lr: 0.0010\n",
      "Epoch 177/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 153.4670 - root_mean_squared_error: 12.3882\n",
      "Epoch 177: val_loss improved from 156.17435 to 152.34154, saving model to Model\\177-318.7339-152.3415.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 318.7339 - root_mean_squared_error: 17.8531 - val_loss: 152.3415 - val_root_mean_squared_error: 12.3427 - lr: 0.0010\n",
      "Epoch 178/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 315.7124 - root_mean_squared_error: 17.7683\n",
      "Epoch 178: val_loss improved from 152.34154 to 151.82155, saving model to Model\\178-319.1686-151.8215.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 319.1686 - root_mean_squared_error: 17.8653 - val_loss: 151.8215 - val_root_mean_squared_error: 12.3216 - lr: 0.0010\n",
      "Epoch 179/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 303.1157 - root_mean_squared_error: 17.4102\n",
      "Epoch 179: val_loss did not improve from 151.82155\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.7496 - root_mean_squared_error: 17.8255 - val_loss: 153.5681 - val_root_mean_squared_error: 12.3923 - lr: 0.0010\n",
      "Epoch 180/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 158.4696 - root_mean_squared_error: 12.5885\n",
      "Epoch 180: val_loss did not improve from 151.82155\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 317.2142 - root_mean_squared_error: 17.8105 - val_loss: 151.9618 - val_root_mean_squared_error: 12.3273 - lr: 0.0010\n",
      "Epoch 181/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 308.6792 - root_mean_squared_error: 17.5693\n",
      "Epoch 181: val_loss improved from 151.82155 to 147.74286, saving model to Model\\181-316.4968-147.7429.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 316.4968 - root_mean_squared_error: 17.7904 - val_loss: 147.7429 - val_root_mean_squared_error: 12.1550 - lr: 0.0010\n",
      "Epoch 182/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 349.8233 - root_mean_squared_error: 18.7036\n",
      "Epoch 182: val_loss did not improve from 147.74286\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 315.1227 - root_mean_squared_error: 17.7517 - val_loss: 148.4933 - val_root_mean_squared_error: 12.1858 - lr: 0.0010\n",
      "Epoch 183/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 361.8545 - root_mean_squared_error: 19.0225\n",
      "Epoch 183: val_loss improved from 147.74286 to 146.63988, saving model to Model\\183-314.6296-146.6399.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.6296 - root_mean_squared_error: 17.7378 - val_loss: 146.6399 - val_root_mean_squared_error: 12.1095 - lr: 0.0010\n",
      "Epoch 184/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 59.8431 - root_mean_squared_error: 7.7358\n",
      "Epoch 184: val_loss did not improve from 146.63988\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 314.0932 - root_mean_squared_error: 17.7227 - val_loss: 149.9799 - val_root_mean_squared_error: 12.2466 - lr: 0.0010\n",
      "Epoch 185/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 194.3284 - root_mean_squared_error: 13.9402\n",
      "Epoch 185: val_loss improved from 146.63988 to 144.08868, saving model to Model\\185-313.4663-144.0887.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 313.4663 - root_mean_squared_error: 17.7050 - val_loss: 144.0887 - val_root_mean_squared_error: 12.0037 - lr: 0.0010\n",
      "Epoch 186/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 197.9716 - root_mean_squared_error: 14.0702\n",
      "Epoch 186: val_loss did not improve from 144.08868\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.5605 - root_mean_squared_error: 17.6794 - val_loss: 144.4064 - val_root_mean_squared_error: 12.0169 - lr: 0.0010\n",
      "Epoch 187/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 294.9684 - root_mean_squared_error: 17.1746\n",
      "Epoch 187: val_loss improved from 144.08868 to 144.00119, saving model to Model\\187-312.1211-144.0012.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 312.1211 - root_mean_squared_error: 17.6669 - val_loss: 144.0012 - val_root_mean_squared_error: 12.0000 - lr: 0.0010\n",
      "Epoch 188/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 473.4018 - root_mean_squared_error: 21.7578\n",
      "Epoch 188: val_loss improved from 144.00119 to 140.21246, saving model to Model\\188-310.3268-140.2125.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 310.3268 - root_mean_squared_error: 17.6161 - val_loss: 140.2125 - val_root_mean_squared_error: 11.8411 - lr: 0.0010\n",
      "Epoch 189/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 191.3622 - root_mean_squared_error: 13.8334\n",
      "Epoch 189: val_loss did not improve from 140.21246\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 310.9925 - root_mean_squared_error: 17.6350 - val_loss: 141.4919 - val_root_mean_squared_error: 11.8950 - lr: 0.0010\n",
      "Epoch 190/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 231.8297 - root_mean_squared_error: 15.2260\n",
      "Epoch 190: val_loss improved from 140.21246 to 140.08281, saving model to Model\\190-309.7639-140.0828.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 309.7639 - root_mean_squared_error: 17.6001 - val_loss: 140.0828 - val_root_mean_squared_error: 11.8357 - lr: 0.0010\n",
      "Epoch 191/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 360.8989 - root_mean_squared_error: 18.9973\n",
      "Epoch 191: val_loss improved from 140.08281 to 136.61975, saving model to Model\\191-310.0532-136.6198.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 310.0532 - root_mean_squared_error: 17.6083 - val_loss: 136.6198 - val_root_mean_squared_error: 11.6884 - lr: 0.0010\n",
      "Epoch 192/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 324.3567 - root_mean_squared_error: 18.0099\n",
      "Epoch 192: val_loss did not improve from 136.61975\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.9933 - root_mean_squared_error: 17.5497 - val_loss: 139.2214 - val_root_mean_squared_error: 11.7992 - lr: 0.0010\n",
      "Epoch 193/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 211.9652 - root_mean_squared_error: 14.5590\n",
      "Epoch 193: val_loss did not improve from 136.61975\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 307.3181 - root_mean_squared_error: 17.5305 - val_loss: 138.1633 - val_root_mean_squared_error: 11.7543 - lr: 0.0010\n",
      "Epoch 194/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 298.7062 - root_mean_squared_error: 17.2831\n",
      "Epoch 194: val_loss improved from 136.61975 to 134.57608, saving model to Model\\194-306.4781-134.5761.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 306.4781 - root_mean_squared_error: 17.5065 - val_loss: 134.5761 - val_root_mean_squared_error: 11.6007 - lr: 0.0010\n",
      "Epoch 195/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 143.3762 - root_mean_squared_error: 11.9740\n",
      "Epoch 195: val_loss improved from 134.57608 to 133.60240, saving model to Model\\195-306.7551-133.6024.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 306.7551 - root_mean_squared_error: 17.5144 - val_loss: 133.6024 - val_root_mean_squared_error: 11.5587 - lr: 0.0010\n",
      "Epoch 196/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 124.0950 - root_mean_squared_error: 11.1398\n",
      "Epoch 196: val_loss improved from 133.60240 to 131.53389, saving model to Model\\196-305.1847-131.5339.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.1847 - root_mean_squared_error: 17.4695 - val_loss: 131.5339 - val_root_mean_squared_error: 11.4688 - lr: 0.0010\n",
      "Epoch 197/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 335.9572 - root_mean_squared_error: 18.3291\n",
      "Epoch 197: val_loss did not improve from 131.53389\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 305.5986 - root_mean_squared_error: 17.4814 - val_loss: 133.0909 - val_root_mean_squared_error: 11.5365 - lr: 0.0010\n",
      "Epoch 198/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 101.5311 - root_mean_squared_error: 10.0763\n",
      "Epoch 198: val_loss improved from 131.53389 to 130.83688, saving model to Model\\198-304.7221-130.8369.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 304.7221 - root_mean_squared_error: 17.4563 - val_loss: 130.8369 - val_root_mean_squared_error: 11.4384 - lr: 0.0010\n",
      "Epoch 199/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 433.6040 - root_mean_squared_error: 20.8232\n",
      "Epoch 199: val_loss improved from 130.83688 to 129.23700, saving model to Model\\199-303.0459-129.2370.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 303.0459 - root_mean_squared_error: 17.4082 - val_loss: 129.2370 - val_root_mean_squared_error: 11.3682 - lr: 0.0010\n",
      "Epoch 200/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 152.5819 - root_mean_squared_error: 12.3524\n",
      "Epoch 200: val_loss improved from 129.23700 to 128.56102, saving model to Model\\200-302.4745-128.5610.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 302.4745 - root_mean_squared_error: 17.3918 - val_loss: 128.5610 - val_root_mean_squared_error: 11.3385 - lr: 0.0010\n",
      "Epoch 201/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 149.6161 - root_mean_squared_error: 12.2318\n",
      "Epoch 201: val_loss improved from 128.56102 to 128.29915, saving model to Model\\201-302.5524-128.2991.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 302.5524 - root_mean_squared_error: 17.3940 - val_loss: 128.2991 - val_root_mean_squared_error: 11.3269 - lr: 0.0010\n",
      "Epoch 202/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 177.2236 - root_mean_squared_error: 13.3125\n",
      "Epoch 202: val_loss did not improve from 128.29915\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.9786 - root_mean_squared_error: 17.3487 - val_loss: 128.9760 - val_root_mean_squared_error: 11.3568 - lr: 0.0010\n",
      "Epoch 203/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 210.4413 - root_mean_squared_error: 14.5066\n",
      "Epoch 203: val_loss improved from 128.29915 to 126.45549, saving model to Model\\203-300.6120-126.4555.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.6120 - root_mean_squared_error: 17.3382 - val_loss: 126.4555 - val_root_mean_squared_error: 11.2452 - lr: 0.0010\n",
      "Epoch 204/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 237.5280 - root_mean_squared_error: 15.4119\n",
      "Epoch 204: val_loss improved from 126.45549 to 124.29709, saving model to Model\\204-300.2443-124.2971.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.2443 - root_mean_squared_error: 17.3276 - val_loss: 124.2971 - val_root_mean_squared_error: 11.1489 - lr: 0.0010\n",
      "Epoch 205/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 110.6440 - root_mean_squared_error: 10.5187\n",
      "Epoch 205: val_loss did not improve from 124.29709\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 300.0248 - root_mean_squared_error: 17.3212 - val_loss: 126.7853 - val_root_mean_squared_error: 11.2599 - lr: 0.0010\n",
      "Epoch 206/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 479.7783 - root_mean_squared_error: 21.9038\n",
      "Epoch 206: val_loss improved from 124.29709 to 121.18153, saving model to Model\\206-298.3123-121.1815.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.3123 - root_mean_squared_error: 17.2717 - val_loss: 121.1815 - val_root_mean_squared_error: 11.0082 - lr: 0.0010\n",
      "Epoch 207/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 193.8292 - root_mean_squared_error: 13.9223\n",
      "Epoch 207: val_loss did not improve from 121.18153\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 298.1270 - root_mean_squared_error: 17.2664 - val_loss: 123.4823 - val_root_mean_squared_error: 11.1123 - lr: 0.0010\n",
      "Epoch 208/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 209.0394 - root_mean_squared_error: 14.4582\n",
      "Epoch 208: val_loss did not improve from 121.18153\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 297.2637 - root_mean_squared_error: 17.2413 - val_loss: 121.3585 - val_root_mean_squared_error: 11.0163 - lr: 0.0010\n",
      "Epoch 209/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 499.7316 - root_mean_squared_error: 22.3547\n",
      "Epoch 209: val_loss improved from 121.18153 to 119.68307, saving model to Model\\209-297.9838-119.6831.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 297.9838 - root_mean_squared_error: 17.2622 - val_loss: 119.6831 - val_root_mean_squared_error: 10.9400 - lr: 0.0010\n",
      "Epoch 210/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 154.1223 - root_mean_squared_error: 12.4146\n",
      "Epoch 210: val_loss did not improve from 119.68307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 296.1021 - root_mean_squared_error: 17.2076 - val_loss: 120.3670 - val_root_mean_squared_error: 10.9712 - lr: 0.0010\n",
      "Epoch 211/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 160.2337 - root_mean_squared_error: 12.6583\n",
      "Epoch 211: val_loss did not improve from 119.68307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 295.6944 - root_mean_squared_error: 17.1958 - val_loss: 119.9396 - val_root_mean_squared_error: 10.9517 - lr: 0.0010\n",
      "Epoch 212/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 436.8813 - root_mean_squared_error: 20.9017\n",
      "Epoch 212: val_loss improved from 119.68307 to 116.82974, saving model to Model\\212-294.4956-116.8297.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.4956 - root_mean_squared_error: 17.1609 - val_loss: 116.8297 - val_root_mean_squared_error: 10.8088 - lr: 0.0010\n",
      "Epoch 213/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 152.2157 - root_mean_squared_error: 12.3376\n",
      "Epoch 213: val_loss improved from 116.82974 to 115.28944, saving model to Model\\213-294.9722-115.2894.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 294.9722 - root_mean_squared_error: 17.1748 - val_loss: 115.2894 - val_root_mean_squared_error: 10.7373 - lr: 0.0010\n",
      "Epoch 214/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 311.7727 - root_mean_squared_error: 17.6571\n",
      "Epoch 214: val_loss did not improve from 115.28944\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.3877 - root_mean_squared_error: 17.1286 - val_loss: 116.9038 - val_root_mean_squared_error: 10.8122 - lr: 0.0010\n",
      "Epoch 215/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 176.1763 - root_mean_squared_error: 13.2731\n",
      "Epoch 215: val_loss improved from 115.28944 to 113.31985, saving model to Model\\215-293.6736-113.3199.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 293.6736 - root_mean_squared_error: 17.1369 - val_loss: 113.3199 - val_root_mean_squared_error: 10.6452 - lr: 0.0010\n",
      "Epoch 216/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 242.9991 - root_mean_squared_error: 15.5884\n",
      "Epoch 216: val_loss did not improve from 113.31985\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.2777 - root_mean_squared_error: 17.0961 - val_loss: 113.9447 - val_root_mean_squared_error: 10.6745 - lr: 0.0010\n",
      "Epoch 217/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 397.6575 - root_mean_squared_error: 19.9414\n",
      "Epoch 217: val_loss improved from 113.31985 to 110.96059, saving model to Model\\217-291.7648-110.9606.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 291.7648 - root_mean_squared_error: 17.0811 - val_loss: 110.9606 - val_root_mean_squared_error: 10.5338 - lr: 0.0010\n",
      "Epoch 218/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 286.1495 - root_mean_squared_error: 16.9160\n",
      "Epoch 218: val_loss improved from 110.96059 to 110.56786, saving model to Model\\218-292.4418-110.5679.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 292.4418 - root_mean_squared_error: 17.1009 - val_loss: 110.5679 - val_root_mean_squared_error: 10.5151 - lr: 0.0010\n",
      "Epoch 219/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 652.1721 - root_mean_squared_error: 25.5377\n",
      "Epoch 219: val_loss improved from 110.56786 to 109.43457, saving model to Model\\219-291.4438-109.4346.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 291.4438 - root_mean_squared_error: 17.0717 - val_loss: 109.4346 - val_root_mean_squared_error: 10.4611 - lr: 0.0010\n",
      "Epoch 220/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 61.3494 - root_mean_squared_error: 7.8326\n",
      "Epoch 220: val_loss did not improve from 109.43457\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.2092 - root_mean_squared_error: 17.0355 - val_loss: 112.3156 - val_root_mean_squared_error: 10.5979 - lr: 0.0010\n",
      "Epoch 221/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 632.9066 - root_mean_squared_error: 25.1576\n",
      "Epoch 221: val_loss improved from 109.43457 to 107.54951, saving model to Model\\221-290.4165-107.5495.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 290.4165 - root_mean_squared_error: 17.0416 - val_loss: 107.5495 - val_root_mean_squared_error: 10.3706 - lr: 0.0010\n",
      "Epoch 222/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 149.6472 - root_mean_squared_error: 12.2330\n",
      "Epoch 222: val_loss improved from 107.54951 to 107.37407, saving model to Model\\222-289.8878-107.3741.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 289.8878 - root_mean_squared_error: 17.0261 - val_loss: 107.3741 - val_root_mean_squared_error: 10.3621 - lr: 0.0010\n",
      "Epoch 223/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 308.0831 - root_mean_squared_error: 17.5523\n",
      "Epoch 223: val_loss did not improve from 107.37407\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 289.7229 - root_mean_squared_error: 17.0212 - val_loss: 107.8964 - val_root_mean_squared_error: 10.3873 - lr: 0.0010\n",
      "Epoch 224/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 317.4681 - root_mean_squared_error: 17.8176\n",
      "Epoch 224: val_loss improved from 107.37407 to 104.86721, saving model to Model\\224-287.9977-104.8672.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 287.9977 - root_mean_squared_error: 16.9705 - val_loss: 104.8672 - val_root_mean_squared_error: 10.2405 - lr: 0.0010\n",
      "Epoch 225/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 333.8539 - root_mean_squared_error: 18.2717\n",
      "Epoch 225: val_loss did not improve from 104.86721\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.1299 - root_mean_squared_error: 16.9744 - val_loss: 105.6814 - val_root_mean_squared_error: 10.2801 - lr: 0.0010\n",
      "Epoch 226/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 236.7554 - root_mean_squared_error: 15.3869\n",
      "Epoch 226: val_loss did not improve from 104.86721\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.8928 - root_mean_squared_error: 16.9379 - val_loss: 107.4147 - val_root_mean_squared_error: 10.3641 - lr: 0.0010\n",
      "Epoch 227/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 264.0068 - root_mean_squared_error: 16.2483\n",
      "Epoch 227: val_loss improved from 104.86721 to 103.37740, saving model to Model\\227-287.0703-103.3774.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 287.0703 - root_mean_squared_error: 16.9431 - val_loss: 103.3774 - val_root_mean_squared_error: 10.1675 - lr: 0.0010\n",
      "Epoch 228/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 326.2832 - root_mean_squared_error: 18.0633\n",
      "Epoch 228: val_loss improved from 103.37740 to 102.79768, saving model to Model\\228-286.8337-102.7977.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 286.8337 - root_mean_squared_error: 16.9362 - val_loss: 102.7977 - val_root_mean_squared_error: 10.1389 - lr: 0.0010\n",
      "Epoch 229/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 312.5523 - root_mean_squared_error: 17.6791\n",
      "Epoch 229: val_loss improved from 102.79768 to 100.48301, saving model to Model\\229-286.9623-100.4830.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.9623 - root_mean_squared_error: 16.9400 - val_loss: 100.4830 - val_root_mean_squared_error: 10.0241 - lr: 0.0010\n",
      "Epoch 230/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 103.9864 - root_mean_squared_error: 10.1974\n",
      "Epoch 230: val_loss did not improve from 100.48301\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 288.0021 - root_mean_squared_error: 16.9706 - val_loss: 104.1072 - val_root_mean_squared_error: 10.2033 - lr: 0.0010\n",
      "Epoch 231/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 143.8413 - root_mean_squared_error: 11.9934\n",
      "Epoch 231: val_loss improved from 100.48301 to 100.08670, saving model to Model\\231-286.0081-100.0867.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 286.0081 - root_mean_squared_error: 16.9118 - val_loss: 100.0867 - val_root_mean_squared_error: 10.0043 - lr: 0.0010\n",
      "Epoch 232/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 207.4184 - root_mean_squared_error: 14.4020\n",
      "Epoch 232: val_loss did not improve from 100.08670\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.8746 - root_mean_squared_error: 16.8486 - val_loss: 101.3536 - val_root_mean_squared_error: 10.0675 - lr: 0.0010\n",
      "Epoch 233/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 334.3621 - root_mean_squared_error: 18.2856\n",
      "Epoch 233: val_loss improved from 100.08670 to 97.26453, saving model to Model\\233-283.6146-97.2645.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.6146 - root_mean_squared_error: 16.8409 - val_loss: 97.2645 - val_root_mean_squared_error: 9.8623 - lr: 0.0010\n",
      "Epoch 234/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 182.0178 - root_mean_squared_error: 13.4914\n",
      "Epoch 234: val_loss did not improve from 97.26453\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.9662 - root_mean_squared_error: 16.8216 - val_loss: 98.0426 - val_root_mean_squared_error: 9.9016 - lr: 0.0010\n",
      "Epoch 235/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 127.0094 - root_mean_squared_error: 11.2698\n",
      "Epoch 235: val_loss did not improve from 97.26453\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 283.1615 - root_mean_squared_error: 16.8274 - val_loss: 99.3125 - val_root_mean_squared_error: 9.9656 - lr: 0.0010\n",
      "Epoch 236/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 123.2818 - root_mean_squared_error: 11.1032\n",
      "Epoch 236: val_loss improved from 97.26453 to 96.16298, saving model to Model\\236-284.6640-96.1630.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 284.6640 - root_mean_squared_error: 16.8720 - val_loss: 96.1630 - val_root_mean_squared_error: 9.8063 - lr: 0.0010\n",
      "Epoch 237/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 88.0917 - root_mean_squared_error: 9.3857\n",
      "Epoch 237: val_loss did not improve from 96.16298\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 282.4368 - root_mean_squared_error: 16.8059 - val_loss: 96.7495 - val_root_mean_squared_error: 9.8361 - lr: 0.0010\n",
      "Epoch 238/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 108.4075 - root_mean_squared_error: 10.4119\n",
      "Epoch 238: val_loss improved from 96.16298 to 92.69410, saving model to Model\\238-283.2191-92.6941.hdf5\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 283.2191 - root_mean_squared_error: 16.8291 - val_loss: 92.6941 - val_root_mean_squared_error: 9.6278 - lr: 0.0010\n",
      "Epoch 239/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 312.2090 - root_mean_squared_error: 17.6694\n",
      "Epoch 239: val_loss did not improve from 92.69410\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.7304 - root_mean_squared_error: 16.7848 - val_loss: 95.9951 - val_root_mean_squared_error: 9.7977 - lr: 0.0010\n",
      "Epoch 240/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 95.6247 - root_mean_squared_error: 9.7788\n",
      "Epoch 240: val_loss did not improve from 92.69410\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 280.6270 - root_mean_squared_error: 16.7519 - val_loss: 93.8495 - val_root_mean_squared_error: 9.6876 - lr: 0.0010\n",
      "Epoch 241/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 58.2229 - root_mean_squared_error: 7.6304\n",
      "Epoch 241: val_loss improved from 92.69410 to 91.95421, saving model to Model\\241-280.0499-91.9542.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 280.0499 - root_mean_squared_error: 16.7347 - val_loss: 91.9542 - val_root_mean_squared_error: 9.5893 - lr: 0.0010\n",
      "Epoch 242/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 490.1541 - root_mean_squared_error: 22.1394\n",
      "Epoch 242: val_loss improved from 91.95421 to 90.09324, saving model to Model\\242-279.5548-90.0932.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 279.5548 - root_mean_squared_error: 16.7199 - val_loss: 90.0932 - val_root_mean_squared_error: 9.4917 - lr: 0.0010\n",
      "Epoch 243/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 93.7926 - root_mean_squared_error: 9.6847\n",
      "Epoch 243: val_loss did not improve from 90.09324\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 281.2137 - root_mean_squared_error: 16.7694 - val_loss: 92.9492 - val_root_mean_squared_error: 9.6410 - lr: 0.0010\n",
      "Epoch 244/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 152.2741 - root_mean_squared_error: 12.3399\n",
      "Epoch 244: val_loss improved from 90.09324 to 90.02217, saving model to Model\\244-278.6976-90.0222.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 278.6976 - root_mean_squared_error: 16.6942 - val_loss: 90.0222 - val_root_mean_squared_error: 9.4880 - lr: 0.0010\n",
      "Epoch 245/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 493.7796 - root_mean_squared_error: 22.2212\n",
      "Epoch 245: val_loss improved from 90.02217 to 89.35815, saving model to Model\\245-278.2474-89.3582.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 278.2474 - root_mean_squared_error: 16.6807 - val_loss: 89.3582 - val_root_mean_squared_error: 9.4529 - lr: 0.0010\n",
      "Epoch 246/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 565.1887 - root_mean_squared_error: 23.7737\n",
      "Epoch 246: val_loss improved from 89.35815 to 88.64686, saving model to Model\\246-277.7001-88.6469.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.7001 - root_mean_squared_error: 16.6643 - val_loss: 88.6469 - val_root_mean_squared_error: 9.4152 - lr: 0.0010\n",
      "Epoch 247/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 776.6791 - root_mean_squared_error: 27.8690\n",
      "Epoch 247: val_loss improved from 88.64686 to 84.33414, saving model to Model\\247-277.6044-84.3341.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.6044 - root_mean_squared_error: 16.6615 - val_loss: 84.3341 - val_root_mean_squared_error: 9.1834 - lr: 0.0010\n",
      "Epoch 248/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 512.3797 - root_mean_squared_error: 22.6358\n",
      "Epoch 248: val_loss did not improve from 84.33414\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.0154 - root_mean_squared_error: 16.6438 - val_loss: 86.8427 - val_root_mean_squared_error: 9.3189 - lr: 0.0010\n",
      "Epoch 249/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 324.3708 - root_mean_squared_error: 18.0103\n",
      "Epoch 249: val_loss did not improve from 84.33414\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 277.0286 - root_mean_squared_error: 16.6442 - val_loss: 87.1021 - val_root_mean_squared_error: 9.3329 - lr: 0.0010\n",
      "Epoch 250/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 566.4791 - root_mean_squared_error: 23.8008\n",
      "Epoch 250: val_loss did not improve from 84.33414\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.8153 - root_mean_squared_error: 16.6077 - val_loss: 86.0594 - val_root_mean_squared_error: 9.2768 - lr: 0.0010\n",
      "Epoch 251/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 536.1053 - root_mean_squared_error: 23.1539\n",
      "Epoch 251: val_loss improved from 84.33414 to 82.46107, saving model to Model\\251-275.7596-82.4611.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.7596 - root_mean_squared_error: 16.6060 - val_loss: 82.4611 - val_root_mean_squared_error: 9.0808 - lr: 0.0010\n",
      "Epoch 252/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 94.4813 - root_mean_squared_error: 9.7201\n",
      "Epoch 252: val_loss did not improve from 82.46107\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.3158 - root_mean_squared_error: 16.5926 - val_loss: 84.1846 - val_root_mean_squared_error: 9.1752 - lr: 0.0010\n",
      "Epoch 253/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 165.5873 - root_mean_squared_error: 12.8681\n",
      "Epoch 253: val_loss did not improve from 82.46107\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 276.1206 - root_mean_squared_error: 16.6169 - val_loss: 82.4887 - val_root_mean_squared_error: 9.0823 - lr: 0.0010\n",
      "Epoch 254/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 269.6152 - root_mean_squared_error: 16.4200\n",
      "Epoch 254: val_loss improved from 82.46107 to 81.00431, saving model to Model\\254-274.7723-81.0043.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.7723 - root_mean_squared_error: 16.5763 - val_loss: 81.0043 - val_root_mean_squared_error: 9.0002 - lr: 0.0010\n",
      "Epoch 255/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 79.8675 - root_mean_squared_error: 8.9369\n",
      "Epoch 255: val_loss did not improve from 81.00431\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 273.8252 - root_mean_squared_error: 16.5477 - val_loss: 83.0131 - val_root_mean_squared_error: 9.1112 - lr: 0.0010\n",
      "Epoch 256/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 176.8843 - root_mean_squared_error: 13.2998\n",
      "Epoch 256: val_loss did not improve from 81.00431\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.0750 - root_mean_squared_error: 16.5552 - val_loss: 86.3089 - val_root_mean_squared_error: 9.2903 - lr: 0.0010\n",
      "Epoch 257/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 343.8853 - root_mean_squared_error: 18.5441\n",
      "Epoch 257: val_loss improved from 81.00431 to 78.15125, saving model to Model\\257-274.3952-78.1513.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 274.3952 - root_mean_squared_error: 16.5649 - val_loss: 78.1513 - val_root_mean_squared_error: 8.8403 - lr: 0.0010\n",
      "Epoch 258/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 153.5500 - root_mean_squared_error: 12.3915\n",
      "Epoch 258: val_loss did not improve from 78.15125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.9260 - root_mean_squared_error: 16.6110 - val_loss: 81.6338 - val_root_mean_squared_error: 9.0351 - lr: 0.0010\n",
      "Epoch 259/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 215.7399 - root_mean_squared_error: 14.6881\n",
      "Epoch 259: val_loss did not improve from 78.15125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.5327 - root_mean_squared_error: 16.5992 - val_loss: 78.4443 - val_root_mean_squared_error: 8.8569 - lr: 0.0010\n",
      "Epoch 260/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 414.5163 - root_mean_squared_error: 20.3597\n",
      "Epoch 260: val_loss did not improve from 78.15125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 275.9962 - root_mean_squared_error: 16.6131 - val_loss: 78.8214 - val_root_mean_squared_error: 8.8781 - lr: 0.0010\n",
      "Epoch 261/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 488.1691 - root_mean_squared_error: 22.0945\n",
      "Epoch 261: val_loss improved from 78.15125 to 77.53496, saving model to Model\\261-271.9030-77.5350.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.9030 - root_mean_squared_error: 16.4895 - val_loss: 77.5350 - val_root_mean_squared_error: 8.8054 - lr: 0.0010\n",
      "Epoch 262/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 118.4587 - root_mean_squared_error: 10.8839\n",
      "Epoch 262: val_loss did not improve from 77.53496\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.3935 - root_mean_squared_error: 16.5043 - val_loss: 79.3688 - val_root_mean_squared_error: 8.9089 - lr: 0.0010\n",
      "Epoch 263/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 498.7980 - root_mean_squared_error: 22.3338\n",
      "Epoch 263: val_loss improved from 77.53496 to 75.71809, saving model to Model\\263-271.1296-75.7181.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.1296 - root_mean_squared_error: 16.4660 - val_loss: 75.7181 - val_root_mean_squared_error: 8.7016 - lr: 0.0010\n",
      "Epoch 264/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 64.1847 - root_mean_squared_error: 8.0115\n",
      "Epoch 264: val_loss did not improve from 75.71809\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.0392 - root_mean_squared_error: 16.4936 - val_loss: 77.1044 - val_root_mean_squared_error: 8.7809 - lr: 0.0010\n",
      "Epoch 265/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 200.1717 - root_mean_squared_error: 14.1482\n",
      "Epoch 265: val_loss did not improve from 75.71809\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 272.1940 - root_mean_squared_error: 16.4983 - val_loss: 78.1216 - val_root_mean_squared_error: 8.8386 - lr: 0.0010\n",
      "Epoch 266/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 75.6055 - root_mean_squared_error: 8.6951\n",
      "Epoch 266: val_loss improved from 75.71809 to 73.47501, saving model to Model\\266-271.1432-73.4750.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.1432 - root_mean_squared_error: 16.4664 - val_loss: 73.4750 - val_root_mean_squared_error: 8.5718 - lr: 0.0010\n",
      "Epoch 267/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 325.1988 - root_mean_squared_error: 18.0333\n",
      "Epoch 267: val_loss did not improve from 73.47501\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.9800 - root_mean_squared_error: 16.4311 - val_loss: 75.6878 - val_root_mean_squared_error: 8.6999 - lr: 0.0010\n",
      "Epoch 268/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 108.9903 - root_mean_squared_error: 10.4398\n",
      "Epoch 268: val_loss did not improve from 73.47501\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.7035 - root_mean_squared_error: 16.4227 - val_loss: 73.5658 - val_root_mean_squared_error: 8.5770 - lr: 0.0010\n",
      "Epoch 269/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 272.0072 - root_mean_squared_error: 16.4926\n",
      "Epoch 269: val_loss did not improve from 73.47501\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8213 - root_mean_squared_error: 16.4262 - val_loss: 73.8700 - val_root_mean_squared_error: 8.5948 - lr: 0.0010\n",
      "Epoch 270/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 323.7107 - root_mean_squared_error: 17.9920\n",
      "Epoch 270: val_loss improved from 73.47501 to 70.83325, saving model to Model\\270-269.8064-70.8333.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8064 - root_mean_squared_error: 16.4258 - val_loss: 70.8333 - val_root_mean_squared_error: 8.4162 - lr: 0.0010\n",
      "Epoch 271/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 103.3496 - root_mean_squared_error: 10.1661\n",
      "Epoch 271: val_loss did not improve from 70.83325\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.2559 - root_mean_squared_error: 16.4395 - val_loss: 74.5297 - val_root_mean_squared_error: 8.6331 - lr: 0.0010\n",
      "Epoch 272/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 59.3904 - root_mean_squared_error: 7.7065\n",
      "Epoch 272: val_loss did not improve from 70.83325\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.9714 - root_mean_squared_error: 16.4308 - val_loss: 73.0836 - val_root_mean_squared_error: 8.5489 - lr: 0.0010\n",
      "Epoch 273/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 279.5000 - root_mean_squared_error: 16.7183\n",
      "Epoch 273: val_loss improved from 70.83325 to 69.39285, saving model to Model\\273-270.8368-69.3928.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 270.8368 - root_mean_squared_error: 16.4571 - val_loss: 69.3928 - val_root_mean_squared_error: 8.3302 - lr: 0.0010\n",
      "Epoch 274/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 294.9930 - root_mean_squared_error: 17.1754\n",
      "Epoch 274: val_loss improved from 69.39285 to 68.70930, saving model to Model\\274-269.1447-68.7093.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.1447 - root_mean_squared_error: 16.4056 - val_loss: 68.7093 - val_root_mean_squared_error: 8.2891 - lr: 0.0010\n",
      "Epoch 275/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 253.1039 - root_mean_squared_error: 15.9092\n",
      "Epoch 275: val_loss did not improve from 68.70930\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.9968 - root_mean_squared_error: 16.3706 - val_loss: 69.4348 - val_root_mean_squared_error: 8.3328 - lr: 0.0010\n",
      "Epoch 276/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 595.1841 - root_mean_squared_error: 24.3964\n",
      "Epoch 276: val_loss did not improve from 68.70930\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.6036 - root_mean_squared_error: 16.3586 - val_loss: 69.6679 - val_root_mean_squared_error: 8.3467 - lr: 0.0010\n",
      "Epoch 277/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 446.7579 - root_mean_squared_error: 21.1366\n",
      "Epoch 277: val_loss improved from 68.70930 to 68.09144, saving model to Model\\277-266.8755-68.0914.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.8755 - root_mean_squared_error: 16.3363 - val_loss: 68.0914 - val_root_mean_squared_error: 8.2518 - lr: 0.0010\n",
      "Epoch 278/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 667.1255 - root_mean_squared_error: 25.8288\n",
      "Epoch 278: val_loss improved from 68.09144 to 66.76733, saving model to Model\\278-267.7657-66.7673.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.7657 - root_mean_squared_error: 16.3635 - val_loss: 66.7673 - val_root_mean_squared_error: 8.1711 - lr: 0.0010\n",
      "Epoch 279/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 321.7062 - root_mean_squared_error: 17.9362\n",
      "Epoch 279: val_loss did not improve from 66.76733\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.1553 - root_mean_squared_error: 16.3754 - val_loss: 68.5760 - val_root_mean_squared_error: 8.2811 - lr: 0.0010\n",
      "Epoch 280/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 797.9705 - root_mean_squared_error: 28.2484\n",
      "Epoch 280: val_loss did not improve from 66.76733\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 271.2583 - root_mean_squared_error: 16.4699 - val_loss: 67.7837 - val_root_mean_squared_error: 8.2331 - lr: 0.0010\n",
      "Epoch 281/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 356.0677 - root_mean_squared_error: 18.8698\n",
      "Epoch 281: val_loss did not improve from 66.76733\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 268.7885 - root_mean_squared_error: 16.3948 - val_loss: 67.0962 - val_root_mean_squared_error: 8.1912 - lr: 0.0010\n",
      "Epoch 282/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 81.3417 - root_mean_squared_error: 9.0190\n",
      "Epoch 282: val_loss did not improve from 66.76733\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 267.5878 - root_mean_squared_error: 16.3581 - val_loss: 69.8382 - val_root_mean_squared_error: 8.3569 - lr: 0.0010\n",
      "Epoch 283/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 257.6732 - root_mean_squared_error: 16.0522\n",
      "Epoch 283: val_loss improved from 66.76733 to 64.60841, saving model to Model\\283-269.6856-64.6084.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.6856 - root_mean_squared_error: 16.4221 - val_loss: 64.6084 - val_root_mean_squared_error: 8.0379 - lr: 0.0010\n",
      "Epoch 284/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 101.8872 - root_mean_squared_error: 10.0939\n",
      "Epoch 284: val_loss did not improve from 64.60841\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.9367 - root_mean_squared_error: 16.3076 - val_loss: 68.7160 - val_root_mean_squared_error: 8.2895 - lr: 0.0010\n",
      "Epoch 285/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 256.9366 - root_mean_squared_error: 16.0292\n",
      "Epoch 285: val_loss improved from 64.60841 to 62.70976, saving model to Model\\285-266.9773-62.7098.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.9773 - root_mean_squared_error: 16.3394 - val_loss: 62.7098 - val_root_mean_squared_error: 7.9189 - lr: 0.0010\n",
      "Epoch 286/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 546.2402 - root_mean_squared_error: 23.3718\n",
      "Epoch 286: val_loss did not improve from 62.70976\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 265.8169 - root_mean_squared_error: 16.3039 - val_loss: 62.9150 - val_root_mean_squared_error: 7.9319 - lr: 0.0010\n",
      "Epoch 287/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 50.7910 - root_mean_squared_error: 7.1268\n",
      "Epoch 287: val_loss did not improve from 62.70976\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.0231 - root_mean_squared_error: 16.3102 - val_loss: 65.3090 - val_root_mean_squared_error: 8.0814 - lr: 0.0010\n",
      "Epoch 288/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 59.3667 - root_mean_squared_error: 7.7050\n",
      "Epoch 288: val_loss improved from 62.70976 to 61.29053, saving model to Model\\288-269.8491-61.2905.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 269.8491 - root_mean_squared_error: 16.4271 - val_loss: 61.2905 - val_root_mean_squared_error: 7.8288 - lr: 0.0010\n",
      "Epoch 289/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 283.0687 - root_mean_squared_error: 16.8246\n",
      "Epoch 289: val_loss did not improve from 61.29053\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.5802 - root_mean_squared_error: 16.3273 - val_loss: 63.5436 - val_root_mean_squared_error: 7.9714 - lr: 0.0010\n",
      "Epoch 290/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 69.2983 - root_mean_squared_error: 8.3246\n",
      "Epoch 290: val_loss did not improve from 61.29053\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.4242 - root_mean_squared_error: 16.2611 - val_loss: 65.4748 - val_root_mean_squared_error: 8.0917 - lr: 0.0010\n",
      "Epoch 291/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 128.1396 - root_mean_squared_error: 11.3199\n",
      "Epoch 291: val_loss did not improve from 61.29053\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 266.6838 - root_mean_squared_error: 16.3305 - val_loss: 64.9737 - val_root_mean_squared_error: 8.0606 - lr: 0.0010\n",
      "Epoch 292/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 92.6073 - root_mean_squared_error: 9.6233\n",
      "Epoch 292: val_loss did not improve from 61.29053\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.4199 - root_mean_squared_error: 16.2610 - val_loss: 62.7311 - val_root_mean_squared_error: 7.9203 - lr: 0.0010\n",
      "Epoch 293/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 42.6513 - root_mean_squared_error: 6.5308\n",
      "Epoch 293: val_loss improved from 61.29053 to 61.28563, saving model to Model\\293-264.0626-61.2856.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.0626 - root_mean_squared_error: 16.2500 - val_loss: 61.2856 - val_root_mean_squared_error: 7.8285 - lr: 0.0010\n",
      "Epoch 294/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 79.9683 - root_mean_squared_error: 8.9425\n",
      "Epoch 294: val_loss did not improve from 61.28563\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.8242 - root_mean_squared_error: 16.2119 - val_loss: 63.1494 - val_root_mean_squared_error: 7.9467 - lr: 0.0010\n",
      "Epoch 295/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 144.3398 - root_mean_squared_error: 12.0141\n",
      "Epoch 295: val_loss did not improve from 61.28563\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.3424 - root_mean_squared_error: 16.2278 - val_loss: 63.7812 - val_root_mean_squared_error: 7.9863 - lr: 0.0010\n",
      "Epoch 296/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 192.5547 - root_mean_squared_error: 13.8764\n",
      "Epoch 296: val_loss did not improve from 61.28563\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.8997 - root_mean_squared_error: 16.2142 - val_loss: 62.4011 - val_root_mean_squared_error: 7.8994 - lr: 0.0010\n",
      "Epoch 297/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 93.6786 - root_mean_squared_error: 9.6788\n",
      "Epoch 297: val_loss did not improve from 61.28563\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.0694 - root_mean_squared_error: 16.2194 - val_loss: 61.9636 - val_root_mean_squared_error: 7.8717 - lr: 0.0010\n",
      "Epoch 298/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 393.9244 - root_mean_squared_error: 19.8475\n",
      "Epoch 298: val_loss improved from 61.28563 to 58.43412, saving model to Model\\298-264.6294-58.4341.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.6294 - root_mean_squared_error: 16.2674 - val_loss: 58.4341 - val_root_mean_squared_error: 7.6442 - lr: 0.0010\n",
      "Epoch 299/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 366.4967 - root_mean_squared_error: 19.1441\n",
      "Epoch 299: val_loss did not improve from 58.43412\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.3207 - root_mean_squared_error: 16.2272 - val_loss: 59.4571 - val_root_mean_squared_error: 7.7108 - lr: 0.0010\n",
      "Epoch 300/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 99.0318 - root_mean_squared_error: 9.9515\n",
      "Epoch 300: val_loss did not improve from 58.43412\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.8738 - root_mean_squared_error: 16.2134 - val_loss: 60.3741 - val_root_mean_squared_error: 7.7701 - lr: 0.0010\n",
      "Epoch 301/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 354.6510 - root_mean_squared_error: 18.8322\n",
      "Epoch 301: val_loss did not improve from 58.43412\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.3580 - root_mean_squared_error: 16.2283 - val_loss: 58.4867 - val_root_mean_squared_error: 7.6477 - lr: 0.0010\n",
      "Epoch 302/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 291.8558 - root_mean_squared_error: 17.0838\n",
      "Epoch 302: val_loss improved from 58.43412 to 57.80158, saving model to Model\\302-264.0611-57.8016.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 264.0611 - root_mean_squared_error: 16.2500 - val_loss: 57.8016 - val_root_mean_squared_error: 7.6027 - lr: 0.0010\n",
      "Epoch 303/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 136.5438 - root_mean_squared_error: 11.6852\n",
      "Epoch 303: val_loss did not improve from 57.80158\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.3362 - root_mean_squared_error: 16.1968 - val_loss: 61.2992 - val_root_mean_squared_error: 7.8294 - lr: 0.0010\n",
      "Epoch 304/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 689.8946 - root_mean_squared_error: 26.2658\n",
      "Epoch 304: val_loss improved from 57.80158 to 55.60212, saving model to Model\\304-262.8324-55.6021.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.8324 - root_mean_squared_error: 16.2121 - val_loss: 55.6021 - val_root_mean_squared_error: 7.4567 - lr: 0.0010\n",
      "Epoch 305/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 428.1235 - root_mean_squared_error: 20.6911\n",
      "Epoch 305: val_loss improved from 55.60212 to 54.81868, saving model to Model\\305-262.3569-54.8187.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.3569 - root_mean_squared_error: 16.1974 - val_loss: 54.8187 - val_root_mean_squared_error: 7.4040 - lr: 0.0010\n",
      "Epoch 306/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 73.5385 - root_mean_squared_error: 8.5755\n",
      "Epoch 306: val_loss did not improve from 54.81868\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.7000 - root_mean_squared_error: 16.1771 - val_loss: 63.0739 - val_root_mean_squared_error: 7.9419 - lr: 0.0010\n",
      "Epoch 307/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 534.0206 - root_mean_squared_error: 23.1089\n",
      "Epoch 307: val_loss improved from 54.81868 to 54.44933, saving model to Model\\307-260.1912-54.4493.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.1912 - root_mean_squared_error: 16.1304 - val_loss: 54.4493 - val_root_mean_squared_error: 7.3790 - lr: 0.0010\n",
      "Epoch 308/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 82.3023 - root_mean_squared_error: 9.0721\n",
      "Epoch 308: val_loss did not improve from 54.44933\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.6469 - root_mean_squared_error: 16.1446 - val_loss: 60.7736 - val_root_mean_squared_error: 7.7957 - lr: 0.0010\n",
      "Epoch 309/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 318.1842 - root_mean_squared_error: 17.8377\n",
      "Epoch 309: val_loss improved from 54.44933 to 53.47901, saving model to Model\\309-263.2823-53.4790.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 263.2823 - root_mean_squared_error: 16.2260 - val_loss: 53.4790 - val_root_mean_squared_error: 7.3129 - lr: 0.0010\n",
      "Epoch 310/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 90.2884 - root_mean_squared_error: 9.5020\n",
      "Epoch 310: val_loss did not improve from 53.47901\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.8385 - root_mean_squared_error: 16.1505 - val_loss: 56.4579 - val_root_mean_squared_error: 7.5138 - lr: 0.0010\n",
      "Epoch 311/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 143.6611 - root_mean_squared_error: 11.9859\n",
      "Epoch 311: val_loss did not improve from 53.47901\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 260.0162 - root_mean_squared_error: 16.1250 - val_loss: 56.9421 - val_root_mean_squared_error: 7.5460 - lr: 0.0010\n",
      "Epoch 312/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 52.8275 - root_mean_squared_error: 7.2683\n",
      "Epoch 312: val_loss did not improve from 53.47901\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.2440 - root_mean_squared_error: 16.1321 - val_loss: 54.8202 - val_root_mean_squared_error: 7.4041 - lr: 0.0010\n",
      "Epoch 313/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 82.2481 - root_mean_squared_error: 9.0691\n",
      "Epoch 313: val_loss did not improve from 53.47901\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 264.2067 - root_mean_squared_error: 16.2544 - val_loss: 61.2208 - val_root_mean_squared_error: 7.8244 - lr: 0.0010\n",
      "Epoch 314/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 236.4485 - root_mean_squared_error: 15.3769\n",
      "Epoch 314: val_loss improved from 53.47901 to 53.20590, saving model to Model\\314-261.6434-53.2059.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 261.6434 - root_mean_squared_error: 16.1754 - val_loss: 53.2059 - val_root_mean_squared_error: 7.2942 - lr: 0.0010\n",
      "Epoch 315/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 69.7160 - root_mean_squared_error: 8.3496\n",
      "Epoch 315: val_loss did not improve from 53.20590\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.6856 - root_mean_squared_error: 16.2076 - val_loss: 55.3249 - val_root_mean_squared_error: 7.4381 - lr: 0.0010\n",
      "Epoch 316/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 130.7875 - root_mean_squared_error: 11.4362\n",
      "Epoch 316: val_loss did not improve from 53.20590\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.3549 - root_mean_squared_error: 16.1974 - val_loss: 56.8794 - val_root_mean_squared_error: 7.5418 - lr: 0.0010\n",
      "Epoch 317/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 218.2490 - root_mean_squared_error: 14.7733\n",
      "Epoch 317: val_loss did not improve from 53.20590\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.2772 - root_mean_squared_error: 16.1021 - val_loss: 54.9840 - val_root_mean_squared_error: 7.4151 - lr: 0.0010\n",
      "Epoch 318/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 261.7191 - root_mean_squared_error: 16.1777\n",
      "Epoch 318: val_loss did not improve from 53.20590\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.0905 - root_mean_squared_error: 16.0963 - val_loss: 54.0328 - val_root_mean_squared_error: 7.3507 - lr: 0.0010\n",
      "Epoch 319/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 346.9388 - root_mean_squared_error: 18.6263\n",
      "Epoch 319: val_loss did not improve from 53.20590\n",
      "\n",
      "Epoch 319: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.2288 - root_mean_squared_error: 16.1006 - val_loss: 54.3914 - val_root_mean_squared_error: 7.3751 - lr: 0.0010\n",
      "Epoch 320/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 387.6594 - root_mean_squared_error: 19.6891\n",
      "Epoch 320: val_loss did not improve from 53.20590\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.3260 - root_mean_squared_error: 16.1346 - val_loss: 54.7423 - val_root_mean_squared_error: 7.3988 - lr: 9.5000e-04\n",
      "Epoch 321/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 255.6313 - root_mean_squared_error: 15.9885\n",
      "Epoch 321: val_loss did not improve from 53.20590\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 262.2316 - root_mean_squared_error: 16.1936 - val_loss: 53.8040 - val_root_mean_squared_error: 7.3351 - lr: 9.5000e-04\n",
      "Epoch 322/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 64.0349 - root_mean_squared_error: 8.0022\n",
      "Epoch 322: val_loss did not improve from 53.20590\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.0023 - root_mean_squared_error: 16.1556 - val_loss: 53.8380 - val_root_mean_squared_error: 7.3374 - lr: 9.5000e-04\n",
      "Epoch 323/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 158.4695 - root_mean_squared_error: 12.5885\n",
      "Epoch 323: val_loss improved from 53.20590 to 52.68279, saving model to Model\\323-258.1984-52.6828.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.1984 - root_mean_squared_error: 16.0686 - val_loss: 52.6828 - val_root_mean_squared_error: 7.2583 - lr: 9.5000e-04\n",
      "Epoch 324/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 111.8685 - root_mean_squared_error: 10.5768\n",
      "Epoch 324: val_loss did not improve from 52.68279\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 259.2318 - root_mean_squared_error: 16.1007 - val_loss: 52.9908 - val_root_mean_squared_error: 7.2795 - lr: 9.5000e-04\n",
      "Epoch 325/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 78.4796 - root_mean_squared_error: 8.8589\n",
      "Epoch 325: val_loss improved from 52.68279 to 50.75338, saving model to Model\\325-258.2466-50.7534.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 258.2466 - root_mean_squared_error: 16.0701 - val_loss: 50.7534 - val_root_mean_squared_error: 7.1241 - lr: 9.5000e-04\n",
      "Epoch 326/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 188.7876 - root_mean_squared_error: 13.7400\n",
      "Epoch 326: val_loss did not improve from 50.75338\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 260.2103 - root_mean_squared_error: 16.1310 - val_loss: 61.6549 - val_root_mean_squared_error: 7.8521 - lr: 9.5000e-04\n",
      "Epoch 327/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 360.8693 - root_mean_squared_error: 18.9966\n",
      "Epoch 327: val_loss improved from 50.75338 to 49.91986, saving model to Model\\327-257.8582-49.9199.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 257.8582 - root_mean_squared_error: 16.0580 - val_loss: 49.9199 - val_root_mean_squared_error: 7.0654 - lr: 9.5000e-04\n",
      "Epoch 328/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 642.3239 - root_mean_squared_error: 25.3441\n",
      "Epoch 328: val_loss improved from 49.91986 to 49.88367, saving model to Model\\328-257.3103-49.8837.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 257.3103 - root_mean_squared_error: 16.0409 - val_loss: 49.8837 - val_root_mean_squared_error: 7.0628 - lr: 9.5000e-04\n",
      "Epoch 329/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 34.4667 - root_mean_squared_error: 5.8708\n",
      "Epoch 329: val_loss improved from 49.88367 to 48.44932, saving model to Model\\329-261.2898-48.4493.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 261.2898 - root_mean_squared_error: 16.1645 - val_loss: 48.4493 - val_root_mean_squared_error: 6.9606 - lr: 9.5000e-04\n",
      "Epoch 330/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 114.6624 - root_mean_squared_error: 10.7081\n",
      "Epoch 330: val_loss did not improve from 48.44932\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.8910 - root_mean_squared_error: 16.0590 - val_loss: 55.2481 - val_root_mean_squared_error: 7.4329 - lr: 9.5000e-04\n",
      "Epoch 331/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 480.2565 - root_mean_squared_error: 21.9148\n",
      "Epoch 331: val_loss did not improve from 48.44932\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 258.0833 - root_mean_squared_error: 16.0650 - val_loss: 49.6634 - val_root_mean_squared_error: 7.0472 - lr: 9.5000e-04\n",
      "Epoch 332/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 498.5818 - root_mean_squared_error: 22.3289\n",
      "Epoch 332: val_loss did not improve from 48.44932\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.1594 - root_mean_squared_error: 16.0362 - val_loss: 50.2021 - val_root_mean_squared_error: 7.0853 - lr: 9.5000e-04\n",
      "Epoch 333/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 93.8546 - root_mean_squared_error: 9.6879\n",
      "Epoch 333: val_loss did not improve from 48.44932\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.3705 - root_mean_squared_error: 16.0116 - val_loss: 50.1331 - val_root_mean_squared_error: 7.0805 - lr: 9.5000e-04\n",
      "Epoch 334/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 81.0070 - root_mean_squared_error: 9.0004\n",
      "Epoch 334: val_loss did not improve from 48.44932\n",
      "\n",
      "Epoch 334: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.4954 - root_mean_squared_error: 16.0155 - val_loss: 51.6726 - val_root_mean_squared_error: 7.1884 - lr: 9.5000e-04\n",
      "Epoch 335/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 630.3291 - root_mean_squared_error: 25.1064\n",
      "Epoch 335: val_loss did not improve from 48.44932\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.5989 - root_mean_squared_error: 16.0499 - val_loss: 49.1239 - val_root_mean_squared_error: 7.0088 - lr: 9.0250e-04\n",
      "Epoch 336/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 260.0904 - root_mean_squared_error: 16.1273\n",
      "Epoch 336: val_loss did not improve from 48.44932\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.5225 - root_mean_squared_error: 16.0163 - val_loss: 50.1298 - val_root_mean_squared_error: 7.0802 - lr: 9.0250e-04\n",
      "Epoch 337/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 27.4981 - root_mean_squared_error: 5.2439\n",
      "Epoch 337: val_loss improved from 48.44932 to 48.25507, saving model to Model\\337-256.3492-48.2551.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.3492 - root_mean_squared_error: 16.0109 - val_loss: 48.2551 - val_root_mean_squared_error: 6.9466 - lr: 9.0250e-04\n",
      "Epoch 338/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 37.8348 - root_mean_squared_error: 6.1510\n",
      "Epoch 338: val_loss did not improve from 48.25507\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.9902 - root_mean_squared_error: 15.9997 - val_loss: 51.1337 - val_root_mean_squared_error: 7.1508 - lr: 9.0250e-04\n",
      "Epoch 339/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 51.8335 - root_mean_squared_error: 7.1995\n",
      "Epoch 339: val_loss improved from 48.25507 to 46.72768, saving model to Model\\339-255.9645-46.7277.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.9645 - root_mean_squared_error: 15.9989 - val_loss: 46.7277 - val_root_mean_squared_error: 6.8358 - lr: 9.0250e-04\n",
      "Epoch 340/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 473.0907 - root_mean_squared_error: 21.7506\n",
      "Epoch 340: val_loss did not improve from 46.72768\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.4652 - root_mean_squared_error: 16.0145 - val_loss: 51.5817 - val_root_mean_squared_error: 7.1820 - lr: 9.0250e-04\n",
      "Epoch 341/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 52.5053 - root_mean_squared_error: 7.2461\n",
      "Epoch 341: val_loss did not improve from 46.72768\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.7940 - root_mean_squared_error: 16.0248 - val_loss: 48.1863 - val_root_mean_squared_error: 6.9416 - lr: 9.0250e-04\n",
      "Epoch 342/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 87.0747 - root_mean_squared_error: 9.3314\n",
      "Epoch 342: val_loss did not improve from 46.72768\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.1752 - root_mean_squared_error: 16.0055 - val_loss: 52.8329 - val_root_mean_squared_error: 7.2686 - lr: 9.0250e-04\n",
      "Epoch 343/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 545.2581 - root_mean_squared_error: 23.3508\n",
      "Epoch 343: val_loss did not improve from 46.72768\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.4677 - root_mean_squared_error: 16.0146 - val_loss: 48.4323 - val_root_mean_squared_error: 6.9593 - lr: 9.0250e-04\n",
      "Epoch 344/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 146.3540 - root_mean_squared_error: 12.0977\n",
      "Epoch 344: val_loss did not improve from 46.72768\n",
      "\n",
      "Epoch 344: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.9154 - root_mean_squared_error: 16.0286 - val_loss: 49.4058 - val_root_mean_squared_error: 7.0289 - lr: 9.0250e-04\n",
      "Epoch 345/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 64.5562 - root_mean_squared_error: 8.0347\n",
      "Epoch 345: val_loss did not improve from 46.72768\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.9299 - root_mean_squared_error: 15.9665 - val_loss: 50.1601 - val_root_mean_squared_error: 7.0824 - lr: 8.5737e-04\n",
      "Epoch 346/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 336.0988 - root_mean_squared_error: 18.3330\n",
      "Epoch 346: val_loss did not improve from 46.72768\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.1225 - root_mean_squared_error: 15.9726 - val_loss: 47.6719 - val_root_mean_squared_error: 6.9045 - lr: 8.5737e-04\n",
      "Epoch 347/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 677.8856 - root_mean_squared_error: 26.0362\n",
      "Epoch 347: val_loss improved from 46.72768 to 45.74974, saving model to Model\\347-256.1345-45.7497.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.1345 - root_mean_squared_error: 16.0042 - val_loss: 45.7497 - val_root_mean_squared_error: 6.7639 - lr: 8.5737e-04\n",
      "Epoch 348/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 312.3763 - root_mean_squared_error: 17.6742\n",
      "Epoch 348: val_loss did not improve from 45.74974\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.3331 - root_mean_squared_error: 15.9791 - val_loss: 46.9640 - val_root_mean_squared_error: 6.8530 - lr: 8.5737e-04\n",
      "Epoch 349/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 132.5128 - root_mean_squared_error: 11.5114\n",
      "Epoch 349: val_loss did not improve from 45.74974\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.6908 - root_mean_squared_error: 15.9903 - val_loss: 47.8714 - val_root_mean_squared_error: 6.9189 - lr: 8.5737e-04\n",
      "Epoch 350/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 86.7172 - root_mean_squared_error: 9.3122\n",
      "Epoch 350: val_loss did not improve from 45.74974\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.5256 - root_mean_squared_error: 15.9852 - val_loss: 47.7060 - val_root_mean_squared_error: 6.9069 - lr: 8.5737e-04\n",
      "Epoch 351/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 52.5920 - root_mean_squared_error: 7.2520\n",
      "Epoch 351: val_loss improved from 45.74974 to 45.58358, saving model to Model\\351-254.4457-45.5836.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 254.4457 - root_mean_squared_error: 15.9514 - val_loss: 45.5836 - val_root_mean_squared_error: 6.7516 - lr: 8.5737e-04\n",
      "Epoch 352/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 328.6196 - root_mean_squared_error: 18.1279\n",
      "Epoch 352: val_loss did not improve from 45.58358\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.3340 - root_mean_squared_error: 15.9792 - val_loss: 48.5617 - val_root_mean_squared_error: 6.9686 - lr: 8.5737e-04\n",
      "Epoch 353/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 78.2807 - root_mean_squared_error: 8.8476\n",
      "Epoch 353: val_loss did not improve from 45.58358\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.2390 - root_mean_squared_error: 15.9762 - val_loss: 51.8888 - val_root_mean_squared_error: 7.2034 - lr: 8.5737e-04\n",
      "Epoch 354/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 243.9131 - root_mean_squared_error: 15.6177\n",
      "Epoch 354: val_loss improved from 45.58358 to 43.93551, saving model to Model\\354-254.0437-43.9355.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.0437 - root_mean_squared_error: 15.9387 - val_loss: 43.9355 - val_root_mean_squared_error: 6.6284 - lr: 8.5737e-04\n",
      "Epoch 355/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 466.5205 - root_mean_squared_error: 21.5991\n",
      "Epoch 355: val_loss did not improve from 43.93551\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.9385 - root_mean_squared_error: 15.9354 - val_loss: 48.7496 - val_root_mean_squared_error: 6.9821 - lr: 8.5737e-04\n",
      "Epoch 356/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 281.9343 - root_mean_squared_error: 16.7909\n",
      "Epoch 356: val_loss improved from 43.93551 to 43.54021, saving model to Model\\356-255.3925-43.5402.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.3925 - root_mean_squared_error: 15.9810 - val_loss: 43.5402 - val_root_mean_squared_error: 6.5985 - lr: 8.5737e-04\n",
      "Epoch 357/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 87.9091 - root_mean_squared_error: 9.3760\n",
      "Epoch 357: val_loss did not improve from 43.54021\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 257.5819 - root_mean_squared_error: 16.0494 - val_loss: 52.6860 - val_root_mean_squared_error: 7.2585 - lr: 8.5737e-04\n",
      "Epoch 358/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 87.0101 - root_mean_squared_error: 9.3279\n",
      "Epoch 358: val_loss improved from 43.54021 to 42.83869, saving model to Model\\358-257.9011-42.8387.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 257.9011 - root_mean_squared_error: 16.0593 - val_loss: 42.8387 - val_root_mean_squared_error: 6.5451 - lr: 8.5737e-04\n",
      "Epoch 359/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 607.9674 - root_mean_squared_error: 24.6570\n",
      "Epoch 359: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.2426 - root_mean_squared_error: 15.9450 - val_loss: 45.2538 - val_root_mean_squared_error: 6.7271 - lr: 8.5737e-04\n",
      "Epoch 360/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 151.6421 - root_mean_squared_error: 12.3143\n",
      "Epoch 360: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.4691 - root_mean_squared_error: 15.9207 - val_loss: 44.6216 - val_root_mean_squared_error: 6.6799 - lr: 8.5737e-04\n",
      "Epoch 361/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 57.4555 - root_mean_squared_error: 7.5799\n",
      "Epoch 361: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 254.4975 - root_mean_squared_error: 15.9530 - val_loss: 45.6622 - val_root_mean_squared_error: 6.7574 - lr: 8.5737e-04\n",
      "Epoch 362/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 87.5941 - root_mean_squared_error: 9.3592\n",
      "Epoch 362: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 255.4832 - root_mean_squared_error: 15.9838 - val_loss: 49.3160 - val_root_mean_squared_error: 7.0225 - lr: 8.5737e-04\n",
      "Epoch 363/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 107.7396 - root_mean_squared_error: 10.3798\n",
      "Epoch 363: val_loss did not improve from 42.83869\n",
      "\n",
      "Epoch 363: ReduceLROnPlateau reducing learning rate to 0.0008145062311086804.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 256.1475 - root_mean_squared_error: 16.0046 - val_loss: 44.9434 - val_root_mean_squared_error: 6.7040 - lr: 8.5737e-04\n",
      "Epoch 364/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 277.1839 - root_mean_squared_error: 16.6488\n",
      "Epoch 364: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.8029 - root_mean_squared_error: 15.8998 - val_loss: 46.3063 - val_root_mean_squared_error: 6.8049 - lr: 8.1451e-04\n",
      "Epoch 365/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 93.9035 - root_mean_squared_error: 9.6904\n",
      "Epoch 365: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.7587 - root_mean_squared_error: 15.8984 - val_loss: 45.1211 - val_root_mean_squared_error: 6.7172 - lr: 8.1451e-04\n",
      "Epoch 366/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.7910 - root_mean_squared_error: 6.8404\n",
      "Epoch 366: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.5967 - root_mean_squared_error: 15.8933 - val_loss: 46.7585 - val_root_mean_squared_error: 6.8380 - lr: 8.1451e-04\n",
      "Epoch 367/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 324.9563 - root_mean_squared_error: 18.0265\n",
      "Epoch 367: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.3741 - root_mean_squared_error: 15.9177 - val_loss: 48.9035 - val_root_mean_squared_error: 6.9931 - lr: 8.1451e-04\n",
      "Epoch 368/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 341.0155 - root_mean_squared_error: 18.4666\n",
      "Epoch 368: val_loss did not improve from 42.83869\n",
      "\n",
      "Epoch 368: ReduceLROnPlateau reducing learning rate to 0.0007737808919046074.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.4639 - root_mean_squared_error: 15.9205 - val_loss: 43.2119 - val_root_mean_squared_error: 6.5736 - lr: 8.1451e-04\n",
      "Epoch 369/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 356.8611 - root_mean_squared_error: 18.8908\n",
      "Epoch 369: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.1081 - root_mean_squared_error: 15.8779 - val_loss: 46.8678 - val_root_mean_squared_error: 6.8460 - lr: 7.7378e-04\n",
      "Epoch 370/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 364.8553 - root_mean_squared_error: 19.1012\n",
      "Epoch 370: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.8589 - root_mean_squared_error: 15.8701 - val_loss: 44.2358 - val_root_mean_squared_error: 6.6510 - lr: 7.7378e-04\n",
      "Epoch 371/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 247.3879 - root_mean_squared_error: 15.7286\n",
      "Epoch 371: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.2005 - root_mean_squared_error: 15.8808 - val_loss: 44.6138 - val_root_mean_squared_error: 6.6794 - lr: 7.7378e-04\n",
      "Epoch 372/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 370.6321 - root_mean_squared_error: 19.2518\n",
      "Epoch 372: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.4186 - root_mean_squared_error: 15.9191 - val_loss: 43.8879 - val_root_mean_squared_error: 6.6248 - lr: 7.7378e-04\n",
      "Epoch 373/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 306.3635 - root_mean_squared_error: 17.5032\n",
      "Epoch 373: val_loss did not improve from 42.83869\n",
      "\n",
      "Epoch 373: ReduceLROnPlateau reducing learning rate to 0.000735091819660738.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 253.1655 - root_mean_squared_error: 15.9112 - val_loss: 44.0323 - val_root_mean_squared_error: 6.6357 - lr: 7.7378e-04\n",
      "Epoch 374/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 99.6702 - root_mean_squared_error: 9.9835\n",
      "Epoch 374: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.6339 - root_mean_squared_error: 15.8630 - val_loss: 43.7405 - val_root_mean_squared_error: 6.6137 - lr: 7.3509e-04\n",
      "Epoch 375/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 118.2120 - root_mean_squared_error: 10.8725\n",
      "Epoch 375: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.0059 - root_mean_squared_error: 15.8747 - val_loss: 44.8317 - val_root_mean_squared_error: 6.6956 - lr: 7.3509e-04\n",
      "Epoch 376/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 83.0399 - root_mean_squared_error: 9.1126\n",
      "Epoch 376: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.7035 - root_mean_squared_error: 15.8652 - val_loss: 44.7207 - val_root_mean_squared_error: 6.6874 - lr: 7.3509e-04\n",
      "Epoch 377/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 32.5960 - root_mean_squared_error: 5.7093\n",
      "Epoch 377: val_loss did not improve from 42.83869\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.1693 - root_mean_squared_error: 15.8483 - val_loss: 43.3514 - val_root_mean_squared_error: 6.5842 - lr: 7.3509e-04\n",
      "Epoch 378/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 370.2520 - root_mean_squared_error: 19.2419\n",
      "Epoch 378: val_loss improved from 42.83869 to 42.47344, saving model to Model\\378-252.0099-42.4734.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 252.0099 - root_mean_squared_error: 15.8748 - val_loss: 42.4734 - val_root_mean_squared_error: 6.5172 - lr: 7.3509e-04\n",
      "Epoch 379/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.8294 - root_mean_squared_error: 6.8432\n",
      "Epoch 379: val_loss did not improve from 42.47344\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.5535 - root_mean_squared_error: 15.8289 - val_loss: 46.5572 - val_root_mean_squared_error: 6.8233 - lr: 7.3509e-04\n",
      "Epoch 380/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 262.6745 - root_mean_squared_error: 16.2072\n",
      "Epoch 380: val_loss did not improve from 42.47344\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.1174 - root_mean_squared_error: 15.8782 - val_loss: 45.2780 - val_root_mean_squared_error: 6.7289 - lr: 7.3509e-04\n",
      "Epoch 381/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 250.5837 - root_mean_squared_error: 15.8298\n",
      "Epoch 381: val_loss did not improve from 42.47344\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.4913 - root_mean_squared_error: 15.8269 - val_loss: 43.7960 - val_root_mean_squared_error: 6.6179 - lr: 7.3509e-04\n",
      "Epoch 382/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 83.8842 - root_mean_squared_error: 9.1588\n",
      "Epoch 382: val_loss did not improve from 42.47344\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.5383 - root_mean_squared_error: 15.8915 - val_loss: 45.6249 - val_root_mean_squared_error: 6.7546 - lr: 7.3509e-04\n",
      "Epoch 383/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 191.4878 - root_mean_squared_error: 13.8379\n",
      "Epoch 383: val_loss did not improve from 42.47344\n",
      "\n",
      "Epoch 383: ReduceLROnPlateau reducing learning rate to 0.0006983372120885178.\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 250.7493 - root_mean_squared_error: 15.8351 - val_loss: 43.1886 - val_root_mean_squared_error: 6.5718 - lr: 7.3509e-04\n",
      "Epoch 384/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 92.2487 - root_mean_squared_error: 9.6046\n",
      "Epoch 384: val_loss improved from 42.47344 to 41.82402, saving model to Model\\384-252.4305-41.8240.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 252.4305 - root_mean_squared_error: 15.8881 - val_loss: 41.8240 - val_root_mean_squared_error: 6.4671 - lr: 6.9834e-04\n",
      "Epoch 385/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 114.2790 - root_mean_squared_error: 10.6901\n",
      "Epoch 385: val_loss did not improve from 41.82402\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.5563 - root_mean_squared_error: 15.8290 - val_loss: 42.9776 - val_root_mean_squared_error: 6.5557 - lr: 6.9834e-04\n",
      "Epoch 386/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 292.2103 - root_mean_squared_error: 17.0942\n",
      "Epoch 386: val_loss did not improve from 41.82402\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.5143 - root_mean_squared_error: 15.8907 - val_loss: 43.0620 - val_root_mean_squared_error: 6.5622 - lr: 6.9834e-04\n",
      "Epoch 387/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 74.3077 - root_mean_squared_error: 8.6202\n",
      "Epoch 387: val_loss did not improve from 41.82402\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 251.0517 - root_mean_squared_error: 15.8446 - val_loss: 42.3446 - val_root_mean_squared_error: 6.5073 - lr: 6.9834e-04\n",
      "Epoch 388/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 286.2813 - root_mean_squared_error: 16.9199\n",
      "Epoch 388: val_loss improved from 41.82402 to 41.27715, saving model to Model\\388-250.5411-41.2771.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 250.5411 - root_mean_squared_error: 15.8285 - val_loss: 41.2771 - val_root_mean_squared_error: 6.4247 - lr: 6.9834e-04\n",
      "Epoch 389/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 28.4163 - root_mean_squared_error: 5.3307\n",
      "Epoch 389: val_loss did not improve from 41.27715\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.5966 - root_mean_squared_error: 15.7986 - val_loss: 45.0619 - val_root_mean_squared_error: 6.7128 - lr: 6.9834e-04\n",
      "Epoch 390/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 309.0689 - root_mean_squared_error: 17.5804\n",
      "Epoch 390: val_loss improved from 41.27715 to 40.42071, saving model to Model\\390-249.6130-40.4207.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 249.6130 - root_mean_squared_error: 15.7991 - val_loss: 40.4207 - val_root_mean_squared_error: 6.3577 - lr: 6.9834e-04\n",
      "Epoch 391/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 246.3609 - root_mean_squared_error: 15.6959\n",
      "Epoch 391: val_loss did not improve from 40.42071\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.4634 - root_mean_squared_error: 15.8260 - val_loss: 42.1512 - val_root_mean_squared_error: 6.4924 - lr: 6.9834e-04\n",
      "Epoch 392/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 331.0652 - root_mean_squared_error: 18.1952\n",
      "Epoch 392: val_loss did not improve from 40.42071\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.9947 - root_mean_squared_error: 15.8428 - val_loss: 42.9530 - val_root_mean_squared_error: 6.5539 - lr: 6.9834e-04\n",
      "Epoch 393/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 66.3534 - root_mean_squared_error: 8.1458\n",
      "Epoch 393: val_loss improved from 40.42071 to 39.85891, saving model to Model\\393-249.7564-39.8589.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.7564 - root_mean_squared_error: 15.8037 - val_loss: 39.8589 - val_root_mean_squared_error: 6.3134 - lr: 6.9834e-04\n",
      "Epoch 394/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 19.6829 - root_mean_squared_error: 4.4365\n",
      "Epoch 394: val_loss did not improve from 39.85891\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.9985 - root_mean_squared_error: 15.8113 - val_loss: 42.0886 - val_root_mean_squared_error: 6.4876 - lr: 6.9834e-04\n",
      "Epoch 395/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 454.6779 - root_mean_squared_error: 21.3232\n",
      "Epoch 395: val_loss did not improve from 39.85891\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.9607 - root_mean_squared_error: 15.8101 - val_loss: 41.1587 - val_root_mean_squared_error: 6.4155 - lr: 6.9834e-04\n",
      "Epoch 396/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 28.9563 - root_mean_squared_error: 5.3811\n",
      "Epoch 396: val_loss did not improve from 39.85891\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.3693 - root_mean_squared_error: 15.8861 - val_loss: 42.0571 - val_root_mean_squared_error: 6.4851 - lr: 6.9834e-04\n",
      "Epoch 397/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 108.4259 - root_mean_squared_error: 10.4128\n",
      "Epoch 397: val_loss did not improve from 39.85891\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.2153 - root_mean_squared_error: 15.8182 - val_loss: 42.2333 - val_root_mean_squared_error: 6.4987 - lr: 6.9834e-04\n",
      "Epoch 398/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 264.8208 - root_mean_squared_error: 16.2733\n",
      "Epoch 398: val_loss did not improve from 39.85891\n",
      "\n",
      "Epoch 398: ReduceLROnPlateau reducing learning rate to 0.0006634203542489559.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.5483 - root_mean_squared_error: 15.7971 - val_loss: 41.9759 - val_root_mean_squared_error: 6.4789 - lr: 6.9834e-04\n",
      "Epoch 399/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 122.3855 - root_mean_squared_error: 11.0628\n",
      "Epoch 399: val_loss improved from 39.85891 to 39.43005, saving model to Model\\399-250.1220-39.4300.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 250.1220 - root_mean_squared_error: 15.8152 - val_loss: 39.4300 - val_root_mean_squared_error: 6.2793 - lr: 6.6342e-04\n",
      "Epoch 400/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 796.2358 - root_mean_squared_error: 28.2177\n",
      "Epoch 400: val_loss did not improve from 39.43005\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 252.3158 - root_mean_squared_error: 15.8844 - val_loss: 43.8266 - val_root_mean_squared_error: 6.6202 - lr: 6.6342e-04\n",
      "Epoch 401/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 139.0570 - root_mean_squared_error: 11.7922\n",
      "Epoch 401: val_loss improved from 39.43005 to 39.23625, saving model to Model\\401-250.5751-39.2362.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.5751 - root_mean_squared_error: 15.8296 - val_loss: 39.2362 - val_root_mean_squared_error: 6.2639 - lr: 6.6342e-04\n",
      "Epoch 402/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 198.3403 - root_mean_squared_error: 14.0833\n",
      "Epoch 402: val_loss did not improve from 39.23625\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.8004 - root_mean_squared_error: 15.7734 - val_loss: 44.0964 - val_root_mean_squared_error: 6.6405 - lr: 6.6342e-04\n",
      "Epoch 403/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 324.9334 - root_mean_squared_error: 18.0259\n",
      "Epoch 403: val_loss did not improve from 39.23625\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.5424 - root_mean_squared_error: 15.7652 - val_loss: 42.1587 - val_root_mean_squared_error: 6.4930 - lr: 6.6342e-04\n",
      "Epoch 404/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 34.1537 - root_mean_squared_error: 5.8441\n",
      "Epoch 404: val_loss did not improve from 39.23625\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.0726 - root_mean_squared_error: 15.7503 - val_loss: 40.2128 - val_root_mean_squared_error: 6.3414 - lr: 6.6342e-04\n",
      "Epoch 405/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 371.7290 - root_mean_squared_error: 19.2803\n",
      "Epoch 405: val_loss did not improve from 39.23625\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.3553 - root_mean_squared_error: 15.7910 - val_loss: 39.5158 - val_root_mean_squared_error: 6.2862 - lr: 6.6342e-04\n",
      "Epoch 406/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 32.4548 - root_mean_squared_error: 5.6969\n",
      "Epoch 406: val_loss did not improve from 39.23625\n",
      "\n",
      "Epoch 406: ReduceLROnPlateau reducing learning rate to 0.0006302493420662358.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.3431 - root_mean_squared_error: 15.7589 - val_loss: 44.6333 - val_root_mean_squared_error: 6.6808 - lr: 6.6342e-04\n",
      "Epoch 407/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 275.4843 - root_mean_squared_error: 16.5977\n",
      "Epoch 407: val_loss did not improve from 39.23625\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.4863 - root_mean_squared_error: 15.7634 - val_loss: 41.3069 - val_root_mean_squared_error: 6.4270 - lr: 6.3025e-04\n",
      "Epoch 408/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 50.5990 - root_mean_squared_error: 7.1133\n",
      "Epoch 408: val_loss did not improve from 39.23625\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 250.2187 - root_mean_squared_error: 15.8183 - val_loss: 40.3236 - val_root_mean_squared_error: 6.3501 - lr: 6.3025e-04\n",
      "Epoch 409/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 48.4130 - root_mean_squared_error: 6.9579\n",
      "Epoch 409: val_loss did not improve from 39.23625\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.6071 - root_mean_squared_error: 15.7355 - val_loss: 42.7267 - val_root_mean_squared_error: 6.5366 - lr: 6.3025e-04\n",
      "Epoch 410/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 463.7692 - root_mean_squared_error: 21.5353\n",
      "Epoch 410: val_loss improved from 39.23625 to 39.05314, saving model to Model\\410-248.5592-39.0531.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.5592 - root_mean_squared_error: 15.7658 - val_loss: 39.0531 - val_root_mean_squared_error: 6.2493 - lr: 6.3025e-04\n",
      "Epoch 411/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 281.8860 - root_mean_squared_error: 16.7895\n",
      "Epoch 411: val_loss did not improve from 39.05314\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.4228 - root_mean_squared_error: 15.7614 - val_loss: 40.4551 - val_root_mean_squared_error: 6.3604 - lr: 6.3025e-04\n",
      "Epoch 412/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 32.6120 - root_mean_squared_error: 5.7107\n",
      "Epoch 412: val_loss did not improve from 39.05314\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.3416 - root_mean_squared_error: 15.7589 - val_loss: 41.1853 - val_root_mean_squared_error: 6.4176 - lr: 6.3025e-04\n",
      "Epoch 413/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 538.7391 - root_mean_squared_error: 23.2108\n",
      "Epoch 413: val_loss improved from 39.05314 to 38.92665, saving model to Model\\413-248.6482-38.9267.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.6482 - root_mean_squared_error: 15.7686 - val_loss: 38.9267 - val_root_mean_squared_error: 6.2391 - lr: 6.3025e-04\n",
      "Epoch 414/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 47.6307 - root_mean_squared_error: 6.9015\n",
      "Epoch 414: val_loss did not improve from 38.92665\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.8580 - root_mean_squared_error: 15.7117 - val_loss: 39.7261 - val_root_mean_squared_error: 6.3029 - lr: 6.3025e-04\n",
      "Epoch 415/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 59.6154 - root_mean_squared_error: 7.7211\n",
      "Epoch 415: val_loss did not improve from 38.92665\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.6126 - root_mean_squared_error: 15.7357 - val_loss: 39.0558 - val_root_mean_squared_error: 6.2495 - lr: 6.3025e-04\n",
      "Epoch 416/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 250.0090 - root_mean_squared_error: 15.8117\n",
      "Epoch 416: val_loss did not improve from 38.92665\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.6138 - root_mean_squared_error: 15.7039 - val_loss: 39.7799 - val_root_mean_squared_error: 6.3071 - lr: 6.3025e-04\n",
      "Epoch 417/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 157.7533 - root_mean_squared_error: 12.5600\n",
      "Epoch 417: val_loss did not improve from 38.92665\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.7157 - root_mean_squared_error: 15.7390 - val_loss: 39.3585 - val_root_mean_squared_error: 6.2736 - lr: 6.3025e-04\n",
      "Epoch 418/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 59.1336 - root_mean_squared_error: 7.6898\n",
      "Epoch 418: val_loss did not improve from 38.92665\n",
      "\n",
      "Epoch 418: ReduceLROnPlateau reducing learning rate to 0.0005987368611386045.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.6512 - root_mean_squared_error: 15.7051 - val_loss: 41.7620 - val_root_mean_squared_error: 6.4624 - lr: 6.3025e-04\n",
      "Epoch 419/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 73.7058 - root_mean_squared_error: 8.5852\n",
      "Epoch 419: val_loss did not improve from 38.92665\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.8593 - root_mean_squared_error: 15.7118 - val_loss: 40.9741 - val_root_mean_squared_error: 6.4011 - lr: 5.9874e-04\n",
      "Epoch 420/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 65.2387 - root_mean_squared_error: 8.0771\n",
      "Epoch 420: val_loss did not improve from 38.92665\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.3694 - root_mean_squared_error: 15.6962 - val_loss: 42.5798 - val_root_mean_squared_error: 6.5253 - lr: 5.9874e-04\n",
      "Epoch 421/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 195.4881 - root_mean_squared_error: 13.9817\n",
      "Epoch 421: val_loss improved from 38.92665 to 38.54783, saving model to Model\\421-247.7225-38.5478.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.7225 - root_mean_squared_error: 15.7392 - val_loss: 38.5478 - val_root_mean_squared_error: 6.2087 - lr: 5.9874e-04\n",
      "Epoch 422/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 80.7128 - root_mean_squared_error: 8.9840\n",
      "Epoch 422: val_loss did not improve from 38.54783\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 247.0412 - root_mean_squared_error: 15.7175 - val_loss: 40.7150 - val_root_mean_squared_error: 6.3808 - lr: 5.9874e-04\n",
      "Epoch 423/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 249.9446 - root_mean_squared_error: 15.8096\n",
      "Epoch 423: val_loss did not improve from 38.54783\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.6936 - root_mean_squared_error: 15.8017 - val_loss: 39.2362 - val_root_mean_squared_error: 6.2639 - lr: 5.9874e-04\n",
      "Epoch 424/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 265.0606 - root_mean_squared_error: 16.2807\n",
      "Epoch 424: val_loss did not improve from 38.54783\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.4004 - root_mean_squared_error: 15.6971 - val_loss: 40.1437 - val_root_mean_squared_error: 6.3359 - lr: 5.9874e-04\n",
      "Epoch 425/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 31.5605 - root_mean_squared_error: 5.6179\n",
      "Epoch 425: val_loss did not improve from 38.54783\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.2686 - root_mean_squared_error: 15.6929 - val_loss: 39.7307 - val_root_mean_squared_error: 6.3032 - lr: 5.9874e-04\n",
      "Epoch 426/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 491.5220 - root_mean_squared_error: 22.1703\n",
      "Epoch 426: val_loss improved from 38.54783 to 37.44373, saving model to Model\\426-245.4408-37.4437.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.4408 - root_mean_squared_error: 15.6665 - val_loss: 37.4437 - val_root_mean_squared_error: 6.1191 - lr: 5.9874e-04\n",
      "Epoch 427/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 270.9352 - root_mean_squared_error: 16.4601\n",
      "Epoch 427: val_loss did not improve from 37.44373\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.0204 - root_mean_squared_error: 15.6850 - val_loss: 39.5495 - val_root_mean_squared_error: 6.2888 - lr: 5.9874e-04\n",
      "Epoch 428/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 64.9544 - root_mean_squared_error: 8.0594\n",
      "Epoch 428: val_loss did not improve from 37.44373\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.7910 - root_mean_squared_error: 15.6777 - val_loss: 38.0094 - val_root_mean_squared_error: 6.1652 - lr: 5.9874e-04\n",
      "Epoch 429/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 42.8586 - root_mean_squared_error: 6.5466\n",
      "Epoch 429: val_loss did not improve from 37.44373\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.3733 - root_mean_squared_error: 15.6963 - val_loss: 42.9601 - val_root_mean_squared_error: 6.5544 - lr: 5.9874e-04\n",
      "Epoch 430/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 294.2395 - root_mean_squared_error: 17.1534\n",
      "Epoch 430: val_loss did not improve from 37.44373\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.3812 - root_mean_squared_error: 15.6965 - val_loss: 38.7829 - val_root_mean_squared_error: 6.2276 - lr: 5.9874e-04\n",
      "Epoch 431/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 242.1595 - root_mean_squared_error: 15.5615\n",
      "Epoch 431: val_loss did not improve from 37.44373\n",
      "\n",
      "Epoch 431: ReduceLROnPlateau reducing learning rate to 0.0005688000208465382.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.3746 - root_mean_squared_error: 15.6644 - val_loss: 38.5358 - val_root_mean_squared_error: 6.2077 - lr: 5.9874e-04\n",
      "Epoch 432/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 51.5316 - root_mean_squared_error: 7.1786\n",
      "Epoch 432: val_loss did not improve from 37.44373\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.5941 - root_mean_squared_error: 15.7033 - val_loss: 38.3842 - val_root_mean_squared_error: 6.1955 - lr: 5.6880e-04\n",
      "Epoch 433/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 60.9909 - root_mean_squared_error: 7.8097\n",
      "Epoch 433: val_loss improved from 37.44373 to 36.46307, saving model to Model\\433-246.3284-36.4631.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.3284 - root_mean_squared_error: 15.6949 - val_loss: 36.4631 - val_root_mean_squared_error: 6.0385 - lr: 5.6880e-04\n",
      "Epoch 434/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 172.5753 - root_mean_squared_error: 13.1368\n",
      "Epoch 434: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6844 - root_mean_squared_error: 15.6424 - val_loss: 41.4193 - val_root_mean_squared_error: 6.4358 - lr: 5.6880e-04\n",
      "Epoch 435/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 39.1226 - root_mean_squared_error: 6.2548\n",
      "Epoch 435: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.2800 - root_mean_squared_error: 15.6614 - val_loss: 37.8164 - val_root_mean_squared_error: 6.1495 - lr: 5.6880e-04\n",
      "Epoch 436/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 304.9560 - root_mean_squared_error: 17.4630\n",
      "Epoch 436: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.0451 - root_mean_squared_error: 15.6539 - val_loss: 37.8708 - val_root_mean_squared_error: 6.1539 - lr: 5.6880e-04\n",
      "Epoch 437/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 191.0000 - root_mean_squared_error: 13.8203\n",
      "Epoch 437: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.5793 - root_mean_squared_error: 15.6390 - val_loss: 37.5445 - val_root_mean_squared_error: 6.1274 - lr: 5.6880e-04\n",
      "Epoch 438/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 60.9372 - root_mean_squared_error: 7.8062\n",
      "Epoch 438: val_loss did not improve from 36.46307\n",
      "\n",
      "Epoch 438: ReduceLROnPlateau reducing learning rate to 0.0005403600225690752.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.7468 - root_mean_squared_error: 15.6444 - val_loss: 38.4995 - val_root_mean_squared_error: 6.2048 - lr: 5.6880e-04\n",
      "Epoch 439/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 454.7542 - root_mean_squared_error: 21.3250\n",
      "Epoch 439: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.8278 - root_mean_squared_error: 15.6470 - val_loss: 38.8829 - val_root_mean_squared_error: 6.2356 - lr: 5.4036e-04\n",
      "Epoch 440/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 477.6378 - root_mean_squared_error: 21.8549\n",
      "Epoch 440: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.6710 - root_mean_squared_error: 15.6739 - val_loss: 38.3249 - val_root_mean_squared_error: 6.1907 - lr: 5.4036e-04\n",
      "Epoch 441/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.6447 - root_mean_squared_error: 6.8297\n",
      "Epoch 441: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.2191 - root_mean_squared_error: 15.6914 - val_loss: 39.0581 - val_root_mean_squared_error: 6.2496 - lr: 5.4036e-04\n",
      "Epoch 442/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 161.3061 - root_mean_squared_error: 12.7006\n",
      "Epoch 442: val_loss did not improve from 36.46307\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.1857 - root_mean_squared_error: 15.6903 - val_loss: 39.6339 - val_root_mean_squared_error: 6.2955 - lr: 5.4036e-04\n",
      "Epoch 443/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 209.3318 - root_mean_squared_error: 14.4683\n",
      "Epoch 443: val_loss improved from 36.46307 to 36.36802, saving model to Model\\443-244.7853-36.3680.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.7853 - root_mean_squared_error: 15.6456 - val_loss: 36.3680 - val_root_mean_squared_error: 6.0306 - lr: 5.4036e-04\n",
      "Epoch 444/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 449.4692 - root_mean_squared_error: 21.2007\n",
      "Epoch 444: val_loss did not improve from 36.36802\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.5540 - root_mean_squared_error: 15.6702 - val_loss: 40.3312 - val_root_mean_squared_error: 6.3507 - lr: 5.4036e-04\n",
      "Epoch 445/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 403.1226 - root_mean_squared_error: 20.0779\n",
      "Epoch 445: val_loss did not improve from 36.36802\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 249.7282 - root_mean_squared_error: 15.8028 - val_loss: 37.1164 - val_root_mean_squared_error: 6.0923 - lr: 5.4036e-04\n",
      "Epoch 446/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 303.4453 - root_mean_squared_error: 17.4197\n",
      "Epoch 446: val_loss did not improve from 36.36802\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 248.0465 - root_mean_squared_error: 15.7495 - val_loss: 40.1088 - val_root_mean_squared_error: 6.3331 - lr: 5.4036e-04\n",
      "Epoch 447/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 71.2175 - root_mean_squared_error: 8.4390\n",
      "Epoch 447: val_loss improved from 36.36802 to 34.86763, saving model to Model\\447-246.7458-34.8676.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 246.7458 - root_mean_squared_error: 15.7081 - val_loss: 34.8676 - val_root_mean_squared_error: 5.9049 - lr: 5.4036e-04\n",
      "Epoch 448/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 23.4919 - root_mean_squared_error: 4.8468\n",
      "Epoch 448: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.5305 - root_mean_squared_error: 15.6375 - val_loss: 39.5783 - val_root_mean_squared_error: 6.2911 - lr: 5.4036e-04\n",
      "Epoch 449/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 377.7689 - root_mean_squared_error: 19.4363\n",
      "Epoch 449: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.2500 - root_mean_squared_error: 15.5965 - val_loss: 35.5761 - val_root_mean_squared_error: 5.9646 - lr: 5.4036e-04\n",
      "Epoch 450/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 306.2693 - root_mean_squared_error: 17.5006\n",
      "Epoch 450: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.9661 - root_mean_squared_error: 15.6194 - val_loss: 38.1516 - val_root_mean_squared_error: 6.1767 - lr: 5.4036e-04\n",
      "Epoch 451/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 34.0342 - root_mean_squared_error: 5.8339\n",
      "Epoch 451: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.8554 - root_mean_squared_error: 15.6159 - val_loss: 36.2709 - val_root_mean_squared_error: 6.0225 - lr: 5.4036e-04\n",
      "Epoch 452/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 110.3173 - root_mean_squared_error: 10.5032\n",
      "Epoch 452: val_loss did not improve from 34.86763\n",
      "\n",
      "Epoch 452: ReduceLROnPlateau reducing learning rate to 0.0005133419937919825.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.0963 - root_mean_squared_error: 15.6556 - val_loss: 37.3580 - val_root_mean_squared_error: 6.1121 - lr: 5.4036e-04\n",
      "Epoch 453/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 171.6780 - root_mean_squared_error: 13.1026\n",
      "Epoch 453: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.2688 - root_mean_squared_error: 15.5971 - val_loss: 36.8970 - val_root_mean_squared_error: 6.0743 - lr: 5.1334e-04\n",
      "Epoch 454/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 54.6397 - root_mean_squared_error: 7.3919\n",
      "Epoch 454: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.0265 - root_mean_squared_error: 15.6533 - val_loss: 36.8552 - val_root_mean_squared_error: 6.0708 - lr: 5.1334e-04\n",
      "Epoch 455/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 77.2389 - root_mean_squared_error: 8.7886\n",
      "Epoch 455: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.6568 - root_mean_squared_error: 15.6095 - val_loss: 38.5966 - val_root_mean_squared_error: 6.2126 - lr: 5.1334e-04\n",
      "Epoch 456/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 492.5468 - root_mean_squared_error: 22.1934\n",
      "Epoch 456: val_loss did not improve from 34.86763\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.8112 - root_mean_squared_error: 15.5824 - val_loss: 35.7325 - val_root_mean_squared_error: 5.9777 - lr: 5.1334e-04\n",
      "Epoch 457/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 681.2408 - root_mean_squared_error: 26.1006\n",
      "Epoch 457: val_loss did not improve from 34.86763\n",
      "\n",
      "Epoch 457: ReduceLROnPlateau reducing learning rate to 0.0004876748775132.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.0775 - root_mean_squared_error: 15.6230 - val_loss: 35.4215 - val_root_mean_squared_error: 5.9516 - lr: 5.1334e-04\n",
      "Epoch 458/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 62.2279 - root_mean_squared_error: 7.8885\n",
      "Epoch 458: val_loss improved from 34.86763 to 34.58208, saving model to Model\\458-244.3897-34.5821.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.3897 - root_mean_squared_error: 15.6330 - val_loss: 34.5821 - val_root_mean_squared_error: 5.8807 - lr: 4.8767e-04\n",
      "Epoch 459/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 295.5640 - root_mean_squared_error: 17.1920\n",
      "Epoch 459: val_loss did not improve from 34.58208\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.3258 - root_mean_squared_error: 15.6629 - val_loss: 39.8741 - val_root_mean_squared_error: 6.3146 - lr: 4.8767e-04\n",
      "Epoch 460/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 163.0515 - root_mean_squared_error: 12.7692\n",
      "Epoch 460: val_loss improved from 34.58208 to 34.27302, saving model to Model\\460-243.4547-34.2730.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.4547 - root_mean_squared_error: 15.6030 - val_loss: 34.2730 - val_root_mean_squared_error: 5.8543 - lr: 4.8767e-04\n",
      "Epoch 461/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 293.4667 - root_mean_squared_error: 17.1309\n",
      "Epoch 461: val_loss did not improve from 34.27302\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.5010 - root_mean_squared_error: 15.5724 - val_loss: 37.7802 - val_root_mean_squared_error: 6.1466 - lr: 4.8767e-04\n",
      "Epoch 462/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 30.6976 - root_mean_squared_error: 5.5405\n",
      "Epoch 462: val_loss did not improve from 34.27302\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.1712 - root_mean_squared_error: 15.6260 - val_loss: 36.1224 - val_root_mean_squared_error: 6.0102 - lr: 4.8767e-04\n",
      "Epoch 463/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 31.5205 - root_mean_squared_error: 5.6143\n",
      "Epoch 463: val_loss did not improve from 34.27302\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.6959 - root_mean_squared_error: 15.6108 - val_loss: 38.5076 - val_root_mean_squared_error: 6.2055 - lr: 4.8767e-04\n",
      "Epoch 464/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 271.0127 - root_mean_squared_error: 16.4625\n",
      "Epoch 464: val_loss did not improve from 34.27302\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.3473 - root_mean_squared_error: 15.6316 - val_loss: 34.7580 - val_root_mean_squared_error: 5.8956 - lr: 4.8767e-04\n",
      "Epoch 465/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 373.8747 - root_mean_squared_error: 19.3358\n",
      "Epoch 465: val_loss did not improve from 34.27302\n",
      "\n",
      "Epoch 465: ReduceLROnPlateau reducing learning rate to 0.00046329112810781223.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.7286 - root_mean_squared_error: 15.5798 - val_loss: 37.0537 - val_root_mean_squared_error: 6.0872 - lr: 4.8767e-04\n",
      "Epoch 466/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 457.9881 - root_mean_squared_error: 21.4007\n",
      "Epoch 466: val_loss did not improve from 34.27302\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.1971 - root_mean_squared_error: 15.5627 - val_loss: 35.8745 - val_root_mean_squared_error: 5.9895 - lr: 4.6329e-04\n",
      "Epoch 467/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 485.7694 - root_mean_squared_error: 22.0402\n",
      "Epoch 467: val_loss improved from 34.27302 to 33.91138, saving model to Model\\467-242.8826-33.9114.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.8826 - root_mean_squared_error: 15.5847 - val_loss: 33.9114 - val_root_mean_squared_error: 5.8233 - lr: 4.6329e-04\n",
      "Epoch 468/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 310.0699 - root_mean_squared_error: 17.6088\n",
      "Epoch 468: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.3426 - root_mean_squared_error: 15.5994 - val_loss: 37.9501 - val_root_mean_squared_error: 6.1604 - lr: 4.6329e-04\n",
      "Epoch 469/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 173.2161 - root_mean_squared_error: 13.1612\n",
      "Epoch 469: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.3193 - root_mean_squared_error: 15.5666 - val_loss: 34.2043 - val_root_mean_squared_error: 5.8484 - lr: 4.6329e-04\n",
      "Epoch 470/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 72.1755 - root_mean_squared_error: 8.4956\n",
      "Epoch 470: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 244.6200 - root_mean_squared_error: 15.6403 - val_loss: 36.7251 - val_root_mean_squared_error: 6.0601 - lr: 4.6329e-04\n",
      "Epoch 471/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 287.3289 - root_mean_squared_error: 16.9508\n",
      "Epoch 471: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 245.3936 - root_mean_squared_error: 15.6650 - val_loss: 37.0065 - val_root_mean_squared_error: 6.0833 - lr: 4.6329e-04\n",
      "Epoch 472/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 261.1603 - root_mean_squared_error: 16.1605\n",
      "Epoch 472: val_loss did not improve from 33.91138\n",
      "\n",
      "Epoch 472: ReduceLROnPlateau reducing learning rate to 0.00044012657308485355.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.2873 - root_mean_squared_error: 15.5656 - val_loss: 34.8798 - val_root_mean_squared_error: 5.9059 - lr: 4.6329e-04\n",
      "Epoch 473/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 109.9232 - root_mean_squared_error: 10.4844\n",
      "Epoch 473: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.0488 - root_mean_squared_error: 15.5900 - val_loss: 34.3501 - val_root_mean_squared_error: 5.8609 - lr: 4.4013e-04\n",
      "Epoch 474/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 37.8649 - root_mean_squared_error: 6.1534\n",
      "Epoch 474: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.6499 - root_mean_squared_error: 15.6093 - val_loss: 38.8304 - val_root_mean_squared_error: 6.2314 - lr: 4.4013e-04\n",
      "Epoch 475/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 76.5917 - root_mean_squared_error: 8.7517\n",
      "Epoch 475: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.7750 - root_mean_squared_error: 15.5491 - val_loss: 35.6886 - val_root_mean_squared_error: 5.9740 - lr: 4.4013e-04\n",
      "Epoch 476/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 256.4610 - root_mean_squared_error: 16.0144\n",
      "Epoch 476: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.8587 - root_mean_squared_error: 15.5518 - val_loss: 34.7484 - val_root_mean_squared_error: 5.8948 - lr: 4.4013e-04\n",
      "Epoch 477/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 99.7694 - root_mean_squared_error: 9.9885\n",
      "Epoch 477: val_loss did not improve from 33.91138\n",
      "\n",
      "Epoch 477: ReduceLROnPlateau reducing learning rate to 0.00041812024719547477.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.9502 - root_mean_squared_error: 15.5547 - val_loss: 35.8924 - val_root_mean_squared_error: 5.9910 - lr: 4.4013e-04\n",
      "Epoch 478/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 484.0371 - root_mean_squared_error: 22.0008\n",
      "Epoch 478: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.9373 - root_mean_squared_error: 15.5543 - val_loss: 35.6147 - val_root_mean_squared_error: 5.9678 - lr: 4.1812e-04\n",
      "Epoch 479/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 232.3160 - root_mean_squared_error: 15.2419\n",
      "Epoch 479: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.9192 - root_mean_squared_error: 15.5538 - val_loss: 33.9289 - val_root_mean_squared_error: 5.8248 - lr: 4.1812e-04\n",
      "Epoch 480/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 239.6180 - root_mean_squared_error: 15.4796\n",
      "Epoch 480: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 243.5915 - root_mean_squared_error: 15.6074 - val_loss: 37.2569 - val_root_mean_squared_error: 6.1038 - lr: 4.1812e-04\n",
      "Epoch 481/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 283.4366 - root_mean_squared_error: 16.8356\n",
      "Epoch 481: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.6254 - root_mean_squared_error: 15.5764 - val_loss: 34.6551 - val_root_mean_squared_error: 5.8869 - lr: 4.1812e-04\n",
      "Epoch 482/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 528.8906 - root_mean_squared_error: 22.9976\n",
      "Epoch 482: val_loss did not improve from 33.91138\n",
      "\n",
      "Epoch 482: ReduceLROnPlateau reducing learning rate to 0.00039721422654110934.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.8304 - root_mean_squared_error: 15.5509 - val_loss: 35.3945 - val_root_mean_squared_error: 5.9493 - lr: 4.1812e-04\n",
      "Epoch 483/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 253.4824 - root_mean_squared_error: 15.9211\n",
      "Epoch 483: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.2061 - root_mean_squared_error: 15.5308 - val_loss: 35.5736 - val_root_mean_squared_error: 5.9644 - lr: 3.9721e-04\n",
      "Epoch 484/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 106.4749 - root_mean_squared_error: 10.3187\n",
      "Epoch 484: val_loss did not improve from 33.91138\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.3893 - root_mean_squared_error: 15.5367 - val_loss: 35.9397 - val_root_mean_squared_error: 5.9950 - lr: 3.9721e-04\n",
      "Epoch 485/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 506.3064 - root_mean_squared_error: 22.5013\n",
      "Epoch 485: val_loss improved from 33.91138 to 33.43633, saving model to Model\\485-242.0401-33.4363.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 242.0401 - root_mean_squared_error: 15.5576 - val_loss: 33.4363 - val_root_mean_squared_error: 5.7824 - lr: 3.9721e-04\n",
      "Epoch 486/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 441.4476 - root_mean_squared_error: 21.0107\n",
      "Epoch 486: val_loss did not improve from 33.43633\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 242.5998 - root_mean_squared_error: 15.5756 - val_loss: 38.3499 - val_root_mean_squared_error: 6.1927 - lr: 3.9721e-04\n",
      "Epoch 487/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 285.8634 - root_mean_squared_error: 16.9075\n",
      "Epoch 487: val_loss did not improve from 33.43633\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.5948 - root_mean_squared_error: 15.5433 - val_loss: 33.7443 - val_root_mean_squared_error: 5.8090 - lr: 3.9721e-04\n",
      "Epoch 488/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 51.1581 - root_mean_squared_error: 7.1525\n",
      "Epoch 488: val_loss did not improve from 33.43633\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.0376 - root_mean_squared_error: 15.5254 - val_loss: 36.3730 - val_root_mean_squared_error: 6.0310 - lr: 3.9721e-04\n",
      "Epoch 489/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 138.7947 - root_mean_squared_error: 11.7811\n",
      "Epoch 489: val_loss did not improve from 33.43633\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.9168 - root_mean_squared_error: 15.5215 - val_loss: 36.5493 - val_root_mean_squared_error: 6.0456 - lr: 3.9721e-04\n",
      "Epoch 490/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 249.6189 - root_mean_squared_error: 15.7993\n",
      "Epoch 490: val_loss did not improve from 33.43633\n",
      "\n",
      "Epoch 490: ReduceLROnPlateau reducing learning rate to 0.00037735351797891776.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.6405 - root_mean_squared_error: 15.5448 - val_loss: 33.9283 - val_root_mean_squared_error: 5.8248 - lr: 3.9721e-04\n",
      "Epoch 491/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 368.4967 - root_mean_squared_error: 19.1963\n",
      "Epoch 491: val_loss did not improve from 33.43633\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.8790 - root_mean_squared_error: 15.5525 - val_loss: 37.8506 - val_root_mean_squared_error: 6.1523 - lr: 3.7735e-04\n",
      "Epoch 492/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 37.7040 - root_mean_squared_error: 6.1404\n",
      "Epoch 492: val_loss did not improve from 33.43633\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.8539 - root_mean_squared_error: 15.5195 - val_loss: 35.1584 - val_root_mean_squared_error: 5.9295 - lr: 3.7735e-04\n",
      "Epoch 493/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 472.0437 - root_mean_squared_error: 21.7266\n",
      "Epoch 493: val_loss improved from 33.43633 to 33.03540, saving model to Model\\493-241.0275-33.0354.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.0275 - root_mean_squared_error: 15.5251 - val_loss: 33.0354 - val_root_mean_squared_error: 5.7476 - lr: 3.7735e-04\n",
      "Epoch 494/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 239.7402 - root_mean_squared_error: 15.4835\n",
      "Epoch 494: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.5595 - root_mean_squared_error: 15.5100 - val_loss: 35.8747 - val_root_mean_squared_error: 5.9895 - lr: 3.7735e-04\n",
      "Epoch 495/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 276.2497 - root_mean_squared_error: 16.6208\n",
      "Epoch 495: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.7218 - root_mean_squared_error: 15.5152 - val_loss: 36.3274 - val_root_mean_squared_error: 6.0272 - lr: 3.7735e-04\n",
      "Epoch 496/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.8398 - root_mean_squared_error: 6.8440\n",
      "Epoch 496: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 241.6618 - root_mean_squared_error: 15.5455 - val_loss: 34.7942 - val_root_mean_squared_error: 5.8987 - lr: 3.7735e-04\n",
      "Epoch 497/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 69.2871 - root_mean_squared_error: 8.3239\n",
      "Epoch 497: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.3409 - root_mean_squared_error: 15.5029 - val_loss: 36.6477 - val_root_mean_squared_error: 6.0537 - lr: 3.7735e-04\n",
      "Epoch 498/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 254.5333 - root_mean_squared_error: 15.9541\n",
      "Epoch 498: val_loss did not improve from 33.03540\n",
      "\n",
      "Epoch 498: ReduceLROnPlateau reducing learning rate to 0.00035848583793267607.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.6522 - root_mean_squared_error: 15.5130 - val_loss: 35.1579 - val_root_mean_squared_error: 5.9294 - lr: 3.7735e-04\n",
      "Epoch 499/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 91.3992 - root_mean_squared_error: 9.5603\n",
      "Epoch 499: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.2797 - root_mean_squared_error: 15.5010 - val_loss: 35.4543 - val_root_mean_squared_error: 5.9544 - lr: 3.5849e-04\n",
      "Epoch 500/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 54.4231 - root_mean_squared_error: 7.3772\n",
      "Epoch 500: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 240.5779 - root_mean_squared_error: 15.5106 - val_loss: 35.3339 - val_root_mean_squared_error: 5.9442 - lr: 3.5849e-04\n",
      "Epoch 501/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 360.4354 - root_mean_squared_error: 18.9851\n",
      "Epoch 501: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1112 - root_mean_squared_error: 15.4955 - val_loss: 34.5070 - val_root_mean_squared_error: 5.8743 - lr: 3.5849e-04\n",
      "Epoch 502/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 103.6639 - root_mean_squared_error: 10.1815\n",
      "Epoch 502: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1424 - root_mean_squared_error: 15.4965 - val_loss: 35.6402 - val_root_mean_squared_error: 5.9699 - lr: 3.5849e-04\n",
      "Epoch 503/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 27.0040 - root_mean_squared_error: 5.1965\n",
      "Epoch 503: val_loss did not improve from 33.03540\n",
      "\n",
      "Epoch 503: ReduceLROnPlateau reducing learning rate to 0.00034056155709549785.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1653 - root_mean_squared_error: 15.4973 - val_loss: 34.2948 - val_root_mean_squared_error: 5.8562 - lr: 3.5849e-04\n",
      "Epoch 504/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 288.0000 - root_mean_squared_error: 16.9706\n",
      "Epoch 504: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.2965 - root_mean_squared_error: 15.5015 - val_loss: 33.2204 - val_root_mean_squared_error: 5.7637 - lr: 3.4056e-04\n",
      "Epoch 505/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 169.4236 - root_mean_squared_error: 13.0163\n",
      "Epoch 505: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9443 - root_mean_squared_error: 15.4901 - val_loss: 36.1166 - val_root_mean_squared_error: 6.0097 - lr: 3.4056e-04\n",
      "Epoch 506/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.4968 - root_mean_squared_error: 6.8189\n",
      "Epoch 506: val_loss did not improve from 33.03540\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7955 - root_mean_squared_error: 15.4853 - val_loss: 34.8222 - val_root_mean_squared_error: 5.9010 - lr: 3.4056e-04\n",
      "Epoch 507/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 266.3937 - root_mean_squared_error: 16.3216\n",
      "Epoch 507: val_loss improved from 33.03540 to 32.90125, saving model to Model\\507-240.4590-32.9012.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 240.4590 - root_mean_squared_error: 15.5067 - val_loss: 32.9012 - val_root_mean_squared_error: 5.7360 - lr: 3.4056e-04\n",
      "Epoch 508/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 47.4073 - root_mean_squared_error: 6.8853\n",
      "Epoch 508: val_loss did not improve from 32.90125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1866 - root_mean_squared_error: 15.4980 - val_loss: 36.3823 - val_root_mean_squared_error: 6.0318 - lr: 3.4056e-04\n",
      "Epoch 509/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 55.1612 - root_mean_squared_error: 7.4271\n",
      "Epoch 509: val_loss did not improve from 32.90125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.8544 - root_mean_squared_error: 15.4872 - val_loss: 34.4563 - val_root_mean_squared_error: 5.8700 - lr: 3.4056e-04\n",
      "Epoch 510/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 28.3577 - root_mean_squared_error: 5.3252\n",
      "Epoch 510: val_loss did not improve from 32.90125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.8854 - root_mean_squared_error: 15.4882 - val_loss: 34.0810 - val_root_mean_squared_error: 5.8379 - lr: 3.4056e-04\n",
      "Epoch 511/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 314.6377 - root_mean_squared_error: 17.7380\n",
      "Epoch 511: val_loss did not improve from 32.90125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9096 - root_mean_squared_error: 15.4890 - val_loss: 33.1909 - val_root_mean_squared_error: 5.7612 - lr: 3.4056e-04\n",
      "Epoch 512/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 264.8513 - root_mean_squared_error: 16.2743\n",
      "Epoch 512: val_loss did not improve from 32.90125\n",
      "\n",
      "Epoch 512: ReduceLROnPlateau reducing learning rate to 0.00032353347924072293.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7555 - root_mean_squared_error: 15.4840 - val_loss: 34.2796 - val_root_mean_squared_error: 5.8549 - lr: 3.4056e-04\n",
      "Epoch 513/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 262.6206 - root_mean_squared_error: 16.2056\n",
      "Epoch 513: val_loss did not improve from 32.90125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7928 - root_mean_squared_error: 15.4852 - val_loss: 33.8684 - val_root_mean_squared_error: 5.8197 - lr: 3.2353e-04\n",
      "Epoch 514/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 743.6002 - root_mean_squared_error: 27.2690\n",
      "Epoch 514: val_loss did not improve from 32.90125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.6327 - root_mean_squared_error: 15.4801 - val_loss: 33.5211 - val_root_mean_squared_error: 5.7897 - lr: 3.2353e-04\n",
      "Epoch 515/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 25.5979 - root_mean_squared_error: 5.0594\n",
      "Epoch 515: val_loss did not improve from 32.90125\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1006 - root_mean_squared_error: 15.4952 - val_loss: 36.8059 - val_root_mean_squared_error: 6.0668 - lr: 3.2353e-04\n",
      "Epoch 516/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 288.5235 - root_mean_squared_error: 16.9860\n",
      "Epoch 516: val_loss improved from 32.90125 to 32.23622, saving model to Model\\516-240.3056-32.2362.hdf5\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 240.3056 - root_mean_squared_error: 15.5018 - val_loss: 32.2362 - val_root_mean_squared_error: 5.6777 - lr: 3.2353e-04\n",
      "Epoch 517/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 315.2227 - root_mean_squared_error: 17.7545\n",
      "Epoch 517: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.5213 - root_mean_squared_error: 15.4765 - val_loss: 35.8314 - val_root_mean_squared_error: 5.9859 - lr: 3.2353e-04\n",
      "Epoch 518/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 84.6515 - root_mean_squared_error: 9.2006\n",
      "Epoch 518: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.4095 - root_mean_squared_error: 15.4729 - val_loss: 33.9572 - val_root_mean_squared_error: 5.8273 - lr: 3.2353e-04\n",
      "Epoch 519/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 69.4636 - root_mean_squared_error: 8.3345\n",
      "Epoch 519: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.6472 - root_mean_squared_error: 15.4805 - val_loss: 35.8701 - val_root_mean_squared_error: 5.9892 - lr: 3.2353e-04\n",
      "Epoch 520/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 306.5131 - root_mean_squared_error: 17.5075\n",
      "Epoch 520: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.9104 - root_mean_squared_error: 15.4890 - val_loss: 32.4456 - val_root_mean_squared_error: 5.6961 - lr: 3.2353e-04\n",
      "Epoch 521/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 265.1741 - root_mean_squared_error: 16.2842\n",
      "Epoch 521: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 521: ReduceLROnPlateau reducing learning rate to 0.00030735681357327847.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7427 - root_mean_squared_error: 15.4836 - val_loss: 36.6924 - val_root_mean_squared_error: 6.0574 - lr: 3.2353e-04\n",
      "Epoch 522/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 528.8184 - root_mean_squared_error: 22.9961\n",
      "Epoch 522: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.7418 - root_mean_squared_error: 15.4836 - val_loss: 33.6307 - val_root_mean_squared_error: 5.7992 - lr: 3.0736e-04\n",
      "Epoch 523/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 62.1464 - root_mean_squared_error: 7.8833\n",
      "Epoch 523: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.4483 - root_mean_squared_error: 15.5064 - val_loss: 32.6925 - val_root_mean_squared_error: 5.7177 - lr: 3.0736e-04\n",
      "Epoch 524/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 298.2224 - root_mean_squared_error: 17.2691\n",
      "Epoch 524: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.2057 - root_mean_squared_error: 15.4986 - val_loss: 36.7072 - val_root_mean_squared_error: 6.0586 - lr: 3.0736e-04\n",
      "Epoch 525/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 247.1022 - root_mean_squared_error: 15.7195\n",
      "Epoch 525: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 240.1286 - root_mean_squared_error: 15.4961 - val_loss: 32.9474 - val_root_mean_squared_error: 5.7400 - lr: 3.0736e-04\n",
      "Epoch 526/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 49.7842 - root_mean_squared_error: 7.0558\n",
      "Epoch 526: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 526: ReduceLROnPlateau reducing learning rate to 0.00029198898118920624.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.8925 - root_mean_squared_error: 15.4561 - val_loss: 34.3586 - val_root_mean_squared_error: 5.8616 - lr: 3.0736e-04\n",
      "Epoch 527/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 49.7543 - root_mean_squared_error: 7.0537\n",
      "Epoch 527: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.3158 - root_mean_squared_error: 15.4698 - val_loss: 34.9849 - val_root_mean_squared_error: 5.9148 - lr: 2.9199e-04\n",
      "Epoch 528/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 163.6028 - root_mean_squared_error: 12.7907\n",
      "Epoch 528: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.2331 - root_mean_squared_error: 15.4672 - val_loss: 35.5375 - val_root_mean_squared_error: 5.9613 - lr: 2.9199e-04\n",
      "Epoch 529/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 183.0177 - root_mean_squared_error: 13.5284\n",
      "Epoch 529: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.1434 - root_mean_squared_error: 15.4643 - val_loss: 35.6922 - val_root_mean_squared_error: 5.9743 - lr: 2.9199e-04\n",
      "Epoch 530/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 199.3976 - root_mean_squared_error: 14.1208\n",
      "Epoch 530: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.3813 - root_mean_squared_error: 15.4720 - val_loss: 32.3583 - val_root_mean_squared_error: 5.6884 - lr: 2.9199e-04\n",
      "Epoch 531/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 292.5670 - root_mean_squared_error: 17.1046\n",
      "Epoch 531: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 531: ReduceLROnPlateau reducing learning rate to 0.00027738953212974593.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.6966 - root_mean_squared_error: 15.4498 - val_loss: 34.9313 - val_root_mean_squared_error: 5.9103 - lr: 2.9199e-04\n",
      "Epoch 532/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 602.5531 - root_mean_squared_error: 24.5470\n",
      "Epoch 532: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.9539 - root_mean_squared_error: 15.4581 - val_loss: 34.2839 - val_root_mean_squared_error: 5.8552 - lr: 2.7739e-04\n",
      "Epoch 533/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.0398 - root_mean_squared_error: 6.7853\n",
      "Epoch 533: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.5921 - root_mean_squared_error: 15.4464 - val_loss: 34.9250 - val_root_mean_squared_error: 5.9097 - lr: 2.7739e-04\n",
      "Epoch 534/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 279.5641 - root_mean_squared_error: 16.7202\n",
      "Epoch 534: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.6217 - root_mean_squared_error: 15.4474 - val_loss: 33.8955 - val_root_mean_squared_error: 5.8220 - lr: 2.7739e-04\n",
      "Epoch 535/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 252.5130 - root_mean_squared_error: 15.8907\n",
      "Epoch 535: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.9497 - root_mean_squared_error: 15.4580 - val_loss: 34.8206 - val_root_mean_squared_error: 5.9009 - lr: 2.7739e-04\n",
      "Epoch 536/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 496.2878 - root_mean_squared_error: 22.2775\n",
      "Epoch 536: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 536: ReduceLROnPlateau reducing learning rate to 0.0002635200624354184.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.2575 - root_mean_squared_error: 15.4356 - val_loss: 32.3756 - val_root_mean_squared_error: 5.6900 - lr: 2.7739e-04\n",
      "Epoch 537/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 312.6497 - root_mean_squared_error: 17.6819\n",
      "Epoch 537: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.9578 - root_mean_squared_error: 15.4583 - val_loss: 32.4311 - val_root_mean_squared_error: 5.6948 - lr: 2.6352e-04\n",
      "Epoch 538/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 350.0331 - root_mean_squared_error: 18.7092\n",
      "Epoch 538: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 239.0363 - root_mean_squared_error: 15.4608 - val_loss: 35.8564 - val_root_mean_squared_error: 5.9880 - lr: 2.6352e-04\n",
      "Epoch 539/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 45.3939 - root_mean_squared_error: 6.7375\n",
      "Epoch 539: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.7149 - root_mean_squared_error: 15.4504 - val_loss: 33.4693 - val_root_mean_squared_error: 5.7853 - lr: 2.6352e-04\n",
      "Epoch 540/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 161.8747 - root_mean_squared_error: 12.7230\n",
      "Epoch 540: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.8479 - root_mean_squared_error: 15.4547 - val_loss: 32.3770 - val_root_mean_squared_error: 5.6901 - lr: 2.6352e-04\n",
      "Epoch 541/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 36.6946 - root_mean_squared_error: 6.0576\n",
      "Epoch 541: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 541: ReduceLROnPlateau reducing learning rate to 0.0002503440482541919.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.1560 - root_mean_squared_error: 15.4323 - val_loss: 35.0363 - val_root_mean_squared_error: 5.9191 - lr: 2.6352e-04\n",
      "Epoch 542/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 300.7608 - root_mean_squared_error: 17.3425\n",
      "Epoch 542: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.7235 - root_mean_squared_error: 15.4507 - val_loss: 36.3049 - val_root_mean_squared_error: 6.0254 - lr: 2.5034e-04\n",
      "Epoch 543/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 89.3064 - root_mean_squared_error: 9.4502\n",
      "Epoch 543: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.4463 - root_mean_squared_error: 15.4417 - val_loss: 33.5782 - val_root_mean_squared_error: 5.7947 - lr: 2.5034e-04\n",
      "Epoch 544/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 74.3731 - root_mean_squared_error: 8.6240\n",
      "Epoch 544: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.4285 - root_mean_squared_error: 15.4411 - val_loss: 33.9585 - val_root_mean_squared_error: 5.8274 - lr: 2.5034e-04\n",
      "Epoch 545/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 82.5443 - root_mean_squared_error: 9.0854\n",
      "Epoch 545: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.2647 - root_mean_squared_error: 15.4358 - val_loss: 33.6479 - val_root_mean_squared_error: 5.8007 - lr: 2.5034e-04\n",
      "Epoch 546/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 68.6953 - root_mean_squared_error: 8.2883\n",
      "Epoch 546: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 546: ReduceLROnPlateau reducing learning rate to 0.00023782684584148226.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.4552 - root_mean_squared_error: 15.4420 - val_loss: 34.3622 - val_root_mean_squared_error: 5.8619 - lr: 2.5034e-04\n",
      "Epoch 547/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 47.9892 - root_mean_squared_error: 6.9274\n",
      "Epoch 547: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.4144 - root_mean_squared_error: 15.4407 - val_loss: 33.5226 - val_root_mean_squared_error: 5.7899 - lr: 2.3783e-04\n",
      "Epoch 548/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 110.1737 - root_mean_squared_error: 10.4964\n",
      "Epoch 548: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3697 - root_mean_squared_error: 15.4392 - val_loss: 32.8555 - val_root_mean_squared_error: 5.7320 - lr: 2.3783e-04\n",
      "Epoch 549/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 193.3775 - root_mean_squared_error: 13.9060\n",
      "Epoch 549: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3215 - root_mean_squared_error: 15.4377 - val_loss: 35.9512 - val_root_mean_squared_error: 5.9959 - lr: 2.3783e-04\n",
      "Epoch 550/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 104.9310 - root_mean_squared_error: 10.2436\n",
      "Epoch 550: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.7805 - root_mean_squared_error: 15.4525 - val_loss: 32.5570 - val_root_mean_squared_error: 5.7059 - lr: 2.3783e-04\n",
      "Epoch 551/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 140.3265 - root_mean_squared_error: 11.8459\n",
      "Epoch 551: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 551: ReduceLROnPlateau reducing learning rate to 0.00022593549801968037.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3463 - root_mean_squared_error: 15.4385 - val_loss: 35.1594 - val_root_mean_squared_error: 5.9295 - lr: 2.3783e-04\n",
      "Epoch 552/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 280.3318 - root_mean_squared_error: 16.7431\n",
      "Epoch 552: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.2142 - root_mean_squared_error: 15.4342 - val_loss: 33.1991 - val_root_mean_squared_error: 5.7619 - lr: 2.2594e-04\n",
      "Epoch 553/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 295.4398 - root_mean_squared_error: 17.1884\n",
      "Epoch 553: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3521 - root_mean_squared_error: 15.4387 - val_loss: 33.7388 - val_root_mean_squared_error: 5.8085 - lr: 2.2594e-04\n",
      "Epoch 554/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 483.8621 - root_mean_squared_error: 21.9969\n",
      "Epoch 554: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.0998 - root_mean_squared_error: 15.4305 - val_loss: 34.2280 - val_root_mean_squared_error: 5.8505 - lr: 2.2594e-04\n",
      "Epoch 555/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 47.9151 - root_mean_squared_error: 6.9221\n",
      "Epoch 555: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.2737 - root_mean_squared_error: 15.4361 - val_loss: 32.2793 - val_root_mean_squared_error: 5.6815 - lr: 2.2594e-04\n",
      "Epoch 556/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 65.1361 - root_mean_squared_error: 8.0707\n",
      "Epoch 556: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 556: ReduceLROnPlateau reducing learning rate to 0.00021463872035383245.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.9692 - root_mean_squared_error: 15.4263 - val_loss: 33.0517 - val_root_mean_squared_error: 5.7491 - lr: 2.2594e-04\n",
      "Epoch 557/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 63.3718 - root_mean_squared_error: 7.9606\n",
      "Epoch 557: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.8425 - root_mean_squared_error: 15.4221 - val_loss: 34.9369 - val_root_mean_squared_error: 5.9107 - lr: 2.1464e-04\n",
      "Epoch 558/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 28.1746 - root_mean_squared_error: 5.3080\n",
      "Epoch 558: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.0555 - root_mean_squared_error: 15.4290 - val_loss: 34.6020 - val_root_mean_squared_error: 5.8823 - lr: 2.1464e-04\n",
      "Epoch 559/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 484.6100 - root_mean_squared_error: 22.0139\n",
      "Epoch 559: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.9433 - root_mean_squared_error: 15.4254 - val_loss: 32.4281 - val_root_mean_squared_error: 5.6946 - lr: 2.1464e-04\n",
      "Epoch 560/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 65.4864 - root_mean_squared_error: 8.0924\n",
      "Epoch 560: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.9727 - root_mean_squared_error: 15.4264 - val_loss: 32.4963 - val_root_mean_squared_error: 5.7006 - lr: 2.1464e-04\n",
      "Epoch 561/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 313.5489 - root_mean_squared_error: 17.7073\n",
      "Epoch 561: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 561: ReduceLROnPlateau reducing learning rate to 0.0002039067905570846.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7360 - root_mean_squared_error: 15.4187 - val_loss: 35.2981 - val_root_mean_squared_error: 5.9412 - lr: 2.1464e-04\n",
      "Epoch 562/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 36.5202 - root_mean_squared_error: 6.0432\n",
      "Epoch 562: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.8451 - root_mean_squared_error: 15.4222 - val_loss: 34.0479 - val_root_mean_squared_error: 5.8351 - lr: 2.0391e-04\n",
      "Epoch 563/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 632.5200 - root_mean_squared_error: 25.1500\n",
      "Epoch 563: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7689 - root_mean_squared_error: 15.4198 - val_loss: 32.2429 - val_root_mean_squared_error: 5.6783 - lr: 2.0391e-04\n",
      "Epoch 564/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 36.1524 - root_mean_squared_error: 6.0127\n",
      "Epoch 564: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3371 - root_mean_squared_error: 15.4382 - val_loss: 34.3926 - val_root_mean_squared_error: 5.8645 - lr: 2.0391e-04\n",
      "Epoch 565/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 96.0988 - root_mean_squared_error: 9.8030\n",
      "Epoch 565: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5918 - root_mean_squared_error: 15.4140 - val_loss: 34.0347 - val_root_mean_squared_error: 5.8339 - lr: 2.0391e-04\n",
      "Epoch 566/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 246.4881 - root_mean_squared_error: 15.6999\n",
      "Epoch 566: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 566: ReduceLROnPlateau reducing learning rate to 0.00019371145172044634.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.1346 - root_mean_squared_error: 15.4316 - val_loss: 32.3070 - val_root_mean_squared_error: 5.6839 - lr: 2.0391e-04\n",
      "Epoch 567/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 25.3716 - root_mean_squared_error: 5.0370\n",
      "Epoch 567: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.3176 - root_mean_squared_error: 15.4375 - val_loss: 34.3222 - val_root_mean_squared_error: 5.8585 - lr: 1.9371e-04\n",
      "Epoch 568/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 317.6919 - root_mean_squared_error: 17.8239\n",
      "Epoch 568: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7355 - root_mean_squared_error: 15.4187 - val_loss: 32.8949 - val_root_mean_squared_error: 5.7354 - lr: 1.9371e-04\n",
      "Epoch 569/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 509.8475 - root_mean_squared_error: 22.5798\n",
      "Epoch 569: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5572 - root_mean_squared_error: 15.4129 - val_loss: 33.4236 - val_root_mean_squared_error: 5.7813 - lr: 1.9371e-04\n",
      "Epoch 570/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 178.4682 - root_mean_squared_error: 13.3592\n",
      "Epoch 570: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5738 - root_mean_squared_error: 15.4134 - val_loss: 32.8997 - val_root_mean_squared_error: 5.7358 - lr: 1.9371e-04\n",
      "Epoch 571/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 289.1537 - root_mean_squared_error: 17.0045\n",
      "Epoch 571: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 571: ReduceLROnPlateau reducing learning rate to 0.00018402588466415182.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.5298 - root_mean_squared_error: 15.4120 - val_loss: 34.2548 - val_root_mean_squared_error: 5.8528 - lr: 1.9371e-04\n",
      "Epoch 572/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 522.3564 - root_mean_squared_error: 22.8551\n",
      "Epoch 572: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.0048 - root_mean_squared_error: 15.4274 - val_loss: 32.7234 - val_root_mean_squared_error: 5.7204 - lr: 1.8403e-04\n",
      "Epoch 573/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 460.2188 - root_mean_squared_error: 21.4527\n",
      "Epoch 573: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7663 - root_mean_squared_error: 15.4197 - val_loss: 34.8481 - val_root_mean_squared_error: 5.9032 - lr: 1.8403e-04\n",
      "Epoch 574/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 188.5717 - root_mean_squared_error: 13.7321\n",
      "Epoch 574: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7374 - root_mean_squared_error: 15.4187 - val_loss: 34.0430 - val_root_mean_squared_error: 5.8346 - lr: 1.8403e-04\n",
      "Epoch 575/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 501.8640 - root_mean_squared_error: 22.4023\n",
      "Epoch 575: val_loss did not improve from 32.23622\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.3833 - root_mean_squared_error: 15.4072 - val_loss: 33.0287 - val_root_mean_squared_error: 5.7471 - lr: 1.8403e-04\n",
      "Epoch 576/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 247.2321 - root_mean_squared_error: 15.7236\n",
      "Epoch 576: val_loss did not improve from 32.23622\n",
      "\n",
      "Epoch 576: ReduceLROnPlateau reducing learning rate to 0.00017482458351878447.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.8674 - root_mean_squared_error: 15.4230 - val_loss: 32.9387 - val_root_mean_squared_error: 5.7392 - lr: 1.8403e-04\n",
      "Epoch 577/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 284.1610 - root_mean_squared_error: 16.8571\n",
      "Epoch 577: val_loss improved from 32.23622 to 31.87499, saving model to Model\\577-237.7108-31.8750.hdf5\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.7108 - root_mean_squared_error: 15.4179 - val_loss: 31.8750 - val_root_mean_squared_error: 5.6458 - lr: 1.7482e-04\n",
      "Epoch 578/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 396.5216 - root_mean_squared_error: 19.9129\n",
      "Epoch 578: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4224 - root_mean_squared_error: 15.4085 - val_loss: 33.3917 - val_root_mean_squared_error: 5.7786 - lr: 1.7482e-04\n",
      "Epoch 579/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 317.0165 - root_mean_squared_error: 17.8050\n",
      "Epoch 579: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4411 - root_mean_squared_error: 15.4091 - val_loss: 33.2514 - val_root_mean_squared_error: 5.7664 - lr: 1.7482e-04\n",
      "Epoch 580/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 222.7092 - root_mean_squared_error: 14.9234\n",
      "Epoch 580: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.3367 - root_mean_squared_error: 15.4057 - val_loss: 34.3408 - val_root_mean_squared_error: 5.8601 - lr: 1.7482e-04\n",
      "Epoch 581/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 414.9701 - root_mean_squared_error: 20.3708\n",
      "Epoch 581: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4481 - root_mean_squared_error: 15.4094 - val_loss: 32.5449 - val_root_mean_squared_error: 5.7048 - lr: 1.7482e-04\n",
      "Epoch 582/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 49.7647 - root_mean_squared_error: 7.0544\n",
      "Epoch 582: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 582: ReduceLROnPlateau reducing learning rate to 0.0001660833557252772.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 238.0201 - root_mean_squared_error: 15.4279 - val_loss: 34.5698 - val_root_mean_squared_error: 5.8796 - lr: 1.7482e-04\n",
      "Epoch 583/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 266.5744 - root_mean_squared_error: 16.3271\n",
      "Epoch 583: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.3929 - root_mean_squared_error: 15.4076 - val_loss: 33.3260 - val_root_mean_squared_error: 5.7729 - lr: 1.6608e-04\n",
      "Epoch 584/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 417.8738 - root_mean_squared_error: 20.4420\n",
      "Epoch 584: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.2234 - root_mean_squared_error: 15.4021 - val_loss: 33.4557 - val_root_mean_squared_error: 5.7841 - lr: 1.6608e-04\n",
      "Epoch 585/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 24.9466 - root_mean_squared_error: 4.9947\n",
      "Epoch 585: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.2873 - root_mean_squared_error: 15.4041 - val_loss: 33.6255 - val_root_mean_squared_error: 5.7988 - lr: 1.6608e-04\n",
      "Epoch 586/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 188.3313 - root_mean_squared_error: 13.7234\n",
      "Epoch 586: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1590 - root_mean_squared_error: 15.4000 - val_loss: 32.7249 - val_root_mean_squared_error: 5.7206 - lr: 1.6608e-04\n",
      "Epoch 587/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 896.9675 - root_mean_squared_error: 29.9494\n",
      "Epoch 587: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 587: ReduceLROnPlateau reducing learning rate to 0.0001577791837917175.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4318 - root_mean_squared_error: 15.4088 - val_loss: 33.2580 - val_root_mean_squared_error: 5.7670 - lr: 1.6608e-04\n",
      "Epoch 588/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 244.4123 - root_mean_squared_error: 15.6337\n",
      "Epoch 588: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0558 - root_mean_squared_error: 15.3966 - val_loss: 33.3017 - val_root_mean_squared_error: 5.7708 - lr: 1.5778e-04\n",
      "Epoch 589/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 64.2604 - root_mean_squared_error: 8.0163\n",
      "Epoch 589: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1146 - root_mean_squared_error: 15.3985 - val_loss: 33.4942 - val_root_mean_squared_error: 5.7874 - lr: 1.5778e-04\n",
      "Epoch 590/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 350.9357 - root_mean_squared_error: 18.7333\n",
      "Epoch 590: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.2162 - root_mean_squared_error: 15.4018 - val_loss: 33.1595 - val_root_mean_squared_error: 5.7584 - lr: 1.5778e-04\n",
      "Epoch 591/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 316.1536 - root_mean_squared_error: 17.7807\n",
      "Epoch 591: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0644 - root_mean_squared_error: 15.3969 - val_loss: 34.5052 - val_root_mean_squared_error: 5.8741 - lr: 1.5778e-04\n",
      "Epoch 592/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 41.4906 - root_mean_squared_error: 6.4413\n",
      "Epoch 592: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 592: ReduceLROnPlateau reducing learning rate to 0.0001498902252933476.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0909 - root_mean_squared_error: 15.3978 - val_loss: 33.0467 - val_root_mean_squared_error: 5.7486 - lr: 1.5778e-04\n",
      "Epoch 593/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 43.3707 - root_mean_squared_error: 6.5856\n",
      "Epoch 593: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.4308 - root_mean_squared_error: 15.4088 - val_loss: 33.7926 - val_root_mean_squared_error: 5.8131 - lr: 1.4989e-04\n",
      "Epoch 594/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 266.8340 - root_mean_squared_error: 16.3351\n",
      "Epoch 594: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9221 - root_mean_squared_error: 15.3923 - val_loss: 33.4596 - val_root_mean_squared_error: 5.7844 - lr: 1.4989e-04\n",
      "Epoch 595/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 123.2718 - root_mean_squared_error: 11.1028\n",
      "Epoch 595: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1318 - root_mean_squared_error: 15.3991 - val_loss: 33.0285 - val_root_mean_squared_error: 5.7470 - lr: 1.4989e-04\n",
      "Epoch 596/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 258.6730 - root_mean_squared_error: 16.0833\n",
      "Epoch 596: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1372 - root_mean_squared_error: 15.3993 - val_loss: 33.2143 - val_root_mean_squared_error: 5.7632 - lr: 1.4989e-04\n",
      "Epoch 597/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 225.1587 - root_mean_squared_error: 15.0053\n",
      "Epoch 597: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 597: ReduceLROnPlateau reducing learning rate to 0.00014239571610232815.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9441 - root_mean_squared_error: 15.3930 - val_loss: 33.1079 - val_root_mean_squared_error: 5.7539 - lr: 1.4989e-04\n",
      "Epoch 598/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 222.6355 - root_mean_squared_error: 14.9210\n",
      "Epoch 598: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9255 - root_mean_squared_error: 15.3924 - val_loss: 32.8784 - val_root_mean_squared_error: 5.7340 - lr: 1.4240e-04\n",
      "Epoch 599/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 269.9569 - root_mean_squared_error: 16.4304\n",
      "Epoch 599: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8343 - root_mean_squared_error: 15.3894 - val_loss: 32.8741 - val_root_mean_squared_error: 5.7336 - lr: 1.4240e-04\n",
      "Epoch 600/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 509.4698 - root_mean_squared_error: 22.5714\n",
      "Epoch 600: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1120 - root_mean_squared_error: 15.3984 - val_loss: 33.6382 - val_root_mean_squared_error: 5.7998 - lr: 1.4240e-04\n",
      "Epoch 601/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 61.0342 - root_mean_squared_error: 7.8124\n",
      "Epoch 601: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.1727 - root_mean_squared_error: 15.4004 - val_loss: 32.9469 - val_root_mean_squared_error: 5.7399 - lr: 1.4240e-04\n",
      "Epoch 602/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 200.6784 - root_mean_squared_error: 14.1661\n",
      "Epoch 602: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 602: ReduceLROnPlateau reducing learning rate to 0.00013527592891477978.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8380 - root_mean_squared_error: 15.3895 - val_loss: 32.8264 - val_root_mean_squared_error: 5.7294 - lr: 1.4240e-04\n",
      "Epoch 603/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 84.9226 - root_mean_squared_error: 9.2153\n",
      "Epoch 603: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7673 - root_mean_squared_error: 15.3872 - val_loss: 33.2432 - val_root_mean_squared_error: 5.7657 - lr: 1.3528e-04\n",
      "Epoch 604/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 266.9773 - root_mean_squared_error: 16.3394\n",
      "Epoch 604: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8051 - root_mean_squared_error: 15.3885 - val_loss: 33.1059 - val_root_mean_squared_error: 5.7538 - lr: 1.3528e-04\n",
      "Epoch 605/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 39.8964 - root_mean_squared_error: 6.3164\n",
      "Epoch 605: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7127 - root_mean_squared_error: 15.3855 - val_loss: 33.7580 - val_root_mean_squared_error: 5.8102 - lr: 1.3528e-04\n",
      "Epoch 606/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 261.0346 - root_mean_squared_error: 16.1566\n",
      "Epoch 606: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7714 - root_mean_squared_error: 15.3874 - val_loss: 34.0693 - val_root_mean_squared_error: 5.8369 - lr: 1.3528e-04\n",
      "Epoch 607/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 44.5627 - root_mean_squared_error: 6.6755\n",
      "Epoch 607: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 607: ReduceLROnPlateau reducing learning rate to 0.00012851213177782482.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0115 - root_mean_squared_error: 15.3952 - val_loss: 32.9497 - val_root_mean_squared_error: 5.7402 - lr: 1.3528e-04\n",
      "Epoch 608/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 70.2518 - root_mean_squared_error: 8.3816\n",
      "Epoch 608: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8281 - root_mean_squared_error: 15.3892 - val_loss: 33.1252 - val_root_mean_squared_error: 5.7554 - lr: 1.2851e-04\n",
      "Epoch 609/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 48.7194 - root_mean_squared_error: 6.9799\n",
      "Epoch 609: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9528 - root_mean_squared_error: 15.3933 - val_loss: 32.6853 - val_root_mean_squared_error: 5.7171 - lr: 1.2851e-04\n",
      "Epoch 610/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 273.1992 - root_mean_squared_error: 16.5287\n",
      "Epoch 610: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6261 - root_mean_squared_error: 15.3827 - val_loss: 33.6717 - val_root_mean_squared_error: 5.8027 - lr: 1.2851e-04\n",
      "Epoch 611/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 104.9353 - root_mean_squared_error: 10.2438\n",
      "Epoch 611: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7707 - root_mean_squared_error: 15.3874 - val_loss: 33.6817 - val_root_mean_squared_error: 5.8036 - lr: 1.2851e-04\n",
      "Epoch 612/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 251.1744 - root_mean_squared_error: 15.8485\n",
      "Epoch 612: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 612: ReduceLROnPlateau reducing learning rate to 0.00012208651896798982.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0999 - root_mean_squared_error: 15.3981 - val_loss: 31.9323 - val_root_mean_squared_error: 5.6509 - lr: 1.2851e-04\n",
      "Epoch 613/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 330.7139 - root_mean_squared_error: 18.1855\n",
      "Epoch 613: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7325 - root_mean_squared_error: 15.3861 - val_loss: 33.8231 - val_root_mean_squared_error: 5.8158 - lr: 1.2209e-04\n",
      "Epoch 614/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 117.4583 - root_mean_squared_error: 10.8378\n",
      "Epoch 614: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 237.0674 - root_mean_squared_error: 15.3970 - val_loss: 32.7307 - val_root_mean_squared_error: 5.7211 - lr: 1.2209e-04\n",
      "Epoch 615/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 461.8566 - root_mean_squared_error: 21.4908\n",
      "Epoch 615: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.8042 - root_mean_squared_error: 15.3884 - val_loss: 34.4101 - val_root_mean_squared_error: 5.8660 - lr: 1.2209e-04\n",
      "Epoch 616/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 262.2732 - root_mean_squared_error: 16.1949\n",
      "Epoch 616: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5928 - root_mean_squared_error: 15.3816 - val_loss: 33.7785 - val_root_mean_squared_error: 5.8119 - lr: 1.2209e-04\n",
      "Epoch 617/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 23.5017 - root_mean_squared_error: 4.8479\n",
      "Epoch 617: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 617: ReduceLROnPlateau reducing learning rate to 0.00011598219716688617.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6773 - root_mean_squared_error: 15.3843 - val_loss: 32.4339 - val_root_mean_squared_error: 5.6951 - lr: 1.2209e-04\n",
      "Epoch 618/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 422.0443 - root_mean_squared_error: 20.5437\n",
      "Epoch 618: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.7085 - root_mean_squared_error: 15.3853 - val_loss: 32.6941 - val_root_mean_squared_error: 5.7179 - lr: 1.1598e-04\n",
      "Epoch 619/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 216.2829 - root_mean_squared_error: 14.7066\n",
      "Epoch 619: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6929 - root_mean_squared_error: 15.3848 - val_loss: 32.0645 - val_root_mean_squared_error: 5.6626 - lr: 1.1598e-04\n",
      "Epoch 620/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 72.0033 - root_mean_squared_error: 8.4855\n",
      "Epoch 620: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6277 - root_mean_squared_error: 15.3827 - val_loss: 33.0316 - val_root_mean_squared_error: 5.7473 - lr: 1.1598e-04\n",
      "Epoch 621/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 48.8304 - root_mean_squared_error: 6.9879\n",
      "Epoch 621: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6595 - root_mean_squared_error: 15.3837 - val_loss: 32.9862 - val_root_mean_squared_error: 5.7434 - lr: 1.1598e-04\n",
      "Epoch 622/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 129.1367 - root_mean_squared_error: 11.3638\n",
      "Epoch 622: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 622: ReduceLROnPlateau reducing learning rate to 0.00011018308869097381.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.9995 - root_mean_squared_error: 15.3948 - val_loss: 31.8866 - val_root_mean_squared_error: 5.6468 - lr: 1.1598e-04\n",
      "Epoch 623/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 24.4231 - root_mean_squared_error: 4.9420\n",
      "Epoch 623: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4111 - root_mean_squared_error: 15.3757 - val_loss: 33.2802 - val_root_mean_squared_error: 5.7689 - lr: 1.1018e-04\n",
      "Epoch 624/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 253.2318 - root_mean_squared_error: 15.9133\n",
      "Epoch 624: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4211 - root_mean_squared_error: 15.3760 - val_loss: 33.2650 - val_root_mean_squared_error: 5.7676 - lr: 1.1018e-04\n",
      "Epoch 625/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 581.9834 - root_mean_squared_error: 24.1243\n",
      "Epoch 625: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4518 - root_mean_squared_error: 15.3770 - val_loss: 33.2442 - val_root_mean_squared_error: 5.7658 - lr: 1.1018e-04\n",
      "Epoch 626/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 24.8825 - root_mean_squared_error: 4.9882\n",
      "Epoch 626: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6878 - root_mean_squared_error: 15.3847 - val_loss: 32.4183 - val_root_mean_squared_error: 5.6937 - lr: 1.1018e-04\n",
      "Epoch 627/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 447.3389 - root_mean_squared_error: 21.1504\n",
      "Epoch 627: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 627: ReduceLROnPlateau reducing learning rate to 0.00010467393149156123.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5399 - root_mean_squared_error: 15.3799 - val_loss: 33.1551 - val_root_mean_squared_error: 5.7580 - lr: 1.1018e-04\n",
      "Epoch 628/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 292.8532 - root_mean_squared_error: 17.1130\n",
      "Epoch 628: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3932 - root_mean_squared_error: 15.3751 - val_loss: 32.9182 - val_root_mean_squared_error: 5.7374 - lr: 1.0467e-04\n",
      "Epoch 629/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 296.0482 - root_mean_squared_error: 17.2061\n",
      "Epoch 629: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4083 - root_mean_squared_error: 15.3756 - val_loss: 32.6013 - val_root_mean_squared_error: 5.7098 - lr: 1.0467e-04\n",
      "Epoch 630/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 520.0535 - root_mean_squared_error: 22.8047\n",
      "Epoch 630: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6731 - root_mean_squared_error: 15.3842 - val_loss: 32.9227 - val_root_mean_squared_error: 5.7378 - lr: 1.0467e-04\n",
      "Epoch 631/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 286.5171 - root_mean_squared_error: 16.9268\n",
      "Epoch 631: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3743 - root_mean_squared_error: 15.3745 - val_loss: 32.9027 - val_root_mean_squared_error: 5.7361 - lr: 1.0467e-04\n",
      "Epoch 632/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 242.8125 - root_mean_squared_error: 15.5824\n",
      "Epoch 632: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 632: ReduceLROnPlateau reducing learning rate to 9.944023768184706e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4192 - root_mean_squared_error: 15.3759 - val_loss: 32.7900 - val_root_mean_squared_error: 5.7263 - lr: 1.0467e-04\n",
      "Epoch 633/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 257.1982 - root_mean_squared_error: 16.0374\n",
      "Epoch 633: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4577 - root_mean_squared_error: 15.3772 - val_loss: 34.2053 - val_root_mean_squared_error: 5.8485 - lr: 9.9440e-05\n",
      "Epoch 634/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 298.2673 - root_mean_squared_error: 17.2704\n",
      "Epoch 634: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3630 - root_mean_squared_error: 15.3741 - val_loss: 33.5325 - val_root_mean_squared_error: 5.7907 - lr: 9.9440e-05\n",
      "Epoch 635/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 62.6557 - root_mean_squared_error: 7.9155\n",
      "Epoch 635: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5215 - root_mean_squared_error: 15.3793 - val_loss: 33.3424 - val_root_mean_squared_error: 5.7743 - lr: 9.9440e-05\n",
      "Epoch 636/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 263.3153 - root_mean_squared_error: 16.2270\n",
      "Epoch 636: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3094 - root_mean_squared_error: 15.3724 - val_loss: 32.8048 - val_root_mean_squared_error: 5.7275 - lr: 9.9440e-05\n",
      "Epoch 637/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 440.2128 - root_mean_squared_error: 20.9812\n",
      "Epoch 637: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 637: ReduceLROnPlateau reducing learning rate to 9.446822441532275e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2693 - root_mean_squared_error: 15.3711 - val_loss: 32.4914 - val_root_mean_squared_error: 5.7001 - lr: 9.9440e-05\n",
      "Epoch 638/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 306.5356 - root_mean_squared_error: 17.5082\n",
      "Epoch 638: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3774 - root_mean_squared_error: 15.3746 - val_loss: 32.4529 - val_root_mean_squared_error: 5.6967 - lr: 9.4468e-05\n",
      "Epoch 639/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 491.6129 - root_mean_squared_error: 22.1723\n",
      "Epoch 639: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5249 - root_mean_squared_error: 15.3794 - val_loss: 32.7494 - val_root_mean_squared_error: 5.7227 - lr: 9.4468e-05\n",
      "Epoch 640/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 187.7715 - root_mean_squared_error: 13.7030\n",
      "Epoch 640: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5002 - root_mean_squared_error: 15.3786 - val_loss: 34.3313 - val_root_mean_squared_error: 5.8593 - lr: 9.4468e-05\n",
      "Epoch 641/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 504.9119 - root_mean_squared_error: 22.4702\n",
      "Epoch 641: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1436 - root_mean_squared_error: 15.3670 - val_loss: 33.1814 - val_root_mean_squared_error: 5.7603 - lr: 9.4468e-05\n",
      "Epoch 642/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 287.4320 - root_mean_squared_error: 16.9538\n",
      "Epoch 642: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 642: ReduceLROnPlateau reducing learning rate to 8.974481388577259e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2536 - root_mean_squared_error: 15.3705 - val_loss: 32.2936 - val_root_mean_squared_error: 5.6827 - lr: 9.4468e-05\n",
      "Epoch 643/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 190.0588 - root_mean_squared_error: 13.7862\n",
      "Epoch 643: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5721 - root_mean_squared_error: 15.3809 - val_loss: 33.1827 - val_root_mean_squared_error: 5.7604 - lr: 8.9745e-05\n",
      "Epoch 644/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 498.4371 - root_mean_squared_error: 22.3257\n",
      "Epoch 644: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1923 - root_mean_squared_error: 15.3685 - val_loss: 33.0640 - val_root_mean_squared_error: 5.7501 - lr: 8.9745e-05\n",
      "Epoch 645/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 85.9561 - root_mean_squared_error: 9.2712\n",
      "Epoch 645: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2490 - root_mean_squared_error: 15.3704 - val_loss: 32.3001 - val_root_mean_squared_error: 5.6833 - lr: 8.9745e-05\n",
      "Epoch 646/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 451.2909 - root_mean_squared_error: 21.2436\n",
      "Epoch 646: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.6085 - root_mean_squared_error: 15.3821 - val_loss: 33.4020 - val_root_mean_squared_error: 5.7794 - lr: 8.9745e-05\n",
      "Epoch 647/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 44.2101 - root_mean_squared_error: 6.6491\n",
      "Epoch 647: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 647: ReduceLROnPlateau reducing learning rate to 8.525757111783605e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2831 - root_mean_squared_error: 15.3715 - val_loss: 32.7553 - val_root_mean_squared_error: 5.7232 - lr: 8.9745e-05\n",
      "Epoch 648/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 291.3701 - root_mean_squared_error: 17.0696\n",
      "Epoch 648: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2048 - root_mean_squared_error: 15.3690 - val_loss: 32.5325 - val_root_mean_squared_error: 5.7037 - lr: 8.5258e-05\n",
      "Epoch 649/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 351.3937 - root_mean_squared_error: 18.7455\n",
      "Epoch 649: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1336 - root_mean_squared_error: 15.3666 - val_loss: 32.7685 - val_root_mean_squared_error: 5.7244 - lr: 8.5258e-05\n",
      "Epoch 650/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 495.5533 - root_mean_squared_error: 22.2610\n",
      "Epoch 650: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2945 - root_mean_squared_error: 15.3719 - val_loss: 33.8637 - val_root_mean_squared_error: 5.8193 - lr: 8.5258e-05\n",
      "Epoch 651/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 61.9319 - root_mean_squared_error: 7.8697\n",
      "Epoch 651: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3634 - root_mean_squared_error: 15.3741 - val_loss: 33.4811 - val_root_mean_squared_error: 5.7863 - lr: 8.5258e-05\n",
      "Epoch 652/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 139.9501 - root_mean_squared_error: 11.8301\n",
      "Epoch 652: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 652: ReduceLROnPlateau reducing learning rate to 8.099469014268834e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0998 - root_mean_squared_error: 15.3655 - val_loss: 32.7082 - val_root_mean_squared_error: 5.7191 - lr: 8.5258e-05\n",
      "Epoch 653/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 203.9993 - root_mean_squared_error: 14.2828\n",
      "Epoch 653: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.3006 - root_mean_squared_error: 15.3721 - val_loss: 33.0560 - val_root_mean_squared_error: 5.7494 - lr: 8.0995e-05\n",
      "Epoch 654/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 384.8275 - root_mean_squared_error: 19.6170\n",
      "Epoch 654: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1310 - root_mean_squared_error: 15.3666 - val_loss: 33.4981 - val_root_mean_squared_error: 5.7878 - lr: 8.0995e-05\n",
      "Epoch 655/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 87.1812 - root_mean_squared_error: 9.3371\n",
      "Epoch 655: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0787 - root_mean_squared_error: 15.3649 - val_loss: 33.1518 - val_root_mean_squared_error: 5.7578 - lr: 8.0995e-05\n",
      "Epoch 656/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 53.1295 - root_mean_squared_error: 7.2890\n",
      "Epoch 656: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4151 - root_mean_squared_error: 15.3758 - val_loss: 32.1950 - val_root_mean_squared_error: 5.6741 - lr: 8.0995e-05\n",
      "Epoch 657/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 274.4950 - root_mean_squared_error: 16.5679\n",
      "Epoch 657: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 657: ReduceLROnPlateau reducing learning rate to 7.694495252508204e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1941 - root_mean_squared_error: 15.3686 - val_loss: 33.2888 - val_root_mean_squared_error: 5.7696 - lr: 8.0995e-05\n",
      "Epoch 658/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 433.6077 - root_mean_squared_error: 20.8232\n",
      "Epoch 658: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0800 - root_mean_squared_error: 15.3649 - val_loss: 32.7955 - val_root_mean_squared_error: 5.7267 - lr: 7.6945e-05\n",
      "Epoch 659/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 349.0906 - root_mean_squared_error: 18.6840\n",
      "Epoch 659: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.2437 - root_mean_squared_error: 15.3702 - val_loss: 33.5756 - val_root_mean_squared_error: 5.7944 - lr: 7.6945e-05\n",
      "Epoch 660/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 401.7986 - root_mean_squared_error: 20.0449\n",
      "Epoch 660: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1255 - root_mean_squared_error: 15.3664 - val_loss: 32.4962 - val_root_mean_squared_error: 5.7005 - lr: 7.6945e-05\n",
      "Epoch 661/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 57.9141 - root_mean_squared_error: 7.6101\n",
      "Epoch 661: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0534 - root_mean_squared_error: 15.3640 - val_loss: 32.6244 - val_root_mean_squared_error: 5.7118 - lr: 7.6945e-05\n",
      "Epoch 662/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 68.7737 - root_mean_squared_error: 8.2930\n",
      "Epoch 662: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 662: ReduceLROnPlateau reducing learning rate to 7.309770662686787e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.5909 - root_mean_squared_error: 15.3815 - val_loss: 32.1255 - val_root_mean_squared_error: 5.6679 - lr: 7.6945e-05\n",
      "Epoch 663/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 144.0418 - root_mean_squared_error: 12.0017\n",
      "Epoch 663: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9368 - root_mean_squared_error: 15.3602 - val_loss: 33.3604 - val_root_mean_squared_error: 5.7758 - lr: 7.3098e-05\n",
      "Epoch 664/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 297.4901 - root_mean_squared_error: 17.2479\n",
      "Epoch 664: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.1116 - root_mean_squared_error: 15.3659 - val_loss: 33.3806 - val_root_mean_squared_error: 5.7776 - lr: 7.3098e-05\n",
      "Epoch 665/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 45.5266 - root_mean_squared_error: 6.7473\n",
      "Epoch 665: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.4580 - root_mean_squared_error: 15.3772 - val_loss: 34.5214 - val_root_mean_squared_error: 5.8755 - lr: 7.3098e-05\n",
      "Epoch 666/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 93.9055 - root_mean_squared_error: 9.6905\n",
      "Epoch 666: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0829 - root_mean_squared_error: 15.3650 - val_loss: 33.3730 - val_root_mean_squared_error: 5.7769 - lr: 7.3098e-05\n",
      "Epoch 667/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 258.2417 - root_mean_squared_error: 16.0699\n",
      "Epoch 667: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 667: ReduceLROnPlateau reducing learning rate to 6.944281922187656e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9784 - root_mean_squared_error: 15.3616 - val_loss: 32.9204 - val_root_mean_squared_error: 5.7376 - lr: 7.3098e-05\n",
      "Epoch 668/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 115.4466 - root_mean_squared_error: 10.7446\n",
      "Epoch 668: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0589 - root_mean_squared_error: 15.3642 - val_loss: 33.8079 - val_root_mean_squared_error: 5.8145 - lr: 6.9443e-05\n",
      "Epoch 669/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 282.7256 - root_mean_squared_error: 16.8144\n",
      "Epoch 669: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0555 - root_mean_squared_error: 15.3641 - val_loss: 33.4080 - val_root_mean_squared_error: 5.7800 - lr: 6.9443e-05\n",
      "Epoch 670/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 333.1202 - root_mean_squared_error: 18.2516\n",
      "Epoch 670: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0165 - root_mean_squared_error: 15.3628 - val_loss: 33.4354 - val_root_mean_squared_error: 5.7823 - lr: 6.9443e-05\n",
      "Epoch 671/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 228.6163 - root_mean_squared_error: 15.1201\n",
      "Epoch 671: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8815 - root_mean_squared_error: 15.3584 - val_loss: 33.0558 - val_root_mean_squared_error: 5.7494 - lr: 6.9443e-05\n",
      "Epoch 672/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 119.2281 - root_mean_squared_error: 10.9192\n",
      "Epoch 672: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 672: ReduceLROnPlateau reducing learning rate to 6.597067549591884e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0101 - root_mean_squared_error: 15.3626 - val_loss: 33.1344 - val_root_mean_squared_error: 5.7563 - lr: 6.9443e-05\n",
      "Epoch 673/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 94.4050 - root_mean_squared_error: 9.7162\n",
      "Epoch 673: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9880 - root_mean_squared_error: 15.3619 - val_loss: 32.6697 - val_root_mean_squared_error: 5.7157 - lr: 6.5971e-05\n",
      "Epoch 674/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 248.0803 - root_mean_squared_error: 15.7506\n",
      "Epoch 674: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9009 - root_mean_squared_error: 15.3591 - val_loss: 32.9487 - val_root_mean_squared_error: 5.7401 - lr: 6.5971e-05\n",
      "Epoch 675/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 34.2368 - root_mean_squared_error: 5.8512\n",
      "Epoch 675: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9152 - root_mean_squared_error: 15.3595 - val_loss: 32.9983 - val_root_mean_squared_error: 5.7444 - lr: 6.5971e-05\n",
      "Epoch 676/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 304.6853 - root_mean_squared_error: 17.4552\n",
      "Epoch 676: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8855 - root_mean_squared_error: 15.3586 - val_loss: 32.6162 - val_root_mean_squared_error: 5.7111 - lr: 6.5971e-05\n",
      "Epoch 677/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 257.4361 - root_mean_squared_error: 16.0448\n",
      "Epoch 677: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 677: ReduceLROnPlateau reducing learning rate to 6.267214448598679e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9688 - root_mean_squared_error: 15.3613 - val_loss: 33.5641 - val_root_mean_squared_error: 5.7934 - lr: 6.5971e-05\n",
      "Epoch 678/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 115.5751 - root_mean_squared_error: 10.7506\n",
      "Epoch 678: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9125 - root_mean_squared_error: 15.3594 - val_loss: 32.8068 - val_root_mean_squared_error: 5.7277 - lr: 6.2672e-05\n",
      "Epoch 679/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 358.6615 - root_mean_squared_error: 18.9384\n",
      "Epoch 679: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9492 - root_mean_squared_error: 15.3606 - val_loss: 33.2533 - val_root_mean_squared_error: 5.7666 - lr: 6.2672e-05\n",
      "Epoch 680/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 102.8378 - root_mean_squared_error: 10.1409\n",
      "Epoch 680: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8370 - root_mean_squared_error: 15.3570 - val_loss: 32.5599 - val_root_mean_squared_error: 5.7061 - lr: 6.2672e-05\n",
      "Epoch 681/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 72.7499 - root_mean_squared_error: 8.5294\n",
      "Epoch 681: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0016 - root_mean_squared_error: 15.3623 - val_loss: 32.1342 - val_root_mean_squared_error: 5.6687 - lr: 6.2672e-05\n",
      "Epoch 682/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 96.2355 - root_mean_squared_error: 9.8100\n",
      "Epoch 682: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 682: ReduceLROnPlateau reducing learning rate to 5.953853760729544e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7892 - root_mean_squared_error: 15.3554 - val_loss: 32.5930 - val_root_mean_squared_error: 5.7090 - lr: 6.2672e-05\n",
      "Epoch 683/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 299.2689 - root_mean_squared_error: 17.2994\n",
      "Epoch 683: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8691 - root_mean_squared_error: 15.3580 - val_loss: 33.2951 - val_root_mean_squared_error: 5.7702 - lr: 5.9539e-05\n",
      "Epoch 684/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 210.5463 - root_mean_squared_error: 14.5102\n",
      "Epoch 684: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9036 - root_mean_squared_error: 15.3592 - val_loss: 33.4596 - val_root_mean_squared_error: 5.7844 - lr: 5.9539e-05\n",
      "Epoch 685/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 93.3613 - root_mean_squared_error: 9.6624\n",
      "Epoch 685: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8351 - root_mean_squared_error: 15.3569 - val_loss: 32.7780 - val_root_mean_squared_error: 5.7252 - lr: 5.9539e-05\n",
      "Epoch 686/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 635.7887 - root_mean_squared_error: 25.2149\n",
      "Epoch 686: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8448 - root_mean_squared_error: 15.3572 - val_loss: 33.0495 - val_root_mean_squared_error: 5.7489 - lr: 5.9539e-05\n",
      "Epoch 687/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 231.1243 - root_mean_squared_error: 15.2028\n",
      "Epoch 687: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 687: ReduceLROnPlateau reducing learning rate to 5.656161210936261e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0203 - root_mean_squared_error: 15.3630 - val_loss: 32.1264 - val_root_mean_squared_error: 5.6680 - lr: 5.9539e-05\n",
      "Epoch 688/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 41.8600 - root_mean_squared_error: 6.4699\n",
      "Epoch 688: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7726 - root_mean_squared_error: 15.3549 - val_loss: 32.4928 - val_root_mean_squared_error: 5.7002 - lr: 5.6562e-05\n",
      "Epoch 689/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 631.8596 - root_mean_squared_error: 25.1368\n",
      "Epoch 689: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 236.0115 - root_mean_squared_error: 15.3627 - val_loss: 33.2764 - val_root_mean_squared_error: 5.7686 - lr: 5.6562e-05\n",
      "Epoch 690/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 29.1597 - root_mean_squared_error: 5.4000\n",
      "Epoch 690: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8052 - root_mean_squared_error: 15.3559 - val_loss: 32.6479 - val_root_mean_squared_error: 5.7138 - lr: 5.6562e-05\n",
      "Epoch 691/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 248.9407 - root_mean_squared_error: 15.7779\n",
      "Epoch 691: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8103 - root_mean_squared_error: 15.3561 - val_loss: 32.9832 - val_root_mean_squared_error: 5.7431 - lr: 5.6562e-05\n",
      "Epoch 692/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 277.3661 - root_mean_squared_error: 16.6543\n",
      "Epoch 692: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 692: ReduceLROnPlateau reducing learning rate to 5.373353305913042e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8632 - root_mean_squared_error: 15.3578 - val_loss: 32.3771 - val_root_mean_squared_error: 5.6901 - lr: 5.6562e-05\n",
      "Epoch 693/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 251.4109 - root_mean_squared_error: 15.8559\n",
      "Epoch 693: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8575 - root_mean_squared_error: 15.3577 - val_loss: 33.1884 - val_root_mean_squared_error: 5.7609 - lr: 5.3734e-05\n",
      "Epoch 694/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 227.7279 - root_mean_squared_error: 15.0907\n",
      "Epoch 694: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8030 - root_mean_squared_error: 15.3559 - val_loss: 32.6460 - val_root_mean_squared_error: 5.7137 - lr: 5.3734e-05\n",
      "Epoch 695/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 66.5199 - root_mean_squared_error: 8.1560\n",
      "Epoch 695: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6776 - root_mean_squared_error: 15.3518 - val_loss: 33.1364 - val_root_mean_squared_error: 5.7564 - lr: 5.3734e-05\n",
      "Epoch 696/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 496.5306 - root_mean_squared_error: 22.2830\n",
      "Epoch 696: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.8113 - root_mean_squared_error: 15.3561 - val_loss: 33.3267 - val_root_mean_squared_error: 5.7729 - lr: 5.3734e-05\n",
      "Epoch 697/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 13.8561 - root_mean_squared_error: 3.7224\n",
      "Epoch 697: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 697: ReduceLROnPlateau reducing learning rate to 5.104685606056591e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7134 - root_mean_squared_error: 15.3530 - val_loss: 32.9846 - val_root_mean_squared_error: 5.7432 - lr: 5.3734e-05\n",
      "Epoch 698/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 254.4907 - root_mean_squared_error: 15.9528\n",
      "Epoch 698: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7000 - root_mean_squared_error: 15.3525 - val_loss: 33.3144 - val_root_mean_squared_error: 5.7719 - lr: 5.1047e-05\n",
      "Epoch 699/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 137.8643 - root_mean_squared_error: 11.7416\n",
      "Epoch 699: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9390 - root_mean_squared_error: 15.3603 - val_loss: 32.7902 - val_root_mean_squared_error: 5.7263 - lr: 5.1047e-05\n",
      "Epoch 700/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 278.8312 - root_mean_squared_error: 16.6982\n",
      "Epoch 700: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7020 - root_mean_squared_error: 15.3526 - val_loss: 32.7142 - val_root_mean_squared_error: 5.7196 - lr: 5.1047e-05\n",
      "Epoch 701/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 270.3146 - root_mean_squared_error: 16.4412\n",
      "Epoch 701: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6955 - root_mean_squared_error: 15.3524 - val_loss: 32.6925 - val_root_mean_squared_error: 5.7177 - lr: 5.1047e-05\n",
      "Epoch 702/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 289.6765 - root_mean_squared_error: 17.0199\n",
      "Epoch 702: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 702: ReduceLROnPlateau reducing learning rate to 4.849451343034161e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6636 - root_mean_squared_error: 15.3513 - val_loss: 32.9394 - val_root_mean_squared_error: 5.7393 - lr: 5.1047e-05\n",
      "Epoch 703/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 26.4066 - root_mean_squared_error: 5.1387\n",
      "Epoch 703: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6664 - root_mean_squared_error: 15.3514 - val_loss: 32.8720 - val_root_mean_squared_error: 5.7334 - lr: 4.8495e-05\n",
      "Epoch 704/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 102.9811 - root_mean_squared_error: 10.1480\n",
      "Epoch 704: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6628 - root_mean_squared_error: 15.3513 - val_loss: 32.8262 - val_root_mean_squared_error: 5.7294 - lr: 4.8495e-05\n",
      "Epoch 705/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 326.2885 - root_mean_squared_error: 18.0635\n",
      "Epoch 705: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6462 - root_mean_squared_error: 15.3508 - val_loss: 33.0382 - val_root_mean_squared_error: 5.7479 - lr: 4.8495e-05\n",
      "Epoch 706/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 66.1624 - root_mean_squared_error: 8.1340\n",
      "Epoch 706: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6615 - root_mean_squared_error: 15.3513 - val_loss: 33.0238 - val_root_mean_squared_error: 5.7466 - lr: 4.8495e-05\n",
      "Epoch 707/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 310.9984 - root_mean_squared_error: 17.6351\n",
      "Epoch 707: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 707: ReduceLROnPlateau reducing learning rate to 4.606978654919658e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6887 - root_mean_squared_error: 15.3522 - val_loss: 32.8918 - val_root_mean_squared_error: 5.7351 - lr: 4.8495e-05\n",
      "Epoch 708/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 198.0911 - root_mean_squared_error: 14.0745\n",
      "Epoch 708: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6146 - root_mean_squared_error: 15.3497 - val_loss: 32.9925 - val_root_mean_squared_error: 5.7439 - lr: 4.6070e-05\n",
      "Epoch 709/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 544.4978 - root_mean_squared_error: 23.3345\n",
      "Epoch 709: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6406 - root_mean_squared_error: 15.3506 - val_loss: 32.9580 - val_root_mean_squared_error: 5.7409 - lr: 4.6070e-05\n",
      "Epoch 710/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 29.2560 - root_mean_squared_error: 5.4089\n",
      "Epoch 710: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6702 - root_mean_squared_error: 15.3516 - val_loss: 32.5853 - val_root_mean_squared_error: 5.7083 - lr: 4.6070e-05\n",
      "Epoch 711/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 384.4247 - root_mean_squared_error: 19.6068\n",
      "Epoch 711: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6583 - root_mean_squared_error: 15.3512 - val_loss: 32.8191 - val_root_mean_squared_error: 5.7288 - lr: 4.6070e-05\n",
      "Epoch 712/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 75.0265 - root_mean_squared_error: 8.6618\n",
      "Epoch 712: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 712: ReduceLROnPlateau reducing learning rate to 4.3766295493696814e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6221 - root_mean_squared_error: 15.3500 - val_loss: 33.1125 - val_root_mean_squared_error: 5.7543 - lr: 4.6070e-05\n",
      "Epoch 713/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 284.5580 - root_mean_squared_error: 16.8688\n",
      "Epoch 713: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.9449 - root_mean_squared_error: 15.3605 - val_loss: 33.7411 - val_root_mean_squared_error: 5.8087 - lr: 4.3766e-05\n",
      "Epoch 714/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 45.7971 - root_mean_squared_error: 6.7674\n",
      "Epoch 714: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.7828 - root_mean_squared_error: 15.3552 - val_loss: 33.2010 - val_root_mean_squared_error: 5.7620 - lr: 4.3766e-05\n",
      "Epoch 715/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 389.4167 - root_mean_squared_error: 19.7336\n",
      "Epoch 715: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6409 - root_mean_squared_error: 15.3506 - val_loss: 33.1618 - val_root_mean_squared_error: 5.7586 - lr: 4.3766e-05\n",
      "Epoch 716/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 192.9932 - root_mean_squared_error: 13.8922\n",
      "Epoch 716: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5912 - root_mean_squared_error: 15.3490 - val_loss: 32.7536 - val_root_mean_squared_error: 5.7231 - lr: 4.3766e-05\n",
      "Epoch 717/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 144.3496 - root_mean_squared_error: 12.0146\n",
      "Epoch 717: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 717: ReduceLROnPlateau reducing learning rate to 4.1577981755835934e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5989 - root_mean_squared_error: 15.3492 - val_loss: 32.8435 - val_root_mean_squared_error: 5.7309 - lr: 4.3766e-05\n",
      "Epoch 718/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 36.1255 - root_mean_squared_error: 6.0104\n",
      "Epoch 718: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6324 - root_mean_squared_error: 15.3503 - val_loss: 32.4587 - val_root_mean_squared_error: 5.6973 - lr: 4.1578e-05\n",
      "Epoch 719/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 349.2313 - root_mean_squared_error: 18.6877\n",
      "Epoch 719: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5805 - root_mean_squared_error: 15.3486 - val_loss: 32.8719 - val_root_mean_squared_error: 5.7334 - lr: 4.1578e-05\n",
      "Epoch 720/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 197.7125 - root_mean_squared_error: 14.0610\n",
      "Epoch 720: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5697 - root_mean_squared_error: 15.3483 - val_loss: 33.0859 - val_root_mean_squared_error: 5.7520 - lr: 4.1578e-05\n",
      "Epoch 721/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 56.0242 - root_mean_squared_error: 7.4849\n",
      "Epoch 721: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6358 - root_mean_squared_error: 15.3504 - val_loss: 33.6977 - val_root_mean_squared_error: 5.8050 - lr: 4.1578e-05\n",
      "Epoch 722/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 374.0152 - root_mean_squared_error: 19.3395\n",
      "Epoch 722: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 722: ReduceLROnPlateau reducing learning rate to 3.949908405047608e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6299 - root_mean_squared_error: 15.3502 - val_loss: 33.1945 - val_root_mean_squared_error: 5.7615 - lr: 4.1578e-05\n",
      "Epoch 723/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 291.5185 - root_mean_squared_error: 17.0739\n",
      "Epoch 723: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5884 - root_mean_squared_error: 15.3489 - val_loss: 33.1711 - val_root_mean_squared_error: 5.7594 - lr: 3.9499e-05\n",
      "Epoch 724/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 93.5565 - root_mean_squared_error: 9.6725\n",
      "Epoch 724: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5665 - root_mean_squared_error: 15.3482 - val_loss: 32.8192 - val_root_mean_squared_error: 5.7288 - lr: 3.9499e-05\n",
      "Epoch 725/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 269.4482 - root_mean_squared_error: 16.4149\n",
      "Epoch 725: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.6070 - root_mean_squared_error: 15.3495 - val_loss: 32.5590 - val_root_mean_squared_error: 5.7061 - lr: 3.9499e-05\n",
      "Epoch 726/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 137.3030 - root_mean_squared_error: 11.7176\n",
      "Epoch 726: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5537 - root_mean_squared_error: 15.3478 - val_loss: 32.8352 - val_root_mean_squared_error: 5.7302 - lr: 3.9499e-05\n",
      "Epoch 727/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 71.2133 - root_mean_squared_error: 8.4388\n",
      "Epoch 727: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 727: ReduceLROnPlateau reducing learning rate to 3.752413140318822e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4924 - root_mean_squared_error: 15.3458 - val_loss: 33.2328 - val_root_mean_squared_error: 5.7648 - lr: 3.9499e-05\n",
      "Epoch 728/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 75.6882 - root_mean_squared_error: 8.6999\n",
      "Epoch 728: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5394 - root_mean_squared_error: 15.3473 - val_loss: 33.1481 - val_root_mean_squared_error: 5.7574 - lr: 3.7524e-05\n",
      "Epoch 729/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 31.5831 - root_mean_squared_error: 5.6199\n",
      "Epoch 729: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5632 - root_mean_squared_error: 15.3481 - val_loss: 33.1500 - val_root_mean_squared_error: 5.7576 - lr: 3.7524e-05\n",
      "Epoch 730/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 132.0611 - root_mean_squared_error: 11.4918\n",
      "Epoch 730: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5606 - root_mean_squared_error: 15.3480 - val_loss: 32.9899 - val_root_mean_squared_error: 5.7437 - lr: 3.7524e-05\n",
      "Epoch 731/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 86.7592 - root_mean_squared_error: 9.3145\n",
      "Epoch 731: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4915 - root_mean_squared_error: 15.3457 - val_loss: 33.0089 - val_root_mean_squared_error: 5.7453 - lr: 3.7524e-05\n",
      "Epoch 732/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 665.9601 - root_mean_squared_error: 25.8062\n",
      "Epoch 732: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 732: ReduceLROnPlateau reducing learning rate to 3.564792586985277e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5571 - root_mean_squared_error: 15.3479 - val_loss: 33.1200 - val_root_mean_squared_error: 5.7550 - lr: 3.7524e-05\n",
      "Epoch 733/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 362.9494 - root_mean_squared_error: 19.0512\n",
      "Epoch 733: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4746 - root_mean_squared_error: 15.3452 - val_loss: 32.8682 - val_root_mean_squared_error: 5.7331 - lr: 3.5648e-05\n",
      "Epoch 734/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 41.4878 - root_mean_squared_error: 6.4411\n",
      "Epoch 734: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4999 - root_mean_squared_error: 15.3460 - val_loss: 32.6634 - val_root_mean_squared_error: 5.7152 - lr: 3.5648e-05\n",
      "Epoch 735/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 91.3334 - root_mean_squared_error: 9.5569\n",
      "Epoch 735: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5687 - root_mean_squared_error: 15.3482 - val_loss: 32.5090 - val_root_mean_squared_error: 5.7017 - lr: 3.5648e-05\n",
      "Epoch 736/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 249.6613 - root_mean_squared_error: 15.8007\n",
      "Epoch 736: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4783 - root_mean_squared_error: 15.3453 - val_loss: 32.6416 - val_root_mean_squared_error: 5.7133 - lr: 3.5648e-05\n",
      "Epoch 737/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 304.0981 - root_mean_squared_error: 17.4384\n",
      "Epoch 737: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 737: ReduceLROnPlateau reducing learning rate to 3.3865528712340166e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4758 - root_mean_squared_error: 15.3452 - val_loss: 32.9053 - val_root_mean_squared_error: 5.7363 - lr: 3.5648e-05\n",
      "Epoch 738/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 251.2978 - root_mean_squared_error: 15.8524\n",
      "Epoch 738: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4758 - root_mean_squared_error: 15.3452 - val_loss: 32.9554 - val_root_mean_squared_error: 5.7407 - lr: 3.3866e-05\n",
      "Epoch 739/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 178.1254 - root_mean_squared_error: 13.3464\n",
      "Epoch 739: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4853 - root_mean_squared_error: 15.3455 - val_loss: 32.9704 - val_root_mean_squared_error: 5.7420 - lr: 3.3866e-05\n",
      "Epoch 740/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 40.0788 - root_mean_squared_error: 6.3308\n",
      "Epoch 740: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4982 - root_mean_squared_error: 15.3460 - val_loss: 32.8779 - val_root_mean_squared_error: 5.7339 - lr: 3.3866e-05\n",
      "Epoch 741/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 59.8324 - root_mean_squared_error: 7.7351\n",
      "Epoch 741: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4680 - root_mean_squared_error: 15.3450 - val_loss: 32.9754 - val_root_mean_squared_error: 5.7424 - lr: 3.3866e-05\n",
      "Epoch 742/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 35.8715 - root_mean_squared_error: 5.9893\n",
      "Epoch 742: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 742: ReduceLROnPlateau reducing learning rate to 3.217225348635111e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4655 - root_mean_squared_error: 15.3449 - val_loss: 32.7860 - val_root_mean_squared_error: 5.7259 - lr: 3.3866e-05\n",
      "Epoch 743/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 577.7407 - root_mean_squared_error: 24.0362\n",
      "Epoch 743: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4832 - root_mean_squared_error: 15.3455 - val_loss: 32.8674 - val_root_mean_squared_error: 5.7330 - lr: 3.2172e-05\n",
      "Epoch 744/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 30.7695 - root_mean_squared_error: 5.5470\n",
      "Epoch 744: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4797 - root_mean_squared_error: 15.3453 - val_loss: 32.6679 - val_root_mean_squared_error: 5.7156 - lr: 3.2172e-05\n",
      "Epoch 745/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 328.1417 - root_mean_squared_error: 18.1147\n",
      "Epoch 745: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4498 - root_mean_squared_error: 15.3444 - val_loss: 32.9896 - val_root_mean_squared_error: 5.7437 - lr: 3.2172e-05\n",
      "Epoch 746/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 475.7832 - root_mean_squared_error: 21.8125\n",
      "Epoch 746: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4525 - root_mean_squared_error: 15.3445 - val_loss: 33.0182 - val_root_mean_squared_error: 5.7461 - lr: 3.2172e-05\n",
      "Epoch 747/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 488.0640 - root_mean_squared_error: 22.0922\n",
      "Epoch 747: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 747: ReduceLROnPlateau reducing learning rate to 3.0563641848857516e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4529 - root_mean_squared_error: 15.3445 - val_loss: 32.7349 - val_root_mean_squared_error: 5.7214 - lr: 3.2172e-05\n",
      "Epoch 748/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 87.2821 - root_mean_squared_error: 9.3425\n",
      "Epoch 748: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.5041 - root_mean_squared_error: 15.3461 - val_loss: 32.8346 - val_root_mean_squared_error: 5.7301 - lr: 3.0564e-05\n",
      "Epoch 749/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 127.4276 - root_mean_squared_error: 11.2884\n",
      "Epoch 749: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4420 - root_mean_squared_error: 15.3441 - val_loss: 32.8221 - val_root_mean_squared_error: 5.7291 - lr: 3.0564e-05\n",
      "Epoch 750/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 311.1035 - root_mean_squared_error: 17.6381\n",
      "Epoch 750: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4960 - root_mean_squared_error: 15.3459 - val_loss: 32.8954 - val_root_mean_squared_error: 5.7355 - lr: 3.0564e-05\n",
      "Epoch 751/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 57.9356 - root_mean_squared_error: 7.6115\n",
      "Epoch 751: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4238 - root_mean_squared_error: 15.3435 - val_loss: 32.5770 - val_root_mean_squared_error: 5.7076 - lr: 3.0564e-05\n",
      "Epoch 752/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 84.3278 - root_mean_squared_error: 9.1830\n",
      "Epoch 752: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 752: ReduceLROnPlateau reducing learning rate to 2.9035460102022626e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4572 - root_mean_squared_error: 15.3446 - val_loss: 32.6322 - val_root_mean_squared_error: 5.7125 - lr: 3.0564e-05\n",
      "Epoch 753/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.9584 - root_mean_squared_error: 6.8526\n",
      "Epoch 753: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4717 - root_mean_squared_error: 15.3451 - val_loss: 33.0304 - val_root_mean_squared_error: 5.7472 - lr: 2.9035e-05\n",
      "Epoch 754/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 161.3014 - root_mean_squared_error: 12.7004\n",
      "Epoch 754: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4044 - root_mean_squared_error: 15.3429 - val_loss: 32.8083 - val_root_mean_squared_error: 5.7279 - lr: 2.9035e-05\n",
      "Epoch 755/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 480.8503 - root_mean_squared_error: 21.9283\n",
      "Epoch 755: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3953 - root_mean_squared_error: 15.3426 - val_loss: 32.7576 - val_root_mean_squared_error: 5.7234 - lr: 2.9035e-05\n",
      "Epoch 756/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 34.1198 - root_mean_squared_error: 5.8412\n",
      "Epoch 756: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4270 - root_mean_squared_error: 15.3436 - val_loss: 32.7692 - val_root_mean_squared_error: 5.7244 - lr: 2.9035e-05\n",
      "Epoch 757/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 317.3553 - root_mean_squared_error: 17.8145\n",
      "Epoch 757: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 757: ReduceLROnPlateau reducing learning rate to 2.7583687096921494e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3982 - root_mean_squared_error: 15.3427 - val_loss: 33.1315 - val_root_mean_squared_error: 5.7560 - lr: 2.9035e-05\n",
      "Epoch 758/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 493.4746 - root_mean_squared_error: 22.2143\n",
      "Epoch 758: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4015 - root_mean_squared_error: 15.3428 - val_loss: 33.1098 - val_root_mean_squared_error: 5.7541 - lr: 2.7584e-05\n",
      "Epoch 759/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 282.9258 - root_mean_squared_error: 16.8204\n",
      "Epoch 759: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3781 - root_mean_squared_error: 15.3420 - val_loss: 32.9873 - val_root_mean_squared_error: 5.7435 - lr: 2.7584e-05\n",
      "Epoch 760/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 25.1266 - root_mean_squared_error: 5.0126\n",
      "Epoch 760: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3909 - root_mean_squared_error: 15.3425 - val_loss: 32.8169 - val_root_mean_squared_error: 5.7286 - lr: 2.7584e-05\n",
      "Epoch 761/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 527.9202 - root_mean_squared_error: 22.9765\n",
      "Epoch 761: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3827 - root_mean_squared_error: 15.3422 - val_loss: 32.7091 - val_root_mean_squared_error: 5.7192 - lr: 2.7584e-05\n",
      "Epoch 762/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 240.9153 - root_mean_squared_error: 15.5214\n",
      "Epoch 762: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 762: ReduceLROnPlateau reducing learning rate to 2.620450213726144e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3890 - root_mean_squared_error: 15.3424 - val_loss: 32.5666 - val_root_mean_squared_error: 5.7067 - lr: 2.7584e-05\n",
      "Epoch 763/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 343.1439 - root_mean_squared_error: 18.5241\n",
      "Epoch 763: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3962 - root_mean_squared_error: 15.3426 - val_loss: 32.7063 - val_root_mean_squared_error: 5.7189 - lr: 2.6205e-05\n",
      "Epoch 764/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 271.3838 - root_mean_squared_error: 16.4737\n",
      "Epoch 764: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3772 - root_mean_squared_error: 15.3420 - val_loss: 32.6499 - val_root_mean_squared_error: 5.7140 - lr: 2.6205e-05\n",
      "Epoch 765/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 38.6385 - root_mean_squared_error: 6.2160\n",
      "Epoch 765: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3907 - root_mean_squared_error: 15.3424 - val_loss: 32.5130 - val_root_mean_squared_error: 5.7020 - lr: 2.6205e-05\n",
      "Epoch 766/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 74.9652 - root_mean_squared_error: 8.6582\n",
      "Epoch 766: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4413 - root_mean_squared_error: 15.3441 - val_loss: 32.4013 - val_root_mean_squared_error: 5.6922 - lr: 2.6205e-05\n",
      "Epoch 767/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 24.7846 - root_mean_squared_error: 4.9784\n",
      "Epoch 767: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 767: ReduceLROnPlateau reducing learning rate to 2.4894276339182395e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3568 - root_mean_squared_error: 15.3413 - val_loss: 32.6667 - val_root_mean_squared_error: 5.7155 - lr: 2.6205e-05\n",
      "Epoch 768/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 247.2319 - root_mean_squared_error: 15.7236\n",
      "Epoch 768: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3561 - root_mean_squared_error: 15.3413 - val_loss: 32.9192 - val_root_mean_squared_error: 5.7375 - lr: 2.4894e-05\n",
      "Epoch 769/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 59.6692 - root_mean_squared_error: 7.7246\n",
      "Epoch 769: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3991 - root_mean_squared_error: 15.3427 - val_loss: 32.7290 - val_root_mean_squared_error: 5.7209 - lr: 2.4894e-05\n",
      "Epoch 770/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 46.3875 - root_mean_squared_error: 6.8108\n",
      "Epoch 770: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3315 - root_mean_squared_error: 15.3405 - val_loss: 32.9507 - val_root_mean_squared_error: 5.7403 - lr: 2.4894e-05\n",
      "Epoch 771/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 258.6530 - root_mean_squared_error: 16.0827\n",
      "Epoch 771: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3395 - root_mean_squared_error: 15.3408 - val_loss: 33.1784 - val_root_mean_squared_error: 5.7601 - lr: 2.4894e-05\n",
      "Epoch 772/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 489.0618 - root_mean_squared_error: 22.1147\n",
      "Epoch 772: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 772: ReduceLROnPlateau reducing learning rate to 2.3649562263017287e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4017 - root_mean_squared_error: 15.3428 - val_loss: 33.2332 - val_root_mean_squared_error: 5.7648 - lr: 2.4894e-05\n",
      "Epoch 773/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 280.8131 - root_mean_squared_error: 16.7575\n",
      "Epoch 773: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3500 - root_mean_squared_error: 15.3411 - val_loss: 33.1081 - val_root_mean_squared_error: 5.7540 - lr: 2.3650e-05\n",
      "Epoch 774/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 263.2377 - root_mean_squared_error: 16.2246\n",
      "Epoch 774: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3562 - root_mean_squared_error: 15.3413 - val_loss: 33.0128 - val_root_mean_squared_error: 5.7457 - lr: 2.3650e-05\n",
      "Epoch 775/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 129.1635 - root_mean_squared_error: 11.3650\n",
      "Epoch 775: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.4232 - root_mean_squared_error: 15.3435 - val_loss: 32.5136 - val_root_mean_squared_error: 5.7021 - lr: 2.3650e-05\n",
      "Epoch 776/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 43.6674 - root_mean_squared_error: 6.6081\n",
      "Epoch 776: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3240 - root_mean_squared_error: 15.3403 - val_loss: 32.6922 - val_root_mean_squared_error: 5.7177 - lr: 2.3650e-05\n",
      "Epoch 777/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 382.2980 - root_mean_squared_error: 19.5524\n",
      "Epoch 777: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 777: ReduceLROnPlateau reducing learning rate to 2.2467083545052444e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3671 - root_mean_squared_error: 15.3417 - val_loss: 32.7538 - val_root_mean_squared_error: 5.7231 - lr: 2.3650e-05\n",
      "Epoch 778/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 39.9618 - root_mean_squared_error: 6.3215\n",
      "Epoch 778: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3335 - root_mean_squared_error: 15.3406 - val_loss: 32.9684 - val_root_mean_squared_error: 5.7418 - lr: 2.2467e-05\n",
      "Epoch 779/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 517.0840 - root_mean_squared_error: 22.7395\n",
      "Epoch 779: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3499 - root_mean_squared_error: 15.3411 - val_loss: 32.7667 - val_root_mean_squared_error: 5.7242 - lr: 2.2467e-05\n",
      "Epoch 780/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 254.5753 - root_mean_squared_error: 15.9554\n",
      "Epoch 780: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3412 - root_mean_squared_error: 15.3408 - val_loss: 32.9310 - val_root_mean_squared_error: 5.7386 - lr: 2.2467e-05\n",
      "Epoch 781/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 283.2833 - root_mean_squared_error: 16.8310\n",
      "Epoch 781: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3554 - root_mean_squared_error: 15.3413 - val_loss: 32.8535 - val_root_mean_squared_error: 5.7318 - lr: 2.2467e-05\n",
      "Epoch 782/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 255.7890 - root_mean_squared_error: 15.9934\n",
      "Epoch 782: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 782: ReduceLROnPlateau reducing learning rate to 2.134372971340781e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3233 - root_mean_squared_error: 15.3403 - val_loss: 32.7185 - val_root_mean_squared_error: 5.7200 - lr: 2.2467e-05\n",
      "Epoch 783/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 624.8274 - root_mean_squared_error: 24.9965\n",
      "Epoch 783: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3342 - root_mean_squared_error: 15.3406 - val_loss: 32.8734 - val_root_mean_squared_error: 5.7335 - lr: 2.1344e-05\n",
      "Epoch 784/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 57.3804 - root_mean_squared_error: 7.5750\n",
      "Epoch 784: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3777 - root_mean_squared_error: 15.3420 - val_loss: 33.1122 - val_root_mean_squared_error: 5.7543 - lr: 2.1344e-05\n",
      "Epoch 785/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 250.9317 - root_mean_squared_error: 15.8408\n",
      "Epoch 785: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3575 - root_mean_squared_error: 15.3414 - val_loss: 32.6581 - val_root_mean_squared_error: 5.7147 - lr: 2.1344e-05\n",
      "Epoch 786/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 207.5157 - root_mean_squared_error: 14.4054\n",
      "Epoch 786: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3110 - root_mean_squared_error: 15.3398 - val_loss: 32.7618 - val_root_mean_squared_error: 5.7238 - lr: 2.1344e-05\n",
      "Epoch 787/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 23.0527 - root_mean_squared_error: 4.8013\n",
      "Epoch 787: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 787: ReduceLROnPlateau reducing learning rate to 2.0276544091757387e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3240 - root_mean_squared_error: 15.3403 - val_loss: 32.6794 - val_root_mean_squared_error: 5.7166 - lr: 2.1344e-05\n",
      "Epoch 788/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 496.0453 - root_mean_squared_error: 22.2721\n",
      "Epoch 788: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3335 - root_mean_squared_error: 15.3406 - val_loss: 32.7966 - val_root_mean_squared_error: 5.7268 - lr: 2.0277e-05\n",
      "Epoch 789/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 297.8095 - root_mean_squared_error: 17.2572\n",
      "Epoch 789: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3001 - root_mean_squared_error: 15.3395 - val_loss: 32.6411 - val_root_mean_squared_error: 5.7132 - lr: 2.0277e-05\n",
      "Epoch 790/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 110.8133 - root_mean_squared_error: 10.5268\n",
      "Epoch 790: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3213 - root_mean_squared_error: 15.3402 - val_loss: 32.5355 - val_root_mean_squared_error: 5.7040 - lr: 2.0277e-05\n",
      "Epoch 791/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 44.7689 - root_mean_squared_error: 6.6910\n",
      "Epoch 791: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3081 - root_mean_squared_error: 15.3398 - val_loss: 32.7529 - val_root_mean_squared_error: 5.7230 - lr: 2.0277e-05\n",
      "Epoch 792/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 300.6871 - root_mean_squared_error: 17.3403\n",
      "Epoch 792: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 792: ReduceLROnPlateau reducing learning rate to 1.9262716887169517e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3951 - root_mean_squared_error: 15.3426 - val_loss: 32.4922 - val_root_mean_squared_error: 5.7002 - lr: 2.0277e-05\n",
      "Epoch 793/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 89.6279 - root_mean_squared_error: 9.4672\n",
      "Epoch 793: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.2673 - root_mean_squared_error: 15.3384 - val_loss: 32.6850 - val_root_mean_squared_error: 5.7171 - lr: 1.9263e-05\n",
      "Epoch 794/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 40.3280 - root_mean_squared_error: 6.3504\n",
      "Epoch 794: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3279 - root_mean_squared_error: 15.3404 - val_loss: 32.9436 - val_root_mean_squared_error: 5.7396 - lr: 1.9263e-05\n",
      "Epoch 795/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 224.0887 - root_mean_squared_error: 14.9696\n",
      "Epoch 795: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3083 - root_mean_squared_error: 15.3398 - val_loss: 32.8915 - val_root_mean_squared_error: 5.7351 - lr: 1.9263e-05\n",
      "Epoch 796/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 63.4304 - root_mean_squared_error: 7.9643\n",
      "Epoch 796: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3055 - root_mean_squared_error: 15.3397 - val_loss: 32.9299 - val_root_mean_squared_error: 5.7385 - lr: 1.9263e-05\n",
      "Epoch 797/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 369.3524 - root_mean_squared_error: 19.2185\n",
      "Epoch 797: val_loss did not improve from 31.87499\n",
      "\n",
      "Epoch 797: ReduceLROnPlateau reducing learning rate to 1.8299581734027014e-05.\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.2855 - root_mean_squared_error: 15.3390 - val_loss: 32.7639 - val_root_mean_squared_error: 5.7240 - lr: 1.9263e-05\n",
      "Epoch 798/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 155.7985 - root_mean_squared_error: 12.4819\n",
      "Epoch 798: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.2850 - root_mean_squared_error: 15.3390 - val_loss: 32.6038 - val_root_mean_squared_error: 5.7100 - lr: 1.8300e-05\n",
      "Epoch 799/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 140.4892 - root_mean_squared_error: 11.8528\n",
      "Epoch 799: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.3025 - root_mean_squared_error: 15.3396 - val_loss: 32.6988 - val_root_mean_squared_error: 5.7183 - lr: 1.8300e-05\n",
      "Epoch 800/800\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 20.6800 - root_mean_squared_error: 4.5475\n",
      "Epoch 800: val_loss did not improve from 31.87499\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 235.2926 - root_mean_squared_error: 15.3393 - val_loss: 32.6255 - val_root_mean_squared_error: 5.7119 - lr: 1.8300e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25fd3830d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=TrainX, y=TrainY, epochs=800, shuffle=True,\n",
    "          batch_size=48, callbacks=CALLBACK, validation_data = (ValidX, ValidY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28688770",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = model.predict(ValidX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e40b9858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Pred, ValidY, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello_world():\n",
    "    return \"<p>Hello, World!</p>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/sosal\")\n",
    "def hello_sosal():\n",
    "    return \"<p>Hello, sosal</p>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68721e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(host='127.0.0.1', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd626c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
